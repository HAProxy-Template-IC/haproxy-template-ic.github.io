{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HAProxy Template IC","text":"<p>A template-driven HAProxy Ingress Controller for Kubernetes that generates HAProxy configurations using Jinja2 templates and deploys them via the HAProxy Dataplane API.</p>"},{"location":"#what-is-haproxy-template-ic","title":"What is HAProxy Template IC?","text":"<p>HAProxy Template IC is an event-driven Kubernetes controller that:</p> <ul> <li>Watches Kubernetes resources (Ingresses, Services, EndpointSlices, Secrets, and Gateway API resources)</li> <li>Renders Jinja2 templates to generate HAProxy configurations</li> <li>Deploys configurations to HAProxy pods via the Dataplane API</li> <li>Validates configurations before deployment to prevent outages</li> </ul> <p>Unlike traditional ingress controllers with hardcoded configuration logic, HAProxy Template IC uses a template-driven approach that gives you full control over the generated HAProxy configuration.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Template Libraries - Pre-built templates for Ingress and Gateway API with HAProxy annotation support</li> <li>Resource-Agnostic Architecture - Watch any Kubernetes resource type via configuration</li> <li>High Availability - Leader election with automatic failover</li> <li>Validation Pipeline - Configurations are validated before deployment</li> <li>Observability - Prometheus metrics, structured logging, and debug endpoints</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#install-with-helm","title":"Install with Helm","text":"<pre><code>helm repo add haproxy-template-ic https://haproxy-template-ic.github.io/charts\nhelm repo update\nhelm install my-controller haproxy-template-ic/haproxy-template-ic\n</code></pre> <p>See the Helm Chart Installation Guide for detailed configuration options.</p>"},{"location":"#create-an-ingress","title":"Create an Ingress","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example\nspec:\n  ingressClassName: haproxy\n  rules:\n    - host: example.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: example-service\n                port:\n                  number: 80\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"Section Description Getting Started Initial setup and basic concepts Helm Chart Installation via Helm with pre-built templates Gateway API HTTPRoute and GRPCRoute support HAProxy Annotations Compatible with haproxy.org annotations Templating Custom template development Operations Production deployment guidance"},{"location":"#architecture","title":"Architecture","text":"<pre><code>                    Kubernetes Cluster\n                           \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                 \u2502                 \u2502\n    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Ingress \u2502      \u2502  Service  \u2502     \u2502  Secret   \u2502\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                 \u2502                 \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502  Controller   \u2502\n              \u2502   (Watcher)   \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502   Template    \u2502\n              \u2502    Engine     \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502   Dataplane   \u2502\n              \u2502     API       \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502    HAProxy    \u2502\n              \u2502     Pods      \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#why-template-driven","title":"Why Template-Driven?","text":"<p>Traditional ingress controllers embed configuration logic in code. When you need custom HAProxy features, you're limited to what the controller supports or waiting for upstream changes.</p> <p>HAProxy Template IC inverts this model:</p> <ol> <li>You control the templates - Full access to HAProxy's configuration language</li> <li>Add features without code changes - New HAProxy directives are just template updates</li> <li>Test configurations independently - Validation tests run against your templates</li> <li>Version your configuration - Templates are declarative and reviewable</li> </ol>"},{"location":"#project-status","title":"Project Status","text":"<p>HAProxy Template IC is in active development. The Helm chart provides production-ready defaults for:</p> <ul> <li>Kubernetes Ingress resources</li> <li>Gateway API HTTPRoute and GRPCRoute</li> <li>HAProxy Tech annotation compatibility</li> </ul> <p>See the Supported Configuration reference for the complete feature matrix.</p>"},{"location":"#license","title":"License","text":"<p>Apache License 2.0</p>"},{"location":"configuration/","title":"Controller Configuration Reference","text":"<p>Deprecated: Use HAProxyTemplateConfig CRD Instead</p> <p>This ConfigMap-based configuration approach is deprecated. For new deployments, use the HAProxyTemplateConfig CRD which provides:</p> <ul> <li>Kubernetes-native configuration with validation</li> <li>Better integration with GitOps workflows</li> <li>Schema validation and type safety</li> <li>Status reporting and conditions</li> </ul> <p>This documentation is maintained for reference only. Migrate existing deployments to the CRD approach.</p>"},{"location":"configuration/#overview","title":"Overview","text":"<p>The HAProxy Template Ingress Controller is configured using two Kubernetes resources:</p> <ol> <li>ConfigMap (deprecated) - Contains the controller configuration, templates, and HAProxy settings</li> <li>Secret - Contains credentials for the HAProxy Dataplane API</li> </ol> <p>The controller automatically watches these resources for changes and reloads configuration dynamically without requiring pod restarts.</p>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>The controller accepts configuration through environment variables and command-line flags.</p> Variable Flag Default Description <code>CONFIGMAP_NAME</code> <code>--configmap-name</code> <code>haproxy-config</code> Name of the ConfigMap containing controller configuration <code>SECRET_NAME</code> <code>--secret-name</code> <code>haproxy-credentials</code> Name of the Secret containing HAProxy Dataplane API credentials <code>VERBOSE</code> (none) <code>1</code> Log level: <code>0</code>=WARNING, <code>1</code>=INFO, <code>2</code>=DEBUG <code>DEBUG_PORT</code> <code>--debug-port</code> <code>0</code> Port for debug HTTP server (0 to disable) <p>Priority Order: CLI flags &gt; Environment variables &gt; Defaults</p> <p>Additional Flags:</p> <ul> <li><code>--kubeconfig</code> - Path to kubeconfig file (for out-of-cluster development)</li> </ul> <p>Example:</p> <pre><code># Using environment variables\nexport CONFIGMAP_NAME=my-haproxy-config\nexport SECRET_NAME=my-haproxy-credentials\nexport VERBOSE=2\n./controller\n\n# Using CLI flags\n./controller --configmap-name=my-haproxy-config --secret-name=my-haproxy-credentials\n</code></pre>"},{"location":"configuration/#configmap-configuration","title":"ConfigMap Configuration","text":"<p>The ConfigMap must contain a <code>config</code> key with YAML configuration.</p>"},{"location":"configuration/#complete-field-reference","title":"Complete Field Reference","text":""},{"location":"configuration/#pod_selector","title":"<code>pod_selector</code>","text":"<p>Identifies which HAProxy pods to configure.</p> Field Type Required Description <code>match_labels</code> map[string]string Yes Labels to match HAProxy pods <p>Example:</p> <pre><code>pod_selector:\n  match_labels:\n    app: haproxy\n    component: loadbalancer\n</code></pre>"},{"location":"configuration/#controller","title":"<code>controller</code>","text":"<p>Controller-level settings.</p> Field Type Default Description <code>healthz_port</code> int <code>8080</code> Port for health check endpoints (<code>/healthz</code>, <code>/readyz</code>) <code>metrics_port</code> int <code>9090</code> Port for Prometheus metrics endpoint (<code>/metrics</code>) <p>Example:</p> <pre><code>controller:\n  healthz_port: 8080\n  metrics_port: 9090\n</code></pre>"},{"location":"configuration/#logging","title":"<code>logging</code>","text":"<p>Logging configuration.</p> Field Type Default Description <code>verbose</code> int <code>1</code> Log level: <code>0</code>=WARNING, <code>1</code>=INFO, <code>2</code>=DEBUG <p>Example:</p> <pre><code>logging:\n  verbose: 1  # INFO level\n</code></pre>"},{"location":"configuration/#dataplane","title":"<code>dataplane</code>","text":"<p>HAProxy Dataplane API configuration and file paths.</p> Field Type Default Description <code>port</code> int <code>5555</code> Dataplane API port for production HAProxy pods <code>min_deployment_interval</code> string <code>2s</code> Minimum time between consecutive deployments (Go duration) <code>drift_prevention_interval</code> string <code>60s</code> Interval for periodic drift prevention deployments (Go duration) <code>maps_dir</code> string <code>/etc/haproxy/maps</code> Directory for HAProxy map files <code>ssl_certs_dir</code> string <code>/etc/haproxy/ssl</code> Directory for SSL certificates <code>general_storage_dir</code> string <code>/etc/haproxy/general</code> Directory for general files (error pages, etc.) <code>config_file</code> string <code>/etc/haproxy/haproxy.cfg</code> Path to main HAProxy configuration file <p>Example:</p> <pre><code>dataplane:\n  port: 5555\n  min_deployment_interval: 2s\n  drift_prevention_interval: 60s\n  maps_dir: /etc/haproxy/maps\n  ssl_certs_dir: /etc/haproxy/ssl\n  general_storage_dir: /etc/haproxy/general\n  config_file: /etc/haproxy/haproxy.cfg\n</code></pre> <p>Notes:</p> <ul> <li>Paths are used for both validation and deployment</li> <li>Paths must match HAProxy Dataplane API resource configuration</li> <li>Duration strings use Go format: <code>500ms</code>, <code>2s</code>, <code>5m</code>, <code>1h</code></li> </ul>"},{"location":"configuration/#templating_settings","title":"<code>templating_settings</code>","text":"<p>Template rendering configuration and custom variables.</p> Field Type Default Description <code>extra_context</code> map[string]interface{} <code>{}</code> Custom variables merged into template context (available in all templates) <p>Example:</p> <pre><code>templating_settings:\n  extra_context:\n    debug:\n      enabled: true\n      verbose_headers: false\n    environment: production\n    feature_flags:\n      rate_limiting: true\n      caching: false\n    custom_timeout: 30\n</code></pre> <p>Usage in templates:</p> <p>Custom variables are merged at the top level of the template context. Access them directly:</p> <pre><code>{% if debug.enabled %}\n  # Debug-specific configuration\n  http-response set-header X-HAProxy-Backend %[be_name]\n{% endif %}\n\n{% if environment == \"production\" %}\n  timeout client {{ custom_timeout }}s\n{% endif %}\n\n{% if feature_flags.rate_limiting %}\n  # Rate limiting configuration\n  stick-table type ip size 100k expire 30s store http_req_rate(10s)\n{% endif %}\n</code></pre> <p>See Templating Guide for detailed examples and use cases.</p>"},{"location":"configuration/#watched_resources_ignore_fields","title":"<code>watched_resources_ignore_fields</code>","text":"<p>JSONPath expressions for fields to remove from watched resources to reduce memory usage.</p> Type Default Description []string <code>[]</code> List of JSONPath expressions <p>Example:</p> <pre><code>watched_resources_ignore_fields:\n  - metadata.managedFields\n  - metadata.annotations[\"kubectl.kubernetes.io/last-applied-configuration\"]\n</code></pre>"},{"location":"configuration/#watched_resources","title":"<code>watched_resources</code>","text":"<p>Defines which Kubernetes resources to watch and how to index them.</p> Field Type Required Description <code>api_version</code> string Yes Kubernetes API version (e.g., <code>networking.k8s.io/v1</code>) <code>kind</code> string Yes Kubernetes resource kind (e.g., <code>Ingress</code>) <code>enable_validation_webhook</code> bool No (default: <code>false</code>) Enable admission webhook validation for this resource <code>index_by</code> []string Yes JSONPath expressions for extracting index keys <code>label_selector</code> map[string]string No Filter resources by labels (server-side filtering) <p>Example:</p> <pre><code>watched_resources:\n  ingresses:\n    api_version: networking.k8s.io/v1\n    kind: Ingress\n    index_by:\n      - metadata.namespace\n      - metadata.name\n    label_selector:\n      app: myapp\n\n  services:\n    api_version: v1\n    kind: Service\n    index_by:\n      - metadata.namespace\n      - metadata.name\n\n  endpointslices:\n    api_version: discovery.k8s.io/v1\n    kind: EndpointSlice\n    index_by:\n      - metadata.namespace\n      - metadata.labels['kubernetes.io/service-name']\n</code></pre>"},{"location":"configuration/#template_snippets","title":"<code>template_snippets</code>","text":"<p>Reusable template fragments that can be included in other templates.</p> Field Type Required Description <code>name</code> string Yes Snippet identifier for <code>{% include \"name\" %}</code> <code>template</code> string Yes Template content <p>Example:</p> <pre><code>template_snippets:\n  logging-config:\n    name: logging-config\n    template: |\n      log stdout local0 info\n      log stdout local1 notice\n\n  ssl-options:\n    name: ssl-options\n    template: |\n      ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256\n      ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets\n</code></pre> <p>Use in templates:</p> <pre><code>global\n    {% include \"logging-config\" %}\n    {% include \"ssl-options\" %}\n</code></pre>"},{"location":"configuration/#maps","title":"<code>maps</code>","text":"<p>HAProxy map file templates. Map files are used for dynamic routing, ACL matching, and other lookups.</p> Field Type Required Description <code>template</code> string Yes Template content that generates the map file <p>Example:</p> <pre><code>maps:\n  host.map:\n    template: |\n      # Generated map file for host-based routing\n      {% for ingress in ingresses %}\n      {%- for rule in ingress.spec.rules %}\n      {{ rule.host }} {{ ingress.metadata.name }}-backend\n      {% endfor %}\n      {%- endfor %}\n</code></pre> <p>Map file names are used as keys (e.g., <code>host.map</code>).</p> <p>Use in HAProxy config with <code>pathResolver.GetPath()</code> method:</p> <pre><code>use_backend %[req.hdr(host),lower,map({{ pathResolver.GetPath(\"host.map\", \"map\") }})]\n</code></pre>"},{"location":"configuration/#files","title":"<code>files</code>","text":"<p>General-purpose auxiliary file templates (error pages, etc.).</p> Field Type Required Description <code>template</code> string Yes Template content that generates the file <p>Example:</p> <pre><code>files:\n  404.http:\n    template: |\n      HTTP/1.1 404 Not Found\n      Content-Type: text/html\n      Cache-Control: no-cache\n      Connection: close\n\n      &lt;!DOCTYPE html&gt;\n      &lt;html&gt;\n      &lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n      &lt;body&gt;\n        &lt;h1&gt;404 Not Found&lt;/h1&gt;\n        &lt;p&gt;The requested resource was not found.&lt;/p&gt;\n      &lt;/body&gt;\n      &lt;/html&gt;\n\n  503.http:\n    template: |\n      HTTP/1.1 503 Service Unavailable\n      Content-Type: text/html\n      Cache-Control: no-cache\n      Connection: close\n\n      &lt;!DOCTYPE html&gt;\n      &lt;html&gt;\n      &lt;head&gt;&lt;title&gt;503 Service Unavailable&lt;/title&gt;&lt;/head&gt;\n      &lt;body&gt;\n        &lt;h1&gt;503 Service Unavailable&lt;/h1&gt;\n        &lt;p&gt;The service is temporarily unavailable.&lt;/p&gt;\n      &lt;/body&gt;\n      &lt;/html&gt;\n</code></pre> <p>Use in HAProxy config with <code>pathResolver.GetPath()</code> method:</p> <pre><code>errorfile 404 {{ pathResolver.GetPath(\"404.http\", \"file\") }}\nerrorfile 503 {{ pathResolver.GetPath(\"503.http\", \"file\") }}\n</code></pre>"},{"location":"configuration/#ssl_certificates","title":"<code>ssl_certificates</code>","text":"<p>SSL certificate file templates. Certificates are typically sourced from Kubernetes Secrets.</p> Field Type Required Description <code>template</code> string Yes Template content that generates the certificate file <p>Example:</p> <pre><code>ssl_certificates:\n  example.com.pem:\n    template: |\n      {%- for secret in secrets %}\n      {%- if secret.metadata.name == \"example-com-tls\" %}\n      {{ secret.data['tls.crt'] | b64decode }}\n      {{ secret.data['tls.key'] | b64decode }}\n      {%- endif %}\n      {%- endfor %}\n\n  wildcard.example.com.pem:\n    template: |\n      {%- for secret in secrets %}\n      {%- if secret.metadata.name == \"wildcard-example-com-tls\" %}\n      {{ secret.data['tls.crt'] | b64decode }}\n      {{ secret.data['tls.key'] | b64decode }}\n      {%- endif %}\n      {%- endfor %}\n</code></pre> <p>Use in HAProxy config with <code>pathResolver.GetPath()</code> method:</p> <pre><code>bind :443 ssl crt {{ pathResolver.GetPath(\"example.com.pem\", \"cert\") }}\n</code></pre>"},{"location":"configuration/#haproxy_config","title":"<code>haproxy_config</code>","text":"<p>Main HAProxy configuration template.</p> Field Type Required Description <code>template</code> string Yes Main HAProxy configuration template <p>Example:</p> <pre><code>haproxy_config:\n  template: |\n    global\n        daemon\n        maxconn 2000\n        {% include \"logging-config\" %}\n        {% include \"ssl-options\" %}\n\n    defaults\n        mode http\n        timeout connect 5s\n        timeout client 30s\n        timeout server 30s\n        errorfile 404 {{ pathResolver.GetPath(\"404.http\", \"file\") }}\n        errorfile 503 {{ pathResolver.GetPath(\"503.http\", \"file\") }}\n\n    frontend http\n        bind :80\n        use_backend %[req.hdr(host),lower,map({{ pathResolver.GetPath(\"host.map\", \"map\") }})]\n\n    frontend https\n        bind :443 ssl crt {{ pathResolver.GetPath(\"example.com.pem\", \"cert\") }}\n        use_backend %[req.hdr(host),lower,map({{ pathResolver.GetPath(\"host.map\", \"map\") }})]\n\n    {% for ingress in ingresses %}\n    backend {{ ingress.metadata.name }}-backend\n        balance roundrobin\n        {%- for rule in ingress.spec.rules %}\n        {%- for path in rule.http.paths %}\n        {%- set service_name = path.backend.service.name %}\n        {%- set service_port = path.backend.service.port.number %}\n        {%- for slice in endpointslices %}\n        {%- if slice.metadata.labels['kubernetes.io/service-name'] == service_name %}\n        {%- for endpoint in slice.endpoints %}\n        {%- if endpoint.conditions.ready %}\n        {%- for address in endpoint.addresses %}\n        server {{ address }}_{{ service_port }} {{ address }}:{{ service_port }} check\n        {%- endfor %}\n        {%- endif %}\n        {%- endfor %}\n        {%- endif %}\n        {%- endfor %}\n        {%- endfor %}\n        {%- endfor %}\n    {% endfor %}\n</code></pre> <p>See templating.md for detailed template syntax and available filters.</p>"},{"location":"configuration/#complete-configmap-example","title":"Complete ConfigMap Example","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: haproxy-config\n  namespace: default\ndata:\n  config: |\n    # Pod selector\n    pod_selector:\n      match_labels:\n        app: haproxy\n        component: loadbalancer\n\n    # Controller settings\n    controller:\n      healthz_port: 8080\n      metrics_port: 9090\n\n    # Logging configuration\n    logging:\n      verbose: 1\n\n    # Dataplane API configuration\n    dataplane:\n      port: 5555\n      min_deployment_interval: 2s\n      drift_prevention_interval: 60s\n      maps_dir: /etc/haproxy/maps\n      ssl_certs_dir: /etc/haproxy/ssl\n      general_storage_dir: /etc/haproxy/general\n      config_file: /etc/haproxy/haproxy.cfg\n\n    # Resource watching configuration\n    watched_resources_ignore_fields:\n      - metadata.managedFields\n\n    watched_resources:\n      ingresses:\n        api_version: networking.k8s.io/v1\n        kind: Ingress\n        index_by:\n          - metadata.namespace\n          - metadata.name\n\n      services:\n        api_version: v1\n        kind: Service\n        index_by:\n          - metadata.namespace\n          - metadata.name\n\n      endpointslices:\n        api_version: discovery.k8s.io/v1\n        kind: EndpointSlice\n        index_by:\n          - metadata.namespace\n          - metadata.labels['kubernetes.io/service-name']\n\n      secrets:\n        api_version: v1\n        kind: Secret\n        index_by:\n          - metadata.namespace\n          - metadata.name\n        label_selector:\n          cert-type: tls\n\n    # Template snippets\n    template_snippets:\n      logging-config:\n        name: logging-config\n        template: |\n          log stdout local0 info\n          log stdout local1 notice\n\n      ssl-options:\n        name: ssl-options\n        template: |\n          ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256\n          ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets\n\n    # Map files\n    maps:\n      host.map:\n        template: |\n          # Host-based routing map\n          {% for ingress in ingresses %}\n          {%- for rule in ingress.spec.rules %}\n          {{ rule.host }} {{ ingress.metadata.name }}-backend\n          {% endfor %}\n          {%- endfor %}\n\n    # Auxiliary files\n    files:\n      404.http:\n        template: |\n          HTTP/1.1 404 Not Found\n          Content-Type: text/html\n\n          &lt;!DOCTYPE html&gt;\n          &lt;html&gt;&lt;body&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\n\n    # SSL certificates\n    ssl_certificates:\n      example.com.pem:\n        template: |\n          {%- for secret in secrets %}\n          {%- if secret.metadata.name == \"example-com-tls\" %}\n          {{ secret.data['tls.crt'] | b64decode }}\n          {{ secret.data['tls.key'] | b64decode }}\n          {%- endif %}\n          {%- endfor %}\n\n    # Main HAProxy configuration\n    haproxy_config:\n      template: |\n        global\n            daemon\n            maxconn 2000\n            {% include \"logging-config\" %}\n            {% include \"ssl-options\" %}\n\n        defaults\n            mode http\n            timeout connect 5s\n            timeout client 30s\n            timeout server 30s\n            errorfile 404 {{ pathResolver.GetPath(\"404.http\", \"file\") }}\n\n        frontend http\n            bind :80\n            use_backend %[req.hdr(host),lower,map({{ pathResolver.GetPath(\"host.map\", \"map\") }})]\n\n        frontend https\n            bind :443 ssl crt {{ pathResolver.GetPath(\"example.com.pem\", \"cert\") }}\n            use_backend %[req.hdr(host),lower,map({{ pathResolver.GetPath(\"host.map\", \"map\") }})]\n\n        {% for ingress in ingresses %}\n        backend {{ ingress.metadata.name }}-backend\n            balance roundrobin\n            {%- for rule in ingress.spec.rules %}\n            {%- for path in rule.http.paths %}\n            {%- set service_name = path.backend.service.name %}\n            {%- set service_port = path.backend.service.port.number %}\n            {%- for slice in endpointslices %}\n            {%- if slice.metadata.labels['kubernetes.io/service-name'] == service_name %}\n            {%- for endpoint in slice.endpoints %}\n            {%- if endpoint.conditions.ready %}\n            {%- for address in endpoint.addresses %}\n            server {{ address }}_{{ service_port }} {{ address }}:{{ service_port }} check\n            {%- endfor %}\n            {%- endif %}\n            {%- endfor %}\n            {%- endif %}\n            {%- endfor %}\n            {%- endfor %}\n            {%- endfor %}\n        {% endfor %}\n</code></pre>"},{"location":"configuration/#secret-configuration","title":"Secret Configuration","text":"<p>The Secret contains credentials for authenticating with the HAProxy Dataplane API.</p>"},{"location":"configuration/#required-keys","title":"Required Keys","text":"Key Type Required Description <code>dataplane_username</code> string Yes Username for HAProxy Dataplane API authentication <code>dataplane_password</code> string Yes Password for HAProxy Dataplane API authentication"},{"location":"configuration/#complete-secret-example","title":"Complete Secret Example","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: haproxy-credentials\n  namespace: default\ntype: Opaque\ndata:\n  # Base64 encoded credentials\n  # echo -n \"admin\" | base64\n  dataplane_username: YWRtaW4=\n  # echo -n \"your-secure-password\" | base64\n  dataplane_password: eW91ci1zZWN1cmUtcGFzc3dvcmQ=\n</code></pre>"},{"location":"configuration/#security-notes","title":"Security Notes","text":"<ol> <li> <p>Base64 Encoding: Kubernetes Secrets automatically base64-encode values. When creating Secrets via kubectl or    YAML, provide base64-encoded values in the <code>data</code> field, or use <code>stringData</code> for plain text (Kubernetes will encode    it).</p> </li> <li> <p>Secret Best Practices:</p> <ul> <li>Use RBAC to restrict Secret access</li> <li>Consider using encrypted Secrets (e.g., sealed-secrets, external-secrets)</li> <li>Rotate credentials regularly</li> <li>Never commit Secrets to version control</li> </ul> </li> <li> <p>Creating Secrets:</p> </li> </ol> <p>Using kubectl:</p> <pre><code>kubectl create secret generic haproxy-credentials \\\n  --from-literal=dataplane_username=admin \\\n  --from-literal=dataplane_password=your-secure-password\n</code></pre> <p>Using YAML with stringData (unencoded):</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: haproxy-credentials\n  namespace: default\ntype: Opaque\nstringData:\n  dataplane_username: admin\n  dataplane_password: your-secure-password\n</code></pre>"},{"location":"configuration/#rbac-configuration","title":"RBAC Configuration","text":"<p>The controller requires specific Kubernetes permissions to function correctly.</p>"},{"location":"configuration/#required-permissions","title":"Required Permissions","text":"<p>The controller needs the following permissions:</p> Resource API Group Verbs Reason <code>configmaps</code> `` (core) <code>get</code>, <code>list</code>, <code>watch</code> Read controller configuration <code>secrets</code> `` (core) <code>get</code>, <code>list</code>, <code>watch</code> Read API credentials and TLS certificates <code>services</code> `` (core) <code>get</code>, <code>list</code>, <code>watch</code> Watch Service resources for backend configuration <code>pods</code> `` (core) <code>get</code>, <code>list</code>, <code>watch</code> Discover HAProxy pods and endpoints <code>namespaces</code> `` (core) <code>get</code>, <code>list</code>, <code>watch</code> Support cross-namespace resource watching <code>ingresses</code> <code>networking.k8s.io</code> <code>get</code>, <code>list</code>, <code>watch</code> Watch Ingress resources for routing configuration <code>endpointslices</code> <code>discovery.k8s.io</code> <code>get</code>, <code>list</code>, <code>watch</code> Watch EndpointSlice for backend server discovery <p>Note: Additional resource permissions may be required based on your <code>watched_resources</code> configuration.</p>"},{"location":"configuration/#complete-rbac-manifests","title":"Complete RBAC Manifests","text":""},{"location":"configuration/#serviceaccount","title":"ServiceAccount","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: haproxy-template-ic\n  namespace: default\n</code></pre>"},{"location":"configuration/#clusterrole","title":"ClusterRole","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: haproxy-template-ic\nrules:\n  # Core resources - required for controller operation\n  - apiGroups: [ \"\" ]\n    resources:\n      - configmaps     # Controller configuration\n      - secrets        # API credentials and TLS certificates\n      - services       # Backend service discovery\n      - pods           # HAProxy pod discovery\n      - namespaces     # Cross-namespace resource watching\n    verbs: [ \"get\", \"list\", \"watch\" ]\n\n  # Ingress resources - routing configuration\n  - apiGroups: [ \"networking.k8s.io\" ]\n    resources:\n      - ingresses\n    verbs: [ \"get\", \"list\", \"watch\" ]\n\n  # EndpointSlice - backend server discovery\n  - apiGroups: [ \"discovery.k8s.io\" ]\n    resources:\n      - endpointslices\n    verbs: [ \"get\", \"list\", \"watch\" ]\n</code></pre>"},{"location":"configuration/#clusterrolebinding","title":"ClusterRoleBinding","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: haproxy-template-ic\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: haproxy-template-ic\nsubjects:\n  - kind: ServiceAccount\n    name: haproxy-template-ic\n    namespace: default\n</code></pre>"},{"location":"configuration/#namespace-scoped-alternative","title":"Namespace-Scoped Alternative","text":"<p>For single-namespace deployments, use Role instead of ClusterRole:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: haproxy-template-ic\n  namespace: default\nrules:\n  - apiGroups: [ \"\" ]\n    resources: [ \"configmaps\", \"secrets\", \"services\", \"pods\" ]\n    verbs: [ \"get\", \"list\", \"watch\" ]\n\n  - apiGroups: [ \"networking.k8s.io\" ]\n    resources: [ \"ingresses\" ]\n    verbs: [ \"get\", \"list\", \"watch\" ]\n\n  - apiGroups: [ \"discovery.k8s.io\" ]\n    resources: [ \"endpointslices\" ]\n    verbs: [ \"get\", \"list\", \"watch\" ]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: haproxy-template-ic\n  namespace: default\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: haproxy-template-ic\nsubjects:\n  - kind: ServiceAccount\n    name: haproxy-template-ic\n    namespace: default\n</code></pre> <p>Limitations: Namespace-scoped RBAC prevents watching resources in other namespaces.</p>"},{"location":"configuration/#deployment-integration","title":"Deployment Integration","text":""},{"location":"configuration/#controller-deployment","title":"Controller Deployment","text":"<p>Example Deployment manifest showing environment variable configuration:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: haproxy-template-ic\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: haproxy-template-ic\n  template:\n    metadata:\n      labels:\n        app: haproxy-template-ic\n      annotations:\n        # Trigger pod restart on config/secret changes\n        checksum/config: \"&lt;config-sha256&gt;\"\n        checksum/secret: \"&lt;secret-sha256&gt;\"\n    spec:\n      serviceAccountName: haproxy-template-ic\n      containers:\n        - name: controller\n          image: haproxy-template-ic:latest\n          imagePullPolicy: IfNotPresent\n\n          # Environment variables\n          env:\n            - name: CONFIGMAP_NAME\n              value: \"haproxy-config\"\n            - name: SECRET_NAME\n              value: \"haproxy-credentials\"\n            - name: VERBOSE\n              value: \"1\"\n            # Optionally enable debug server\n            # - name: DEBUG_PORT\n            #   value: \"6060\"\n\n          # Container ports\n          ports:\n            - name: healthz\n              containerPort: 8080\n              protocol: TCP\n            - name: metrics\n              containerPort: 9090\n              protocol: TCP\n\n          # Health probes\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            failureThreshold: 3\n\n          readinessProbe:\n            httpGet:\n              path: /readyz\n              port: healthz\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 3\n            failureThreshold: 3\n\n          # Resource limits\n          resources:\n            requests:\n              cpu: 100m\n              memory: 128Mi\n            limits:\n              cpu: 500m\n              memory: 512Mi\n\n          # Security context\n          securityContext:\n            runAsNonRoot: true\n            runAsUser: 65534\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop:\n                - ALL\n            readOnlyRootFilesystem: true\n</code></pre>"},{"location":"configuration/#service-for-metrics","title":"Service for Metrics","text":"<p>Expose metrics endpoint for Prometheus:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: haproxy-template-ic-metrics\n  namespace: default\n  labels:\n    app: haproxy-template-ic\nspec:\n  type: ClusterIP\n  ports:\n    - name: metrics\n      port: 9090\n      targetPort: metrics\n      protocol: TCP\n  selector:\n    app: haproxy-template-ic\n</code></pre>"},{"location":"configuration/#servicemonitor-optional","title":"ServiceMonitor (Optional)","text":"<p>For Prometheus Operator:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: haproxy-template-ic\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: haproxy-template-ic\n  endpoints:\n    - port: metrics\n      interval: 30s\n      path: /metrics\n</code></pre>"},{"location":"configuration/#configuration-validation","title":"Configuration Validation","text":"<p>The controller performs multi-stage validation:</p> <ol> <li>YAML Parsing: Validates YAML syntax when loading ConfigMap</li> <li>Structural Validation: Checks required fields, types, and value ranges</li> <li>Template Compilation: Pre-compiles all templates to catch syntax errors</li> <li>JSONPath Validation: Validates <code>index_by</code> expressions</li> <li>HAProxy Validation: Validates rendered HAProxy configuration using Dataplane API</li> </ol> <p>Configuration errors are logged and the controller will not apply invalid configurations. Check controller logs for detailed error messages.</p>"},{"location":"configuration/#further-reading","title":"Further Reading","text":"<ul> <li>Template Syntax and Filters - Detailed template documentation</li> <li>Supported HAProxy Configuration - HAProxy sections and child components</li> <li>Helm Chart Values - Production deployment examples</li> </ul>"},{"location":"configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configuration/#controller-not-starting","title":"Controller Not Starting","text":"<ol> <li> <p>Check environment variables are set correctly:    <pre><code>kubectl get deployment haproxy-template-ic -o jsonpath='{.spec.template.spec.containers[0].env}'\n</code></pre></p> </li> <li> <p>Verify ConfigMap exists and has correct key:    <pre><code>kubectl get configmap haproxy-config -o yaml\n# Should have data.config key with YAML content\n</code></pre></p> </li> <li> <p>Verify Secret exists and has required keys:    <pre><code>kubectl get secret haproxy-credentials -o jsonpath='{.data}' | jq 'keys'\n# Should include: dataplane_username, dataplane_password\n</code></pre></p> </li> <li> <p>Check RBAC permissions:    <pre><code>kubectl auth can-i get configmaps --as=system:serviceaccount:default:haproxy-template-ic\nkubectl auth can-i get secrets --as=system:serviceaccount:default:haproxy-template-ic\n</code></pre></p> </li> </ol>"},{"location":"configuration/#configuration-not-loading","title":"Configuration Not Loading","text":"<ol> <li> <p>Check controller logs for parsing errors:    <pre><code>kubectl logs -l app=haproxy-template-ic | grep -i \"config\"\n</code></pre></p> </li> <li> <p>Validate YAML syntax:    <pre><code>kubectl get configmap haproxy-config -o jsonpath='{.data.config}' | yq eval -\n</code></pre></p> </li> <li> <p>Check for validation errors:    <pre><code>kubectl logs -l app=haproxy-template-ic | grep -i \"validation\"\n</code></pre></p> </li> </ol>"},{"location":"configuration/#templates-not-rendering","title":"Templates Not Rendering","text":"<ol> <li> <p>Enable debug logging:    <pre><code>kubectl set env deployment/haproxy-template-ic VERBOSE=2\n</code></pre></p> </li> <li> <p>Check template compilation errors:    <pre><code>kubectl logs -l app=haproxy-template-ic | grep -i \"template\"\n</code></pre></p> </li> <li> <p>Verify watched resources are being indexed:    <pre><code>kubectl logs -l app=haproxy-template-ic | grep -i \"index\"\n</code></pre></p> </li> </ol>"},{"location":"configuration/#credentials-not-working","title":"Credentials Not Working","text":"<ol> <li> <p>Verify credentials are correctly base64 encoded:    <pre><code>kubectl get secret haproxy-credentials -o jsonpath='{.data.dataplane_username}' | base64 -d\n</code></pre></p> </li> <li> <p>Test credentials against HAProxy Dataplane API:    <pre><code># Port-forward to HAProxy pod\nkubectl port-forward pod/&lt;haproxy-pod&gt; 5555:5555\n\n# Test authentication\ncurl -u admin:password http://localhost:5555/v2/services/haproxy/configuration/version\n</code></pre></p> </li> <li> <p>Check for credential rotation events:    <pre><code>kubectl logs -l app=haproxy-template-ic | grep -i \"credentials\"\n</code></pre></p> </li> </ol>"},{"location":"crd-reference/","title":"HAProxyTemplateConfig CRD Reference","text":""},{"location":"crd-reference/#overview","title":"Overview","text":"<p>The <code>HAProxyTemplateConfig</code> custom resource is the recommended way to configure the HAProxy Template Ingress Controller. It provides a Kubernetes-native API with built-in validation, type safety, and embedded testing capabilities.</p> <p>Benefits over ConfigMap: - Schema validation catches errors before deployment - Status conditions provide feedback on configuration health - Native Kubernetes resource with proper RBAC - Better GitOps integration with declarative configuration</p> <p>API Group: <code>haproxy-template-ic.github.io</code> API Version: <code>v1alpha1</code> Kind: <code>HAProxyTemplateConfig</code> Short Names: <code>htplcfg</code>, <code>haptpl</code></p>"},{"location":"crd-reference/#basic-example","title":"Basic Example","text":"<pre><code>apiVersion: haproxy-template-ic.github.io/v1alpha1\nkind: HAProxyTemplateConfig\nmetadata:\n  name: haproxy-config\n  namespace: default\nspec:\n  credentialsSecretRef:\n    name: haproxy-credentials\n\n  podSelector:\n    matchLabels:\n      app: haproxy\n      component: loadbalancer\n\n  watchedResources:\n    ingresses:\n      apiVersion: networking.k8s.io/v1\n      resources: ingresses\n      indexBy:\n        - metadata.namespace\n        - metadata.name\n\n  haproxyConfig:\n    template: |\n      global\n          daemon\n      defaults\n          timeout connect 5s\n      frontend http\n          bind *:80\n</code></pre>"},{"location":"crd-reference/#spec-fields","title":"Spec Fields","text":""},{"location":"crd-reference/#credentialssecretref-required","title":"credentialsSecretRef (required)","text":"<p>References a Secret containing Dataplane API credentials.</p> <pre><code>credentialsSecretRef:\n  name: haproxy-credentials\n  namespace: default  # Optional, defaults to config namespace\n</code></pre> <p>Required Secret keys: - <code>dataplane_username</code> - Production Dataplane API username - <code>dataplane_password</code> - Production Dataplane API password - <code>validation_username</code> - Validation HAProxy username (if validation enabled) - <code>validation_password</code> - Validation HAProxy password (if validation enabled)</p>"},{"location":"crd-reference/#podselector-required","title":"podSelector (required)","text":"<p>Labels that identify which HAProxy pods the controller should manage.</p> <pre><code>podSelector:\n  matchLabels:\n    app: haproxy\n    component: loadbalancer\n</code></pre> <p>At least one label must be specified.</p>"},{"location":"crd-reference/#controller","title":"controller","text":"<p>Controller-level settings for ports and leader election.</p> <pre><code>controller:\n  healthzPort: 8080  # Health check endpoints\n  metricsPort: 9090  # Prometheus metrics\n\n  leaderElection:\n    enabled: true\n    leaseName: haproxy-template-ic-leader\n    leaseDuration: 60s\n    renewDeadline: 15s\n    retryPeriod: 5s\n</code></pre> <p>Defaults: - <code>healthzPort</code>: 8080 - <code>metricsPort</code>: 9090 - <code>leaderElection.enabled</code>: true - <code>leaderElection.leaseDuration</code>: 60s - <code>leaderElection.renewDeadline</code>: 15s - <code>leaderElection.retryPeriod</code>: 5s</p> <p>See High Availability for leader election details.</p>"},{"location":"crd-reference/#logging","title":"logging","text":"<p>Log level configuration.</p> <pre><code>logging:\n  verbose: 1  # 0=WARNING, 1=INFO, 2=DEBUG\n</code></pre>"},{"location":"crd-reference/#dataplane","title":"dataplane","text":"<p>Dataplane API connection and deployment settings.</p> <pre><code>dataplane:\n  port: 5555  # Dataplane API port\n  minDeploymentInterval: 2s  # Rate limiting\n  driftPreventionInterval: 60s  # Periodic sync\n  mapsDir: /etc/haproxy/maps\n  sslCertsDir: /etc/haproxy/ssl\n  generalStorageDir: /etc/haproxy/general\n  configFile: /etc/haproxy/haproxy.cfg\n</code></pre> <p>Paths must match Dataplane API resource configuration.</p>"},{"location":"crd-reference/#watchedresourcesignorefields","title":"watchedResourcesIgnoreFields","text":"<p>JSONPath expressions for fields to remove from all watched resources.</p> <pre><code>watchedResourcesIgnoreFields:\n  - metadata.managedFields\n  - metadata.annotations[\"kubectl.kubernetes.io/last-applied-configuration\"]\n</code></pre> <p>Reduces memory usage by filtering unnecessary data.</p>"},{"location":"crd-reference/#watchedresources-required","title":"watchedResources (required)","text":"<p>Defines which Kubernetes resources to watch.</p> <pre><code>watchedResources:\n  ingresses:\n    apiVersion: networking.k8s.io/v1\n    resources: ingresses\n    enableValidationWebhook: true  # Optional\n    indexBy:\n      - metadata.namespace\n      - metadata.name\n    labelSelector:  # Optional\n      app: myapp\n    namespace: production  # Optional, restricts to single namespace\n    store: full  # or \"on-demand\" for cached store\n</code></pre> <p>See Watching Resources for detailed configuration.</p>"},{"location":"crd-reference/#templatesnippets","title":"templateSnippets","text":"<p>Reusable template fragments.</p> <pre><code>templateSnippets:\n  backend-name: |\n    ing_{{ ingress.metadata.namespace }}_{{ ingress.metadata.name }}\n</code></pre> <p>Include in templates: <code>{% include \"backend-name\" %}</code></p>"},{"location":"crd-reference/#maps","title":"maps","text":"<p>HAProxy map file templates.</p> <pre><code>maps:\n  host.map:\n    template: |\n      {% for ingress in resources.ingresses.List() %}\n      {{ rule.host }} {{ ingress.metadata.name }}_backend\n      {% endfor %}\n</code></pre> <p>Reference in config: <code>{{ pathResolver.GetPath(\"host.map\", \"map\") }}</code></p>"},{"location":"crd-reference/#files","title":"files","text":"<p>General auxiliary files (error pages, etc.).</p> <pre><code>files:\n  error_503:\n    path: /etc/haproxy/errors/503.http\n    template: |\n      HTTP/1.1 503 Service Unavailable\n      &lt;html&gt;&lt;body&gt;&lt;h1&gt;503&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\n</code></pre> <p>Reference in config: <code>errorfile 503 {{ pathResolver.GetPath(\"error_503\", \"file\") }}</code></p>"},{"location":"crd-reference/#sslcertificates","title":"sslCertificates","text":"<p>SSL certificate templates.</p> <pre><code>sslCertificates:\n  example-com:\n    template: |\n      {% set secret = resources.secrets.GetSingle(\"default\", \"tls-cert\") %}\n      {{ secret.data['tls.crt'] | b64decode }}\n      {{ secret.data['tls.key'] | b64decode }}\n</code></pre> <p>Reference in config: <code>bind :443 ssl crt {{ pathResolver.GetPath(\"example-com\", \"cert\") }}</code></p>"},{"location":"crd-reference/#haproxyconfig-required","title":"haproxyConfig (required)","text":"<p>Main HAProxy configuration template.</p> <pre><code>haproxyConfig:\n  template: |\n    global\n        daemon\n        maxconn 4096\n\n    defaults\n        mode http\n        timeout connect 5s\n\n    frontend http\n        bind *:80\n        use_backend %[req.hdr(host),map({{ pathResolver.GetPath(\"host.map\", \"map\") }})]\n</code></pre> <p>See Templating Guide for syntax and filters.</p>"},{"location":"crd-reference/#templatingsettings","title":"templatingSettings","text":"<p>Template rendering configuration and custom variables.</p> <pre><code>templatingSettings:\n  extraContext:\n    debug:\n      enabled: true\n      verboseHeaders: false\n    environment: production\n    featureFlags:\n      rateLimiting: true\n      caching: false\n    customTimeout: 30\n</code></pre> <p>Fields:</p> Field Type Required Description <code>extraContext</code> map[string]interface{} No Custom variables merged into template context (available in all templates) <p>Usage in templates:</p> <p>Custom variables are merged at the top level of the template context. Access them directly:</p> <pre><code>{% if debug.enabled %}\n  # Debug-specific configuration\n  http-response set-header X-HAProxy-Backend %[be_name]\n{% endif %}\n\n{% if environment == \"production\" %}\n  timeout client {{ customTimeout }}s\n{% else %}\n  timeout client 300s\n{% endif %}\n</code></pre> <p>The <code>extraContext</code> field accepts any valid JSON value (strings, numbers, booleans, objects, arrays). This allows you to configure template behavior for different environments, enable feature flags, or inject custom metadata without modifying controller code.</p> <p>See Templating Guide - Custom Template Variables for detailed examples and use cases.</p>"},{"location":"crd-reference/#validationtests","title":"validationTests","text":"<p>Embedded validation tests (optional, used by webhook and CLI).</p> <pre><code>validationTests:\n  - name: test_basic_ingress\n    description: Validate basic ingress routing\n    fixtures:\n      ingresses:\n        - apiVersion: networking.k8s.io/v1\n          kind: Ingress\n          metadata:\n            name: test-ingress\n            namespace: default\n          spec:\n            rules:\n              - host: example.com\n                http:\n                  paths:\n                    - path: /\n                      pathType: Prefix\n                      backend:\n                        service:\n                          name: test-service\n                          port:\n                            number: 80\n    assertions:\n      - type: haproxy_valid\n        description: Generated config must be valid\n\n      - type: contains\n        target: haproxy_config\n        pattern: \"example.com\"\n        description: Config must include host\n</code></pre> <p>See CRD Validation Design for test framework details.</p>"},{"location":"crd-reference/#status-subresource","title":"Status Subresource","text":"<p>The controller updates the status field with validation results:</p> <pre><code>status:\n  observedGeneration: 1\n  lastValidated: \"2025-01-27T10:00:00Z\"\n  validationStatus: Valid  # Valid, Invalid, or Unknown\n  validationMessage: \"All validation tests passed\"\n  conditions:\n    - type: Ready\n      status: \"True\"\n      reason: ValidationSucceeded\n      lastTransitionTime: \"2025-01-27T10:00:00Z\"\n</code></pre>"},{"location":"crd-reference/#command-line-management","title":"Command-Line Management","text":""},{"location":"crd-reference/#view-configurations","title":"View Configurations","text":"<pre><code># List all configs\nkubectl get haproxytemplateconfig\nkubectl get htplcfg  # Short name\n\n# View specific config\nkubectl get htplcfg haproxy-config -o yaml\n\n# Watch for changes\nkubectl get htplcfg -w\n</code></pre>"},{"location":"crd-reference/#validate-before-applying","title":"Validate Before Applying","text":"<pre><code># Validate local file\ncontroller validate --config haproxy-config.yaml\n\n# Validate deployed config\ncontroller validate --name haproxy-config --namespace default\n</code></pre>"},{"location":"crd-reference/#edit-configuration","title":"Edit Configuration","text":"<pre><code># Interactive edit\nkubectl edit htplcfg haproxy-config\n\n# Apply from file\nkubectl apply -f haproxy-config.yaml\n\n# Patch specific fields\nkubectl patch htplcfg haproxy-config --type=merge -p '\nspec:\n  logging:\n    verbose: 2\n'\n</code></pre>"},{"location":"crd-reference/#migration-from-configmap","title":"Migration from ConfigMap","text":"<p>If upgrading from ConfigMap-based configuration:</p> <p>Old (ConfigMap): <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: haproxy-config\ndata:\n  config: |\n    pod_selector:\n      match_labels:\n        app: haproxy\n    # ... rest of YAML config\n</code></pre></p> <p>New (CRD): <pre><code>apiVersion: haproxy-template-ic.github.io/v1alpha1\nkind: HAProxyTemplateConfig\nmetadata:\n  name: haproxy-config\nspec:\n  credentialsSecretRef:\n    name: haproxy-credentials\n  podSelector:\n    matchLabels:\n      app: haproxy\n  # ... rest of configuration as spec fields\n</code></pre></p> <p>Key differences: - Configuration is now strongly typed with validation - Credentials moved to separate Secret reference - Field names use camelCase (e.g., <code>podSelector</code> vs <code>pod_selector</code>) - Validation tests can be embedded inline</p>"},{"location":"crd-reference/#validation","title":"Validation","text":"<p>The CRD includes OpenAPI schema validation that checks: - Required fields are present - Field types are correct - String lengths meet minimum/maximum requirements - Integer values are within valid ranges - Enum values match allowed options</p> <p>Additional validation occurs when: 1. Admission webhook - Runs embedded validation tests (if webhook enabled) 2. Controller startup - Validates configuration before starting 3. CLI command - <code>controller validate</code> runs tests locally</p>"},{"location":"crd-reference/#best-practices","title":"Best Practices","text":"<p>Security: - Never include credentials in the CRD - use credentialsSecretRef - Restrict RBAC access to HAProxyTemplateConfig resources - Use separate namespaces for controller and configs in multi-tenant scenarios</p> <p>Organization: - One HAProxyTemplateConfig per controller instance - Use descriptive names that indicate purpose or environment - Label configs for filtering: <code>environment: production</code></p> <p>Testing: - Include validation tests for critical routing paths - Test with realistic fixtures, not toy examples - Run <code>controller validate</code> before applying changes - Use CI/CD to validate configs in pull requests</p> <p>Templates: - Use templateSnippets for reusable logic - Keep haproxyConfig template focused on structure - Comment complex template logic - Test templates with various resource combinations</p>"},{"location":"crd-reference/#see-also","title":"See Also","text":"<ul> <li>Configuration Reference - Detailed field descriptions</li> <li>Templating Guide - Template syntax and examples</li> <li>Watching Resources - Resource watching configuration</li> <li>CRD Validation Design - Validation framework</li> <li>Getting Started - Installation and basic usage</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#overview","title":"Overview","text":"<p>This guide walks you through deploying the HAProxy Template Ingress Controller and creating your first template-driven configuration. You'll learn how to:</p> <ul> <li>Deploy HAProxy pods with Dataplane API sidecars</li> <li>Install the controller using Helm</li> <li>Create a basic Ingress configuration</li> <li>Verify the deployment and test routing</li> </ul> <p>The entire process takes approximately 15 minutes on a local Kubernetes cluster.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (1.19+) - kind, minikube, or cloud provider</li> <li>kubectl configured to access your cluster</li> <li>Helm 3.0+</li> </ul>"},{"location":"getting-started/#step-1-deploy-haproxy-with-dataplane-api","title":"Step 1: Deploy HAProxy with Dataplane API","text":"<p>The controller requires HAProxy pods running with Dataplane API sidecars. These pods serve as the load balancers that the controller will configure.</p>"},{"location":"getting-started/#understanding-the-architecture","title":"Understanding the Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      HAProxy Pod                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 HAProxy  \u2502  \u2502  Dataplane  \u2502  \u2502\n\u2502  \u2502  Process \u2502\u25c4\u2500\u2524  API Sidecar\u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502       \u25b2              \u25b2           \u2502\n\u2502       \u2502              \u2502           \u2502\n\u2502    haproxy.cfg    :5555 API     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502              \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Template Controller\u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/#create-haproxy-deployment","title":"Create HAProxy Deployment","text":"<p>Apply this manifest to create a basic HAProxy deployment:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: haproxy\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: haproxy\n      component: loadbalancer\n  template:\n    metadata:\n      labels:\n        app: haproxy\n        component: loadbalancer\n    spec:\n      containers:\n      # Main HAProxy container\n      - name: haproxy\n        image: haproxytech/haproxy-debian:3.2\n        command: [\"/bin/sh\", \"-c\"]\n        args:\n          - |\n            # Create required directories\n            mkdir -p /etc/haproxy/maps /etc/haproxy/ssl /etc/haproxy/general\n\n            # Create minimal bootstrap configuration\n            cat &gt; /etc/haproxy/haproxy.cfg &lt;&lt;EOF\n            global\n                log stdout format raw local0 info\n                daemon\n\n            defaults\n                timeout connect 5s\n                timeout client 30s\n                timeout server 30s\n\n            frontend bootstrap\n                bind *:8404\n                http-request return status 200 if { path /healthz }\n            EOF\n\n            # Start HAProxy in master-worker mode\n            exec haproxy -W -db -S \"/etc/haproxy/haproxy-master.sock,level,admin\" -f /etc/haproxy/haproxy.cfg\n        volumeMounts:\n        - name: haproxy-config\n          mountPath: /etc/haproxy\n        ports:\n        - containerPort: 80\n          name: http\n        - containerPort: 443\n          name: https\n        - containerPort: 8404\n          name: stats\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8404\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8404\n          initialDelaySeconds: 5\n          periodSeconds: 5\n\n      # Dataplane API sidecar\n      - name: dataplane\n        image: haproxytech/haproxy-debian:3.2\n        command: [\"/bin/sh\", \"-c\"]\n        args:\n          - |\n            # Wait for HAProxy master socket\n            while [ ! -S /etc/haproxy/haproxy-master.sock ]; do\n              echo \"Waiting for HAProxy master socket...\"\n              sleep 1\n            done\n\n            # Create Dataplane API configuration\n            cat &gt; /etc/haproxy/dataplaneapi.yaml &lt;&lt;'EOF'\n            config_version: 2\n            name: haproxy-dataplaneapi\n            dataplaneapi:\n              host: 0.0.0.0\n              port: 5555\n              user:\n                - name: admin\n                  password: adminpass\n                  insecure: true\n              transaction:\n                transaction_dir: /var/lib/dataplaneapi/transactions\n                backups_number: 10\n                backups_dir: /var/lib/dataplaneapi/backups\n              resources:\n                maps_dir: /etc/haproxy/maps\n                ssl_certs_dir: /etc/haproxy/ssl\n                general_storage_dir: /etc/haproxy/general\n            haproxy:\n              config_file: /etc/haproxy/haproxy.cfg\n              haproxy_bin: /usr/local/sbin/haproxy\n              master_worker_mode: true\n              master_runtime: /etc/haproxy/haproxy-master.sock\n              reload:\n                reload_delay: 1\n                reload_cmd: /bin/sh -c \"echo 'reload' | socat stdio unix-connect:/etc/haproxy/haproxy-master.sock\"\n                restart_cmd: /bin/sh -c \"echo 'reload' | socat stdio unix-connect:/etc/haproxy/haproxy-master.sock\"\n                reload_strategy: custom\n            log_targets:\n              - log_to: stdout\n                log_level: info\n            EOF\n\n            # Start Dataplane API\n            exec dataplaneapi -f /etc/haproxy/dataplaneapi.yaml\n        volumeMounts:\n        - name: haproxy-config\n          mountPath: /etc/haproxy\n        ports:\n        - containerPort: 5555\n          name: dataplane-api\n\n      volumes:\n      - name: haproxy-config\n        emptyDir: {}\n</code></pre> <p>Save this as <code>haproxy-deployment.yaml</code> and apply:</p> <pre><code>kubectl apply -f haproxy-deployment.yaml\n</code></pre> <p>Verify the pods are running:</p> <pre><code>kubectl get pods -l app=haproxy\n</code></pre> <p>You should see two pods with <code>2/2 RUNNING</code>.</p>"},{"location":"getting-started/#step-2-install-the-controller","title":"Step 2: Install the Controller","text":"<p>Install the controller using Helm with default configuration:</p> <pre><code># Add the Helm repository (if published)\n# helm repo add haproxy-template-ic https://haproxy-template-ic.github.io/haproxy-template-ingress-controller\n# helm repo update\n\n# Install from local chart\nhelm install haproxy-ic ./charts/haproxy-template-ic \\\n  --set credentials.dataplane.username=admin \\\n  --set credentials.dataplane.password=adminpass\n</code></pre> <p>The default Helm installation: - Runs 2 replicas with leader election enabled for high availability - Creates a HAProxyTemplateConfig CRD resource with basic Ingress watching - Sets up RBAC permissions for watching Ingress, Service, and EndpointSlice resources - Configures the controller to find HAProxy pods using labels <code>app=haproxy, component=loadbalancer</code></p> <p>Verify the controller is running:</p> <pre><code>kubectl get pods -l app.kubernetes.io/name=haproxy-template-ic\nkubectl logs -l app.kubernetes.io/name=haproxy-template-ic --tail=20\n</code></pre> <p>You should see logs indicating the controller has started and is watching resources.</p>"},{"location":"getting-started/#step-3-deploy-a-sample-application","title":"Step 3: Deploy a Sample Application","text":"<p>Create a simple echo service to test routing:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      containers:\n      - name: echo\n        image: ealen/echo-server:latest\n        ports:\n        - containerPort: 80\n        env:\n        - name: PORT\n          value: \"80\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: echo\n  namespace: default\nspec:\n  selector:\n    app: echo\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre> <p>Save as <code>echo-app.yaml</code> and apply:</p> <pre><code>kubectl apply -f echo-app.yaml\n</code></pre>"},{"location":"getting-started/#step-4-create-an-ingress-resource","title":"Step 4: Create an Ingress Resource","text":"<p>Create an Ingress resource that the controller will process:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: echo-ingress\n  namespace: default\nspec:\n  ingressClassName: haproxy-template-ic\n  rules:\n  - host: echo.example.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: echo\n            port:\n              number: 80\n</code></pre> <p>Save as <code>echo-ingress.yaml</code> and apply:</p> <pre><code>kubectl apply -f echo-ingress.yaml\n</code></pre>"},{"location":"getting-started/#step-5-verify-the-configuration","title":"Step 5: Verify the Configuration","text":""},{"location":"getting-started/#check-controller-logs","title":"Check Controller Logs","text":"<p>Watch the controller process the Ingress:</p> <pre><code>kubectl logs -l app.kubernetes.io/name=haproxy-template-ic --tail=50 -f\n</code></pre> <p>You should see log entries showing: - Ingress resource detected - Template rendering completed - Configuration validation passed - Deployment to HAProxy instances succeeded</p>"},{"location":"getting-started/#inspect-haproxy-configuration","title":"Inspect HAProxy Configuration","text":"<p>Verify the generated HAProxy configuration was deployed:</p> <pre><code># Get one of the HAProxy pods\nHAPROXY_POD=$(kubectl get pods -l app=haproxy -o jsonpath='{.items[0].metadata.name}')\n\n# View the generated configuration\nkubectl exec $HAPROXY_POD -c haproxy -- cat /etc/haproxy/haproxy.cfg\n</code></pre> <p>You should see: - A frontend section with routing rules - A backend section referencing the echo service - Server entries pointing to the echo pod endpoints</p>"},{"location":"getting-started/#step-6-test-the-routing","title":"Step 6: Test the Routing","text":""},{"location":"getting-started/#port-forward-to-haproxy","title":"Port-Forward to HAProxy","text":"<pre><code>kubectl port-forward -n default deployment/haproxy 8080:80\n</code></pre>"},{"location":"getting-started/#test-the-endpoint","title":"Test the Endpoint","text":"<p>In another terminal:</p> <pre><code># Test with Host header\ncurl -H \"Host: echo.example.local\" http://localhost:8080/\n\n# You should receive a response from the echo server showing:\n# - Request headers\n# - Host information\n# - Environment variables\n</code></pre>"},{"location":"getting-started/#test-load-balancing","title":"Test Load Balancing","text":"<p>Make multiple requests to see load balancing across echo pods:</p> <pre><code>for i in {1..10}; do\n  curl -s -H \"Host: echo.example.local\" http://localhost:8080/ | grep -i hostname\ndone\n</code></pre> <p>You should see responses from different echo pods.</p>"},{"location":"getting-started/#whats-happening-behind-the-scenes","title":"What's Happening Behind the Scenes","text":"<p>When you created the Ingress resource, the controller:</p> <ol> <li>Detected the change via Kubernetes watch API</li> <li>Rendered templates using the default HAProxyTemplateConfig with your Ingress data</li> <li>Validated the configuration using HAProxy's native parser</li> <li>Compared with current state to determine what changed</li> <li>Deployed updates to all HAProxy pods via Dataplane API</li> <li>Used runtime API where possible (server addresses) to avoid reloads</li> </ol> <p>The entire process typically completes in under 1 second.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have a working setup, explore these topics:</p>"},{"location":"getting-started/#customize-the-configuration","title":"Customize the Configuration","text":"<p>The default configuration is generated from the HAProxyTemplateConfig CRD created by Helm. To customize:</p> <pre><code># View the current configuration\nkubectl get haproxytemplateconfig haproxy-template-ic-config -o yaml\n\n# Edit the configuration\nkubectl edit haproxytemplateconfig haproxy-template-ic-config\n</code></pre> <p>See Configuration Reference for all available options.</p>"},{"location":"getting-started/#template-customization","title":"Template Customization","text":"<p>Learn how to write custom templates for advanced HAProxy features:</p> <ul> <li>Path-based routing: Route requests based on URL paths</li> <li>SSL termination: Configure TLS certificates and HTTPS listeners</li> <li>Rate limiting: Add rate limits using stick tables</li> <li>Authentication: Enable HTTP basic auth on specific paths</li> <li>Custom error pages: Serve custom error responses</li> </ul> <p>See Templating Guide for template syntax and examples.</p>"},{"location":"getting-started/#watched-resources","title":"Watched Resources","text":"<p>Extend the controller to watch additional Kubernetes resources:</p> <ul> <li>EndpointSlices: Use actual pod IPs instead of service DNS</li> <li>Secrets: Load TLS certificates dynamically</li> <li>ConfigMaps: Inject custom HAProxy configuration snippets</li> <li>Custom CRDs: Define your own resource types</li> </ul> <p>See Watching Resources for configuration details.</p>"},{"location":"getting-started/#high-availability","title":"High Availability","text":"<p>Configure the controller for production deployments:</p> <ul> <li>Scale to 3+ replicas across availability zones</li> <li>Configure PodDisruptionBudgets</li> <li>Set up monitoring and alerting</li> <li>Enable leader election (already enabled by default)</li> </ul> <p>See High Availability for HA configuration.</p>"},{"location":"getting-started/#monitoring","title":"Monitoring","text":"<p>Set up Prometheus monitoring for the controller:</p> <pre><code># Enable ServiceMonitor if using Prometheus Operator\nhelm upgrade haproxy-ic ./charts/haproxy-template-ic \\\n  --reuse-values \\\n  --set monitoring.serviceMonitor.enabled=true \\\n  --set monitoring.serviceMonitor.interval=30s\n</code></pre> <p>See Monitoring Guide for metrics and dashboards.</p>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#controller-not-starting","title":"Controller Not Starting","text":"<p>Check the controller logs for errors:</p> <pre><code>kubectl logs -l app.kubernetes.io/name=haproxy-template-ic\n</code></pre> <p>Common issues: - Missing HAProxyTemplateConfig or Secret - Insufficient RBAC permissions - Cannot connect to Kubernetes API</p>"},{"location":"getting-started/#haproxy-pods-not-updating","title":"HAProxy Pods Not Updating","text":"<p>Verify the controller can connect to HAProxy Dataplane API:</p> <pre><code># Port-forward to Dataplane API\nkubectl port-forward $HAPROXY_POD 5555:5555\n\n# Test the API\ncurl -u admin:adminpass http://localhost:5555/v2/info\n</code></pre> <p>If this fails, check: - Dataplane API sidecar is running - Credentials match between controller and HAProxy - Master socket exists at <code>/etc/haproxy/haproxy-master.sock</code></p>"},{"location":"getting-started/#ingress-not-routing","title":"Ingress Not Routing","text":"<p>Check that: 1. The Ingress has <code>ingressClassName: haproxy-template-ic</code> 2. The Ingress is in the same namespace as watched resources 3. The backend Service exists and has endpoints</p> <pre><code># Check Ingress\nkubectl get ingress echo-ingress -o yaml\n\n# Check Service\nkubectl get service echo\n\n# Check Endpoints\nkubectl get endpointslices -l kubernetes.io/service-name=echo\n</code></pre> <p>For more troubleshooting guidance, see Troubleshooting Guide.</p>"},{"location":"getting-started/#clean-up","title":"Clean Up","text":"<p>Remove all resources created in this guide:</p> <pre><code># Remove Ingress\nkubectl delete -f echo-ingress.yaml\n\n# Remove echo application\nkubectl delete -f echo-app.yaml\n\n# Uninstall controller\nhelm uninstall haproxy-ic\n\n# Remove HAProxy deployment\nkubectl delete -f haproxy-deployment.yaml\n\n# Remove CRD (optional, removes all configs)\nkubectl delete crd haproxytemplateconfigs.haproxy-template-ic.github.io\n</code></pre>"},{"location":"getting-started/#see-also","title":"See Also","text":"<ul> <li>Configuration Reference - Complete configuration options</li> <li>Templating Guide - Template syntax and filters</li> <li>HAProxy Configuration - Supported HAProxy features</li> <li>Watching Resources - Resource watching configuration</li> <li>Helm Chart Documentation - Chart values and options</li> </ul>"},{"location":"supported-configuration/","title":"Supported HAProxy Configuration","text":"<p>This document provides an overview of HAProxy configuration sections and child components supported by the haproxy-template-ic via the HAProxy Dataplane API.</p>"},{"location":"supported-configuration/#overview","title":"Overview","text":"<p>The controller supports all configuration sections that can be managed through the HAProxy Dataplane API. Configuration changes are applied using fine-grained operations to minimize HAProxy reloads and maximize use of the Runtime API for zero-downtime updates.</p> <p>Coverage: - 15 main configuration sections (global, defaults, frontends, backends, etc.) - 23 child component types across frontends and backends (servers, ACLs, rules, etc.) - Complete Dataplane API compatibility - All manageable resources are supported</p>"},{"location":"supported-configuration/#supported-configuration-sections","title":"Supported Configuration Sections","text":"Section Description Priority Implementation Global Global HAProxy settings (singleton) 5 Update only Defaults Default settings for proxies 8 Create/Update/Delete Frontends Frontend proxy definitions 20 Create/Update/Delete Backends Backend server pools 30 Create/Update/Delete Peers Peer sections for stick-table replication 10 Create/Update/Delete Resolvers DNS resolver configurations 10 Create/Update/Delete Mailers Email alert configurations 10 Create/Update/Delete Caches Cache configurations 10 Create/Update/Delete Rings Ring buffer configurations 10 Create/Update/Delete HTTPErrors HTTP error response sections 10 Create/Update/Delete Userlists User authentication lists 10 Create/Delete (no update) Programs External program configurations 10 Create/Update/Delete LogForwards Syslog forwarding sections 10 Create/Update/Delete FCGIApps FastCGI application configs 10 Create/Update/Delete CrtStores Certificate store sections 10 Create/Update/Delete <p>Note: Lower priority numbers are processed first. Operations are automatically ordered by dependency and priority.</p>"},{"location":"supported-configuration/#child-components-by-section","title":"Child Components by Section","text":""},{"location":"supported-configuration/#frontend-child-components","title":"Frontend Child Components","text":"<p>Frontends support 9 child component types with individual Create/Update/Delete operations:</p> Component Description Code Reference Binds Listen addresses and ports <code>comparator.go:1191</code> ACLs Access control lists <code>comparator.go:744</code> HTTP Request Rules HTTP request processing rules <code>comparator.go:808</code> HTTP Response Rules HTTP response processing rules <code>comparator.go:857</code> TCP Request Rules TCP request processing rules <code>comparator.go:906</code> Backend Switching Rules Dynamic backend selection rules <code>comparator.go:1117</code> Filters Data filters (compression, trace, etc.) <code>comparator.go:1261</code> Captures Request/response capture declarations <code>comparator.go:1384</code> Log Targets Logging destinations <code>comparator.go:992</code>"},{"location":"supported-configuration/#backend-child-components","title":"Backend Child Components","text":"<p>Backends support 14 child component types with individual Create/Update/Delete operations:</p> Component Description Code Reference Servers Backend server definitions <code>comparator.go:338</code> Server Templates Dynamic server templates <code>comparator.go:1420</code> ACLs Access control lists <code>comparator.go:744</code> HTTP Request Rules HTTP request processing rules <code>comparator.go:808</code> HTTP Response Rules HTTP response processing rules <code>comparator.go:857</code> HTTP After Response Rules Post-response processing rules <code>comparator.go:1080</code> TCP Request Rules TCP request processing rules <code>comparator.go:906</code> TCP Response Rules TCP response processing rules <code>comparator.go:955</code> Server Switching Rules Dynamic server selection rules <code>comparator.go:1154</code> Stick Rules Session persistence rules <code>comparator.go:1042</code> Filters Data filters <code>comparator.go:1261</code> HTTP Checks HTTP health check configurations <code>comparator.go:1310</code> TCP Checks TCP health check configurations <code>comparator.go:1347</code> Log Targets Logging destinations <code>comparator.go:992</code>"},{"location":"supported-configuration/#mailers-resolvers-and-peers-child-components","title":"Mailers, Resolvers, and Peers Child Components","text":"<p>The following sections support fine-grained child component management:</p> <p>Mailers - Individual mailer entry operations:</p> Component Description Mailer Entries SMTP server definitions for email alerts <p>Resolvers - Individual nameserver operations:</p> Component Description Nameservers DNS server definitions for service discovery <p>Peers - Individual peer entry operations:</p> Component Description Peer Entries Peer server definitions for stick-table replication"},{"location":"supported-configuration/#other-section-components","title":"Other Section Components","text":"<p>The following sections use whole-section comparison via the models' <code>.Equal()</code> method, which includes all nested components:</p> <ul> <li>Rings: All ring attributes</li> <li>HTTPErrors: Includes errorfiles</li> <li>Userlists: Includes users and groups</li> <li>Programs: All program attributes</li> <li>LogForwards: Includes log targets</li> <li>FCGIApps: Includes pass-header and set-param directives</li> <li>CrtStores: Includes crt-load entries</li> </ul>"},{"location":"supported-configuration/#reload-behavior","title":"Reload Behavior","text":"<p>The controller minimizes HAProxy reloads by leveraging the Runtime API when possible. However, only specific operations can avoid reloads.</p>"},{"location":"supported-configuration/#zero-reload-operations-runtime-api","title":"Zero-Reload Operations (Runtime API)","text":"<p>The following changes are applied without reloading HAProxy:</p>"},{"location":"supported-configuration/#server-modifications-specific-fields-only","title":"Server Modifications (Specific Fields Only)","text":"<p>Server modifications avoid reloads only when changing these Runtime API-supported fields:</p> Field Description API Endpoint Weight Server weight for load balancing Runtime API <code>/runtime/servers</code> Address Server IP address Runtime API <code>/runtime/servers</code> Port Server port number Runtime API <code>/runtime/servers</code> Maintenance Enable/disable/drain server state Runtime API <code>/runtime/servers</code> AgentCheck Agent check status Runtime API <code>/runtime/servers</code> AgentAddr Agent check address Runtime API <code>/runtime/servers</code> AgentSend Agent check send string Runtime API <code>/runtime/servers</code> HealthCheckPort Health check port Runtime API <code>/runtime/servers</code>"},{"location":"supported-configuration/#frontend-modifications","title":"Frontend Modifications","text":"Field Description API Endpoint Maxconn Maximum connections Runtime API <code>/runtime/frontends</code> <p>Note: Map files and auxiliary files are updated via the Storage API, which also avoids reloads when only file contents change.</p>"},{"location":"supported-configuration/#reload-required-operations","title":"Reload-Required Operations","text":"<p>The following changes require an HAProxy reload:</p>"},{"location":"supported-configuration/#server-operations","title":"Server Operations","text":"Operation Reason Creating servers New server requires configuration reload Deleting servers Removing server requires configuration reload Modifying non-runtime fields Fields like <code>check</code>, <code>inter</code>, <code>rise</code>, <code>fall</code>, <code>ssl</code>, <code>verify</code>, etc. are not supported by Runtime API <p>Examples of server attributes that require reload when modified: - Health check settings (<code>check</code>, <code>inter</code>, <code>rise</code>, <code>fall</code>, <code>fastinter</code>, <code>downinter</code>) - SSL/TLS settings (<code>ssl</code>, <code>verify</code>, <code>ca-file</code>, <code>crt</code>, <code>sni</code>) - Connection settings (<code>maxconn</code>, <code>maxqueue</code>, <code>minconn</code>) - Advanced options (<code>send-proxy</code>, <code>send-proxy-v2</code>, <code>cookie</code>, <code>track</code>)</p>"},{"location":"supported-configuration/#structural-and-logic-changes","title":"Structural and Logic Changes","text":"Category Components Reason Structural Changes Frontends, Backends, Binds Configuration structure changed Routing Logic ACLs, HTTP Rules, TCP Rules Request processing logic changed Advanced Features Filters, Captures, Stick Rules Feature configuration changed Section Changes All main sections Section-level modifications Health Checks HTTP Checks, TCP Checks Health check logic changed Frontend Attributes Most frontend settings except Maxconn Not supported by Runtime API"},{"location":"supported-configuration/#optimization-strategy","title":"Optimization Strategy","text":"<p>The controller uses fine-grained comparison to detect changes at the attribute level:</p> <ol> <li>Server Weight/Address/Port Changes: Applied via Runtime API (no reload)</li> <li>Server Creation/Deletion: Triggers reload (required by HAProxy)</li> <li>Server Health Check Changes: Triggers reload (not supported by Runtime API)</li> <li>Other Changes: Evaluated individually; structural changes trigger reloads</li> </ol> <p>Important: The Dataplane API automatically determines whether a change can use the Runtime API. If any modified field is not runtime-supported, a reload is triggered. The controller delegates this decision to the Dataplane API's <code>changeThroughRuntimeAPI</code> function.</p> <p>Reference: See HAProxy Dataplane API runtime.go for the complete Runtime API field support logic.</p>"},{"location":"supported-configuration/#not-supported","title":"Not Supported","text":""},{"location":"supported-configuration/#listen-sections","title":"Listen Sections","text":"<p>Listen sections are NOT supported because the HAProxy Dataplane API does not expose them as manageable resources.</p> <p>Background: HAProxy's <code>listen</code> directive combines frontend and backend functionality into a single section. However, the Dataplane API enforces separation of concerns by requiring distinct <code>frontend</code> and <code>backend</code> sections.</p> <p>Workaround: Any Listen section can be decomposed into: - One Frontend section (handles client connections) - One Backend section (handles server connections)</p> <p>Since both Frontend and Backend are fully supported, this provides equivalent functionality.</p>"},{"location":"supported-configuration/#implementation-details","title":"Implementation Details","text":""},{"location":"supported-configuration/#comparison-strategies","title":"Comparison Strategies","text":"<p>The implementation uses two approaches for optimal performance:</p> <ol> <li>Fine-Grained Child Resource Management (frequently-changing resources)</li> <li>Frontends: 9 child resource types</li> <li>Backends: 14 child resource types</li> <li>Each child resource has individual Create/Update/Delete operations</li> <li>Changes to individual ACLs, rules, or servers are applied independently</li> <li> <p>Benefit: Minimizes API calls and reduces reload frequency</p> </li> <li> <p>Whole-Section Replacement (infrequently-changing resources)</p> </li> <li>Other sections (Resolvers, Mailers, Peers, etc.)</li> <li>Uses <code>.Equal()</code> method to compare entire section including nested components</li> <li>If any attribute changes, the entire section is replaced</li> <li>Benefit: Simpler code, fewer operations for resources that rarely change</li> </ol>"},{"location":"supported-configuration/#operation-ordering","title":"Operation Ordering","text":"<p>Operations are automatically ordered by: 1. Priority (lower numbers first) 2. Type (Delete \u2192 Create \u2192 Update) 3. Dependencies (parent sections before child components)</p> <p>This ensures that, for example, a Backend is created before its Servers, and Servers are deleted before the Backend is removed.</p>"},{"location":"supported-configuration/#code-references","title":"Code References","text":"<p>All comparison logic is implemented in: - Main Comparator: <code>pkg/dataplane/comparator/comparator.go</code> - Operation Definitions: <code>pkg/dataplane/comparator/sections/*.go</code></p> <p>The comparator uses the <code>haproxytech/client-native</code> models' built-in <code>.Equal()</code> methods for comprehensive attribute comparison, ensuring zero-maintenance compatibility with future HAProxy features.</p>"},{"location":"supported-configuration/#summary","title":"Summary","text":"<p>The haproxy-template-ic provides complete HAProxy Dataplane API coverage:</p> <ul> <li>\u2705 15 main sections fully supported</li> <li>\u2705 23 child component types with fine-grained operations</li> <li>\u2705 Runtime API optimization for zero-reload server updates</li> <li>\u2705 Dependency-aware operation ordering for safe deployments</li> <li>\u2705 Future-proof comparison using HAProxy models' <code>.Equal()</code> methods</li> <li>\u274c Listen sections not supported (Dataplane API limitation)</li> </ul> <p>For implementation details, see: - Architecture: <code>docs/design.md</code> - Parser: <code>pkg/dataplane/parser/</code> - Comparator: <code>pkg/dataplane/comparator/</code> - Operations: <code>pkg/dataplane/comparator/sections/</code></p>"},{"location":"templating/","title":"Templating Guide","text":""},{"location":"templating/#overview","title":"Overview","text":"<p>The HAProxy Template Ingress Controller uses Gonja v2, a Jinja2-like template engine for Go, to generate HAProxy configurations from Kubernetes resources. You define templates that access watched Kubernetes resources, and the controller renders these templates whenever resources change, validates the output, and deploys it to HAProxy instances.</p> <p>Why use templates: - Complete control over HAProxy configuration without annotation limitations - Access to all HAProxy features (ACLs, stick tables, rate limiting, custom error pages) - Define your own data model using any Kubernetes resources - Reusable template snippets for common patterns - Type-safe template rendering with early error detection</p> <p>Templates are rendered automatically when any watched resource changes, during initial synchronization, or periodically for drift detection.</p>"},{"location":"templating/#what-you-can-template","title":"What You Can Template","text":"<p>The controller supports five types of templatable components, each serving a specific purpose in HAProxy configuration.</p>"},{"location":"templating/#haproxy-configuration","title":"HAProxy Configuration","text":"<p>The main <code>haproxy_config</code> template generates the complete HAProxy configuration file (<code>/etc/haproxy/haproxy.cfg</code>).</p> <pre><code>haproxy_config:\n  template: |\n    global\n        log stdout len 4096 local0 info\n        daemon\n        maxconn 4096\n\n    defaults\n        mode http\n        timeout connect 5s\n        timeout client 50s\n        timeout server 50s\n\n    frontend http\n        bind *:80\n        # Use map files for routing\n        use_backend %[req.hdr(host),lower,map({{ pathResolver.GetPath(\"host.map\", \"map\") }})]\n\n    {% for ingress in resources.ingresses.List() %}\n    backend {{ ingress.metadata.name }}\n        balance roundrobin\n        # Backend configuration here\n    {% endfor %}\n</code></pre> <p>[!IMPORTANT] All auxiliary file references in HAProxy configuration must use absolute paths from <code>/etc/haproxy/</code>. For example, use <code>/etc/haproxy/maps/host.map</code> for map files, <code>/etc/haproxy/ssl/cert.pem</code> for SSL certificates, and <code>/etc/haproxy/general/error.http</code> for general files. This requirement exists because the HAProxy Dataplane API validation needs absolute paths to verify files exist before applying configuration changes.</p>"},{"location":"templating/#map-files","title":"Map Files","text":"<p>Map files (<code>maps</code>) generate HAProxy map files for backend routing, ACL matching, and other lookup operations. Maps are stored in <code>/etc/haproxy/maps/</code> and referenced in HAProxy configuration using absolute paths like <code>/etc/haproxy/maps/host.map</code>.</p> <pre><code>maps:\n  host.map:\n    template: |\n      # Map host headers to normalized host values\n      {%- for ingress in resources.ingresses.List() %}\n      {%- for rule in (ingress.spec.rules | default([]) | selectattr(\"http\", \"defined\")) %}\n      {{ rule.host }} {{ rule.host }}\n      {%- endfor %}\n      {%- endfor %}\n\n  path-prefix.map:\n    template: |\n      # Map host+path to backend names using prefix matching\n      {%- for ingress in resources.ingresses.List() %}\n      {%- for rule in (ingress.spec.rules | default([])) %}\n      {%- for path in (rule.http.paths | default([])) %}\n      {{ rule.host }}{{ path.path }}/ backend_{{ ingress.metadata.name }}\n      {%- endfor %}\n      {%- endfor %}\n      {%- endfor %}\n</code></pre> <p>Use maps for: - Host-based backend selection - Path-based routing with prefix or exact matching - ACL lookups for access control - Rate limiting based on client characteristics</p>"},{"location":"templating/#general-files","title":"General Files","text":"<p>General files (<code>files</code>) generate auxiliary files like custom error pages. Files are stored in <code>/etc/haproxy/general/</code> and referenced in HAProxy configuration using absolute paths like <code>/etc/haproxy/general/503.http</code>.</p> <pre><code>files:\n  503.http:\n    template: |\n      HTTP/1.0 503 Service Unavailable\n      Cache-Control: no-cache\n      Connection: close\n      Content-Type: text/html\n\n      &lt;html&gt;&lt;body&gt;&lt;h1&gt;503 Service Unavailable&lt;/h1&gt;\n      &lt;p&gt;No server is available to handle this request.&lt;/p&gt;\n      &lt;/body&gt;&lt;/html&gt;\n\n  maintenance.html:\n    template: |\n      &lt;html&gt;&lt;body&gt;\n      &lt;h1&gt;Maintenance Mode&lt;/h1&gt;\n      &lt;p&gt;System will be back online at {{ maintenance_end_time }}.&lt;/p&gt;\n      &lt;/body&gt;&lt;/html&gt;\n</code></pre> <p>Use general files for: - Custom error pages (400, 403, 404, 500, 503, etc.) - Maintenance mode pages - Health check responses - Static content served by HAProxy</p>"},{"location":"templating/#ssl-certificates","title":"SSL Certificates","text":"<p>SSL certificates (<code>ssl_certificates</code>) generate SSL/TLS certificate files from Kubernetes Secret data. Certificates are stored in <code>/etc/haproxy/ssl/</code> and referenced in HAProxy configuration using absolute paths like <code>/etc/haproxy/ssl/example-com.pem</code>.</p> <pre><code>ssl_certificates:\n  example-com.pem:\n    template: |\n      {%- for secret in resources.secrets.Fetch(\"default\", \"kubernetes.io/tls\") %}\n      {%- if secret.metadata.name == \"example-com-tls\" %}\n      {{ secret.data.tls_crt | b64decode }}\n      {{ secret.data.tls_key | b64decode }}\n      {%- endif %}\n      {%- endfor %}\n</code></pre> <p>Use SSL certificates for: - TLS termination at HAProxy - Client certificate authentication - Backend SSL connections</p> <p>[!NOTE] Certificate data in Secrets is base64-encoded. Use the <code>b64decode</code> filter to decode it in your templates.</p>"},{"location":"templating/#template-snippets","title":"Template Snippets","text":"<p>Template snippets (<code>template_snippets</code>) define reusable template fragments that can be included in other templates. This promotes code reuse and keeps templates maintainable.</p> <pre><code>template_snippets:\n  backend-name:\n    name: backend-name\n    template: &gt;-\n      ing_{{ ingress.metadata.namespace }}_{{ ingress.metadata.name }}_{{ path.backend.service.name }}\n\n  backend-servers:\n    name: backend-servers\n    template: |\n      {%- for endpoint_slice in resources.endpoints.Fetch(service_name) %}\n      {%- for endpoint in (endpoint_slice.endpoints | default([])) %}\n      {%- for address in (endpoint.addresses | default([])) %}\n      server {{ endpoint.targetRef.name }} {{ address }}:{{ port }} check\n      {%- endfor %}\n      {%- endfor %}\n      {%- endfor %}\n</code></pre> <p>Include snippets using <code>{% include \"snippet-name\" %}</code>:</p> <pre><code>backend {% include \"backend-name\" %}\n    balance roundrobin\n    {% include \"backend-servers\" %}\n</code></pre>"},{"location":"templating/#template-syntax","title":"Template Syntax","text":"<p>Templates use Gonja v2, which provides Jinja2-like syntax for Go. The syntax is familiar if you've used Jinja2, Django templates, or Ansible.</p>"},{"location":"templating/#control-structures","title":"Control Structures","text":"<p>Loops iterate over collections:</p> <pre><code>{% for ingress in resources.ingresses.List() %}\n  backend {{ ingress.metadata.name }}\n      server srv1 192.168.1.10:80\n{% endfor %}\n</code></pre> <p>Conditionals control template logic:</p> <pre><code>{% if ingress.spec.tls %}\n  {% set cert_name = ingress.metadata.name ~ \".pem\" %}\n  bind *:443 ssl crt {{ pathResolver.GetPath(cert_name, \"cert\") }}\n{% else %}\n  bind *:80\n{% endif %}\n</code></pre> <p>Variables store values for reuse:</p> <pre><code>{% set service_name = path.backend.service.name %}\n{% set port = path.backend.service.port.number | default(80) %}\n</code></pre> <p>Comments document your templates:</p> <pre><code>{# This backend handles all traffic for the production namespace #}\nbackend production_backend\n    balance roundrobin\n</code></pre> <p>For complete syntax reference, see the Gonja documentation.</p>"},{"location":"templating/#filters","title":"Filters","text":"<p>Filters transform values in templates using the pipe (<code>|</code>) operator.</p> <p>Common filters:</p> <pre><code>{# Provide default values for missing data #}\n{{ path.backend.service.port.number | default(80) }}\n\n{# String manipulation #}\n{{ rule.host | lower }}\n{{ ingress.metadata.name | upper }}\n\n{# Get collection length #}\n{{ ingress.spec.rules | length }}\n\n{# Indent text blocks #}\n{%- filter indent(2, first=True) %}\n{% include \"backend-servers\" %}\n{%- endfilter %}\n</code></pre> <p>Context method - pathResolver.GetPath():</p> <p>The <code>pathResolver.GetPath()</code> method resolves filenames to absolute paths based on file type. This simplifies template writing by automatically constructing correct absolute paths for HAProxy auxiliary files.</p> <p>The <code>pathResolver</code> is a context variable that provides path resolution based on the controller's dataplane configuration.</p> <pre><code>{# Map files - resolve to maps directory #}\nuse_backend %[req.hdr(host),lower,map({{ pathResolver.GetPath(\"host.map\", \"map\") }})]\n{# Output: use_backend %[req.hdr(host),lower,map(/etc/haproxy/maps/host.map)] #}\n\n{# General files - resolve to general directory #}\nerrorfile 504 {{ pathResolver.GetPath(\"504.http\", \"file\") }}\n{# Output: errorfile 504 /etc/haproxy/general/504.http #}\n\n{# SSL certificates - resolve to SSL directory #}\nbind *:443 ssl crt {{ pathResolver.GetPath(\"example.com.pem\", \"cert\") }}\n{# Output: bind *:443 ssl crt /etc/haproxy/ssl/example.com.pem #}\n\n{# Use with variables #}\n{% set cert_name = ingress.metadata.name ~ \".pem\" %}\nbind *:443 ssl crt {{ pathResolver.GetPath(cert_name, \"cert\") }}\n</code></pre> <p>Arguments: - filename (required): The base filename without directory path - type (required): File type - <code>\"map\"</code>, <code>\"file\"</code>, <code>\"cert\"</code>, or <code>\"crt-list\"</code></p> <p>The method uses the paths configured in <code>dataplane.maps_dir</code>, <code>dataplane.ssl_certs_dir</code>, and <code>dataplane.general_storage_dir</code> from your controller configuration.</p> <p>Custom filter - glob_match:</p> <p>The <code>glob_match</code> filter filters lists of strings using glob patterns with <code>*</code> (match any characters) and <code>?</code> (match single character) wildcards.</p> <pre><code>{# Filter template snippet names by pattern #}\n{% set backend_snippets = template_snippets | glob_match(\"backend-annotation-*\") %}\n{% for snippet_name in backend_snippets %}\n  {% include snippet_name %}\n{% endfor %}\n\n{# Filter resource names #}\n{% set prod_ingresses = resources.ingresses.List() | map(attribute='metadata.name') | glob_match(\"prod-*\") %}\n</code></pre> <p>Arguments: - input (required): List of strings to filter - pattern (required): Glob pattern string</p> <p>Returns: List of strings matching the pattern</p> <p>Custom filter - b64decode:</p> <p>The <code>b64decode</code> filter decodes base64-encoded strings. This is essential for accessing Kubernetes Secret data, as Kubernetes automatically base64-encodes all secret values.</p> <pre><code>{# Decode secret data #}\n{% set secret = resources.secrets.GetSingle(\"default\", \"my-secret\") %}\n{% if secret %}\n  password: {{ secret.data.password | b64decode }}\n{% endif %}\n\n{# Decode credentials for HAProxy userlist #}\nuserlist auth_users\n{% for username in secret.data %}\n  user {{ username }} password {{ secret.data[username] | b64decode }}\n{% endfor %}\n</code></pre> <p>Arguments: - input (required): Base64-encoded string</p> <p>Returns: Decoded plaintext string</p> <p>Custom filter - regex_escape:</p> <p>The <code>regex_escape</code> filter escapes special regex characters for safe use in HAProxy ACL patterns. Essential when constructing regex patterns from user-controlled data.</p> <pre><code>{# Escape path prefixes for regex matching #}\n{% set path_pattern = route.match.path.value | regex_escape %}\nacl path_match path_reg ^{{ path_pattern }}\n\n{# Example with Gateway API HTTPRoute path matching #}\n{% for route in httproutes %}\n  {% set escaped_path = route.spec.rules[0].matches[0].path.value | regex_escape %}\n  use_backend backend_{{ route.metadata.name }} if { path_reg ^{{ escaped_path }} }\n{% endfor %}\n</code></pre> <p>Custom filter - sort_by:</p> <p>The <code>sort_by</code> filter sorts arrays of objects by JSONPath expressions. Supports multiple sort criteria with modifiers.</p> <pre><code>{# Sort routes by priority (descending) then name #}\n{% set sorted_routes = routes | sort_by([\"$.priority:desc\", \"$.name\"]) %}\n\n{# Gateway API route precedence: method &gt; headers &gt; query params &gt; path specificity #}\n{% set sorted = routes | sort_by([\n    \"$.match.method:exists:desc\",\n    \"$.match.headers | length:desc\",\n    \"$.match.queryParams | length:desc\",\n    \"$.match.path.value | length:desc\"\n]) %}\n{% for route in sorted %}\n  {# Routes now ordered by Gateway API precedence rules #}\n  use_backend {{ route.backend }} if {{ route.acl }}\n{% endfor %}\n</code></pre> <p>Available modifiers: - <code>:desc</code> - Sort in descending order - <code>:exists</code> - Sort by field presence (present items first) - <code>| length</code> - Sort by string or array length</p> <p>Custom filter - extract:</p> <p>The <code>extract</code> filter extracts values from objects using JSONPath expressions. Automatically flattens nested arrays.</p> <pre><code>{# Extract all HTTP methods from Gateway API routes #}\n{% set all_methods = httproutes | extract(\"$.spec.rules[*].matches[*].method\") %}\n{# Returns: [\"GET\", \"POST\", \"PUT\", \"DELETE\"] #}\n\n{# Extract service names from ingresses #}\n{% set services = ingresses | extract(\"$.spec.rules[*].http.paths[*].backend.service.name\") %}\n\n{# Use extracted values for ACL construction #}\n{% set hostnames = ingresses | extract(\"$.spec.rules[*].host\") | unique %}\n{% for host in hostnames %}\n  acl is_{{ host | replace(\".\", \"_\") }} hdr(host) -i {{ host }}\n{% endfor %}\n</code></pre> <p>Custom filter - group_by:</p> <p>The <code>group_by</code> filter groups array items by the value of a JSONPath expression.</p> <pre><code>{# Group ingresses by namespace for multi-tenant configuration #}\n{% set by_namespace = ingresses | group_by(\"$.metadata.namespace\") %}\n{% for namespace, ingresses in by_namespace.items() %}\n  # Namespace: {{ namespace }} ({{ ingresses|length }} ingresses)\n  {% for ingress in ingresses %}\n    backend ing_{{ namespace }}_{{ ingress.metadata.name }}\n  {% endfor %}\n{% endfor %}\n\n{# Group routes by priority level #}\n{% set by_priority = routes | group_by(\"$.priority\") %}\n{% for priority in by_priority.keys() | sort | reverse %}\n  # Priority {{ priority }} routes\n  {% for route in by_priority[priority] %}\n    {# Process high-priority routes first #}\n  {% endfor %}\n{% endfor %}\n</code></pre> <p>Custom filter - transform:</p> <p>The <code>transform</code> filter applies regex substitution to array elements.</p> <pre><code>{# Strip API version prefixes from paths #}\n{% set paths = [\"/api/v1/users\", \"/api/v1/posts\", \"/api/v2/comments\"] %}\n{% set clean_paths = paths | transform(\"^/api/v\\\\d+\", \"\") %}\n{# Returns: [\"/users\", \"/posts\", \"/comments\"] #}\n\n{# Normalize hostname formats #}\n{% set hosts = [\"www.example.com\", \"api.example.com\"] %}\n{% set domains = hosts | transform(\"^[^.]+\\\\.\", \"\") %}\n{# Returns: [\"example.com\", \"example.com\"] #}\n</code></pre> <p>Custom filter - debug:</p> <p>The <code>debug</code> filter outputs variables as JSON-formatted HAProxy comments. Useful for template development and troubleshooting.</p> <pre><code>{# Debug route structure during development #}\n{{ routes | debug(\"available-routes\") }}\n\n{# Output:\n# DEBUG available-routes:\n# [\n#   {\n#     \"name\": \"api-route\",\n#     \"priority\": 100,\n#     \"match\": {\"method\": \"GET\", \"path\": \"/api\"}\n#   }\n# ]\n#}\n\n{# Compare before/after transformations #}\n{{ routes | debug(\"before-sorting\") }}\n{% set sorted = routes | sort_by([\"$.priority:desc\"]) %}\n{{ sorted | debug(\"after-sorting\") }}\n</code></pre> <p>Custom filter - eval:</p> <p>The <code>eval</code> filter evaluates JSONPath expressions and shows results with type information. Useful for testing sort_by criteria.</p> <pre><code>{# Test sort criteria before applying #}\n{% for route in routes %}\n  Route: {{ route.name }}\n    Priority: {{ route | eval(\"$.priority\") }}\n    Has method: {{ route | eval(\"$.match.method:exists\") }}\n    Header count: {{ route | eval(\"$.match.headers | length\") }}\n{% endfor %}\n\n{# Understand why items are sorted in a specific order #}\n{% for item in items %}\n  {{ item | eval(\"$.weight:desc\") }}  {# Shows: 100 (int), 50 (int), 10 (int) #}\n{% endfor %}\n</code></pre> <p>For the complete list of built-in filters, see Gonja filters.</p>"},{"location":"templating/#functions","title":"Functions","text":"<p>Gonja provides built-in functions for common operations. See the Gonja documentation for available functions.</p>"},{"location":"templating/#available-template-data","title":"Available Template Data","text":"<p>Templates have access to the <code>resources</code> variable, which contains stores for all watched Kubernetes resource types.</p>"},{"location":"templating/#the-resources-variable","title":"The <code>resources</code> Variable","text":"<p>The <code>resources</code> variable is a collection of stores, one for each resource type you configure in <code>watched_resources</code>. Each store provides <code>List()</code> and <code>Get()</code> methods for accessing resources.</p> <p>Structure:</p> <pre><code>resources:\n  ingresses:    # Store for Ingress resources (if configured)\n  services:     # Store for Service resources (if configured)\n  endpoints:    # Store for EndpointSlice resources (if configured)\n  secrets:      # Store for Secret resources (if configured)\n  # ... any other configured resource types\n</code></pre> <p>The store names match the keys in your <code>watched_resources</code> configuration.</p>"},{"location":"templating/#using-list-method","title":"Using List() Method","text":"<p>The <code>List()</code> method returns all resources of a specific type. Use this to iterate over all resources.</p> <p>Example:</p> <pre><code>{# Iterate over all Ingress resources #}\n{% for ingress in resources.ingresses.List() %}\nbackend {{ ingress.metadata.name }}\n    balance roundrobin\n    server srv1 192.168.1.10:80\n{% endfor %}\n</code></pre> <p>When to use List(): - Generate configuration for all resources of a type - Build map files with all hosts/paths - Count resources for capacity planning</p>"},{"location":"templating/#using-get-method","title":"Using Get() Method","text":"<p>The <code>Get()</code> method returns resources matching specific index keys. The parameters you provide to <code>Get()</code> are determined by the <code>index_by</code> configuration for that resource type.</p> <p>How indexing works:</p> <pre><code>watched_resources:\n  ingresses:\n    api_version: networking.k8s.io/v1\n    kind: Ingress\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n    # Get() expects: Get(namespace, name)\n\n  endpoints:\n    api_version: discovery.k8s.io/v1\n    kind: EndpointSlice\n    index_by: [\"metadata.labels.kubernetes\\\\.io/service-name\"]\n    # Get() expects: Get(service_name)\n</code></pre> <p>Examples:</p> <pre><code>{# Get specific ingress by namespace and name #}\n{% for ingress in resources.ingresses.Fetch(\"default\", \"my-ingress\") %}\n  {# Usually returns 0 or 1 items #}\n{% endfor %}\n\n{# Get all endpoint slices for a service #}\n{% set service_name = path.backend.service.name %}\n{% for endpoint_slice in resources.endpoints.Fetch(service_name) %}\n  {# Returns all endpoint slices labeled with this service name #}\n{% endfor %}\n</code></pre> <p>When to use Get(): - Look up specific resources by key - Find related resources (e.g., endpoints for a service) - Implement cross-resource matching patterns</p>"},{"location":"templating/#index-configuration","title":"Index Configuration","text":"<p>The <code>index_by</code> field in <code>watched_resources</code> determines: 1. How resources are indexed in the store 2. What parameters <code>Get()</code> expects 3. Query performance (O(1) lookups with proper indexing)</p> <p>Common indexing patterns:</p> <pre><code># By namespace and name (most resources)\nindex_by: [\"metadata.namespace\", \"metadata.name\"]\n# Get(namespace, name) returns specific resource\n\n# By service name (endpoint slices)\nindex_by: [\"metadata.labels.kubernetes\\\\.io/service-name\"]\n# Get(service_name) returns all endpoint slices for that service\n\n# By type (secrets)\nindex_by: [\"metadata.namespace\", \"type\"]\n# Get(namespace, type) returns all secrets of that type in namespace\n</code></pre> <p>[!TIP] Escape dots in JSONPath expressions for labels: <code>kubernetes\\\\.io/service-name</code></p>"},{"location":"templating/#custom-template-variables","title":"Custom Template Variables","text":"<p>You can add custom variables to the template context using <code>templatingSettings.extraContext</code>. These variables are available in all templates, allowing you to configure template behavior without modifying controller code.</p>"},{"location":"templating/#configuration","title":"Configuration","text":"<p>Define custom variables in your HAProxyTemplateConfig CRD or ConfigMap configuration:</p> <p>Using CRD:</p> <pre><code>apiVersion: haproxy-template-ic.github.io/v1alpha1\nkind: HAProxyTemplateConfig\nspec:\n  templatingSettings:\n    extraContext:\n      debug:\n        enabled: true\n        verboseHeaders: false\n      environment: production\n      featureFlags:\n        rateLimiting: true\n        caching: false\n      customTimeout: 30\n</code></pre> <p>Using ConfigMap:</p> <pre><code># config.yaml in ConfigMap\ntemplating_settings:\n  extra_context:\n    debug:\n      enabled: true\n      verbose_headers: false\n    environment: production\n    feature_flags:\n      rate_limiting: true\n      caching: false\n    custom_timeout: 30\n</code></pre>"},{"location":"templating/#accessing-custom-variables","title":"Accessing Custom Variables","text":"<p>Custom variables are merged at the top level of the template context. Access them directly without prefixes:</p> <pre><code>{% if debug.enabled %}\n  # Debug mode - add diagnostic headers\n  http-response set-header X-HAProxy-Backend %[be_name]\n  http-response set-header X-HAProxy-Server %[srv_name]\n{% endif %}\n\n{% if environment == \"production\" %}\n  # Production-specific settings\n  timeout client {{ customTimeout }}s\n  timeout server {{ customTimeout }}s\n{% else %}\n  # Development settings\n  timeout client 300s\n  timeout server 300s\n{% endif %}\n\n{% if featureFlags.rateLimiting %}\n  # Rate limiting configuration\n  stick-table type ip size 100k expire 30s store http_req_rate(10s)\n  http-request track-sc0 src\n  http-request deny if { sc_http_req_rate(0) gt 100 }\n{% endif %}\n</code></pre>"},{"location":"templating/#use-cases","title":"Use Cases","text":"<p>Environment-Specific Configuration:</p> <pre><code>extraContext:\n  environment: staging\n  limits:\n    maxConn: 1000\n    timeout: 10\n</code></pre> <pre><code>global\n  maxconn {% if environment == \"production\" %}10000{% else %}{{ limits.maxConn }}{% endif %}\n\ndefaults\n  timeout connect {{ limits.timeout }}s\n</code></pre> <p>Feature Flags:</p> <pre><code>extraContext:\n  features:\n    compression: true\n    http2: false\n</code></pre> <pre><code>{% if features.compression %}\n  compression algo gzip\n  compression type text/html text/plain text/css\n{% endif %}\n\n{% if features.http2 %}\n  bind *:443 ssl crt /etc/haproxy/ssl/ alpn h2,http/1.1\n{% else %}\n  bind *:443 ssl crt /etc/haproxy/ssl/\n{% endif %}\n</code></pre> <p>Debug Headers:</p> <pre><code>extraContext:\n  debug: true\n</code></pre> <pre><code>{% if debug %}\n  # Add diagnostic headers showing routing decisions\n  http-response set-header X-Gateway-Matched-Route %[var(txn.matched_route)]\n  http-response set-header X-Gateway-Backend %[var(txn.backend_name)]\n{% endif %}\n</code></pre>"},{"location":"templating/#supported-value-types","title":"Supported Value Types","text":"<p>The <code>extraContext</code> field accepts any valid JSON value: - Strings: <code>environment: \"production\"</code> - Numbers: <code>timeout: 30</code>, <code>limit: 1.5</code> - Booleans: <code>enabled: true</code> - Objects: <code>debug: { enabled: true, level: 2 }</code> - Arrays: <code>allowedIPs: [\"10.0.0.1\", \"10.0.0.2\"]</code></p>"},{"location":"templating/#authentication-annotations","title":"Authentication Annotations","text":"<p>The controller provides built-in support for HAProxy basic authentication through Ingress annotations. When you add authentication annotations to an Ingress, the controller automatically generates HAProxy userlist sections and configures <code>http-request auth</code> directives.</p>"},{"location":"templating/#basic-authentication","title":"Basic Authentication","text":"<p>Use these annotations on Ingress resources to enable HTTP basic authentication:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: protected-app\n  annotations:\n    haproxy.org/auth-type: \"basic-auth\"\n    haproxy.org/auth-secret: \"my-auth-secret\"\n    haproxy.org/auth-realm: \"Protected Application\"\nspec:\n  rules:\n    - host: app.example.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: my-service\n                port:\n                  number: 80\n</code></pre> <p>Annotations: - <code>haproxy.org/auth-type</code>: Authentication type (currently only <code>\"basic-auth\"</code> is supported) - <code>haproxy.org/auth-secret</code>: Name of the Kubernetes Secret containing credentials (format: <code>\"secret-name\"</code> or <code>\"namespace/secret-name\"</code>) - <code>haproxy.org/auth-realm</code>: HTTP authentication realm displayed to users (optional, defaults to <code>\"Restricted Area\"</code>)</p>"},{"location":"templating/#creating-authentication-secrets","title":"Creating Authentication Secrets","text":"<p>Secrets must contain username-password pairs where values are base64-encoded crypt(3) SHA-512 password hashes.</p> <p>Generate password hashes:</p> <pre><code># Generate SHA-512 hash and encode for Kubernetes\nHASH=$(openssl passwd -6 mypassword)\nkubectl create secret generic my-auth-secret \\\n  --from-literal=admin=$(echo -n \"$HASH\" | base64 -w0) \\\n  --from-literal=user=$(echo -n \"$HASH\" | base64 -w0)\n</code></pre> <p>Secret structure:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-auth-secret\ntype: Opaque\ndata:\n  # Each key is a username, value is base64-encoded password hash\n  admin: JDYkMVd3c2YxNmprcDBkMVBpTyRkS3FHUTF0SW0uOGF1VlJIcVA3dVcuMVV5dVNtZ3YveEc3dEFiOXdZNzc1REw3ZGE0N0hIeVB4ZllDS1BMTktZclJvMHRNQWQyQk1YUHBDd2Z5ZW03MA==\n  user: JDYkbkdxOHJ1T2kyd3l4MUtyZyQ1a2d1azEzb2tKWmpzZ2Z2c3JqdmkvOVoxQjZIbDRUcGVvdkpzb2lQeHA2eGRKWUpha21wUmIwSUVHb1ZUSC8zRzZrLmRMRzBuVUNMWEZnMEhTRTJ5MA==\n</code></pre>"},{"location":"templating/#secret-sharing","title":"Secret Sharing","text":"<p>Multiple Ingress resources can reference the same authentication secret. The controller automatically deduplicates userlist generation, so the HAProxy userlist is created only once regardless of how many Ingresses use it.</p> <pre><code># First ingress\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: app1\n  annotations:\n    haproxy.org/auth-type: \"basic-auth\"\n    haproxy.org/auth-secret: \"shared-auth\"  # Shared secret\n    haproxy.org/auth-realm: \"App 1\"\n# ...\n\n---\n# Second ingress using same secret\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: app2\n  annotations:\n    haproxy.org/auth-type: \"basic-auth\"\n    haproxy.org/auth-secret: \"shared-auth\"  # Same secret\n    haproxy.org/auth-realm: \"App 2\"\n# ...\n</code></pre>"},{"location":"templating/#on-demand-secret-access","title":"On-Demand Secret Access","text":"<p>Authentication secrets are fetched on-demand during template rendering. You must configure the secrets store with <code>store: on-demand</code> in your <code>watched_resources</code>:</p> <pre><code>watched_resources:\n  secrets:\n    api_version: v1\n    kind: Secret\n    store: on-demand\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n</code></pre> <p>This allows the controller to fetch only the secrets referenced by Ingress annotations rather than watching all secrets in the cluster.</p>"},{"location":"templating/#tips-tricks","title":"Tips &amp; Tricks","text":"<p>These patterns solve common challenges when templating HAProxy configurations.</p>"},{"location":"templating/#reserved-server-slots-pattern-avoid-reloads","title":"Reserved Server Slots Pattern (Avoid Reloads)","text":"<p>Problem: Adding or removing servers in HAProxy backends requires a process reload, which can briefly interrupt existing connections.</p> <p>Solution: Pre-allocate a fixed number of server slots with static names. Active endpoints fill slots with real addresses, while unused slots contain disabled placeholders.</p> <p>How it works:</p> <pre><code>{%- set initial_slots = 10 %}  {# Adjust based on expected endpoints #}\n\n{# Collect active endpoints #}\n{%- set ns = namespace(active_endpoints=[]) %}\n{%- for endpoint_slice in resources.endpoints.Fetch(service_name) %}\n  {%- for endpoint in (endpoint_slice.endpoints | default([])) %}\n    {%- for address in (endpoint.addresses | default([])) %}\n      {%- set ns.active_endpoints = ns.active_endpoints + [{'name': endpoint.targetRef.name, 'address': address, 'port': port}] %}\n    {%- endfor %}\n  {%- endfor %}\n{%- endfor %}\n\n{# Generate fixed server slots #}\n{%- for i in range(1, initial_slots + 1) %}\n  {%- if loop.index0 &lt; ns.active_endpoints|length %}\n    {# Active server with real endpoint #}\n    {%- set endpoint = ns.active_endpoints[loop.index0] %}\nserver SRV_{{ i }} {{ endpoint.address }}:{{ endpoint.port }} check\n  {%- else %}\n    {# Disabled placeholder server #}\nserver SRV_{{ i }} 127.0.0.1:1 disabled\n  {%- endif %}\n{%- endfor %}\n</code></pre> <p>Benefits: - Endpoint changes only update server addresses via runtime API (no reload) - Server names remain stable (<code>SRV_1</code>, <code>SRV_2</code>, etc.) - HAProxy can update addresses without dropping connections</p> <p>Auto-expansion: You can implement automatic slot doubling when all slots are filled:</p> <pre><code>{%- set active_count = ns.active_endpoints|length %}\n{%- if active_count &gt; initial_slots %}\n  {%- set max_servers = initial_slots * 2 %}  {# Double when full #}\n{%- else %}\n  {%- set max_servers = initial_slots %}\n{%- endif %}\n</code></pre>"},{"location":"templating/#matching-resources-across-types","title":"Matching Resources Across Types","text":"<p>Pattern: Use fields from one resource type to query another resource type, enabling cross-resource relationships.</p> <p>Example: Matching Ingress resources with their corresponding EndpointSlices:</p> <pre><code>{# Step 1: Iterate over ingresses #}\n{% for ingress in resources.ingresses.List() %}\n{% for rule in (ingress.spec.rules | default([])) %}\n{% for path in (rule.http.paths | default([])) %}\n\n  {# Step 2: Extract service name from ingress #}\n  {% set service_name = path.backend.service.name %}\n  {% set port = path.backend.service.port.number | default(80) %}\n\n  {# Step 3: Look up endpoint slices for this service #}\n  backend ing_{{ ingress.metadata.name }}_{{ service_name }}\n      balance roundrobin\n      {%- for endpoint_slice in resources.endpoints.Fetch(service_name) %}\n      {%- for endpoint in (endpoint_slice.endpoints | default([])) %}\n      {%- for address in (endpoint.addresses | default([])) %}\n      server {{ endpoint.targetRef.name }} {{ address }}:{{ port }} check\n      {%- endfor %}\n      {%- endfor %}\n      {%- endfor %}\n\n{% endfor %}\n{% endfor %}\n{% endfor %}\n</code></pre> <p>Key insight: The index configuration enables this pattern. EndpointSlices are indexed by <code>metadata.labels.kubernetes\\.io/service-name</code>, so <code>Get(service_name)</code> returns all endpoint slices for that service.</p> <p>Other cross-resource patterns:</p> <p>Services with Endpoints: <pre><code>{% for service in resources.services.List() %}\n  {% set service_name = service.metadata.name %}\n  {# Look up endpoints #}\n  {% for endpoint_slice in resources.endpoints.Fetch(service_name) %}\n    {# Process endpoints #}\n  {% endfor %}\n{% endfor %}\n</code></pre></p> <p>Ingresses with TLS Secrets: <pre><code>{% for ingress in resources.ingresses.List() %}\n  {% if ingress.spec.tls %}\n    {% for tls in ingress.spec.tls %}\n      {% set secret_name = tls.secretName %}\n      {% set namespace = ingress.metadata.namespace %}\n      {# Look up TLS secret #}\n      {% for secret in resources.secrets.Fetch(namespace, secret_name) %}\n        {# Use cert data: secret.data.tls_crt | b64decode #}\n      {% endfor %}\n    {% endfor %}\n  {% endif %}\n{% endfor %}\n</code></pre></p> <p>Required index configuration: <pre><code>watched_resources:\n  ingresses:\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n\n  endpoints:\n    index_by: [\"metadata.labels.kubernetes\\\\.io/service-name\"]\n\n  secrets:\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n</code></pre></p>"},{"location":"templating/#safe-iteration-with-default-filter","title":"Safe Iteration with default Filter","text":"<p>Always use the <code>default</code> filter when iterating over optional fields to prevent template errors when fields are missing or null.</p> <pre><code>{# Safe: Returns empty array if field is missing #}\n{% for endpoint in (endpoint_slice.endpoints | default([])) %}\n  {% for address in (endpoint.addresses | default([])) %}\n    server srv {{ address }}:80\n  {% endfor %}\n{% endfor %}\n\n{# Unsafe: Fails if endpoints field is null #}\n{% for endpoint in endpoint_slice.endpoints %}\n  {# ERROR if endpoints is null #}\n{% endfor %}\n</code></pre>"},{"location":"templating/#filtering-with-selectattr","title":"Filtering with selectattr","text":"<p>Use <code>selectattr</code> to filter resources that have specific attributes, useful for optional Kubernetes fields.</p> <pre><code>{# Only process rules that have HTTP configuration #}\n{% for rule in (ingress.spec.rules | default([]) | selectattr(\"http\", \"defined\")) %}\n  {# rule.http is guaranteed to exist #}\n  {% for path in rule.http.paths %}\n    {# Process paths #}\n  {% endfor %}\n{% endfor %}\n\n{# Only process paths with specific pathType #}\n{% set path_types = [\"Prefix\", \"Exact\"] %}\n{% for path in (rule.http.paths | default([]) | selectattr(\"pathType\", \"in\", path_types)) %}\n  {# Only Prefix and Exact paths #}\n{% endfor %}\n</code></pre>"},{"location":"templating/#maintaining-variables-across-loop-scopes","title":"Maintaining Variables Across Loop Scopes","text":"<p>Jinja2/Gonja scoping rules prevent modifying variables inside loops. Use <code>namespace()</code> to create a mutable container for accumulating values across iterations.</p> <pre><code>{# Create namespace for mutable list #}\n{% set ns = namespace(active_endpoints=[]) %}\n\n{# Append items inside loops #}\n{% for endpoint_slice in resources.endpoints.Fetch(service_name) %}\n  {% for endpoint in (endpoint_slice.endpoints | default([])) %}\n    {% for address in (endpoint.addresses | default([])) %}\n      {% set ns.active_endpoints = ns.active_endpoints + [{'address': address, 'port': port}] %}\n    {% endfor %}\n  {% endfor %}\n{% endfor %}\n\n{# Use accumulated list #}\n{% for endpoint in ns.active_endpoints %}\n  server srv{{ loop.index }} {{ endpoint.address }}:{{ endpoint.port }}\n{% endfor %}\n</code></pre>"},{"location":"templating/#template-snippet-composition","title":"Template Snippet Composition","text":"<p>Break complex templates into reusable snippets for better maintainability and code reuse.</p> <p>Define snippets:</p> <pre><code>template_snippets:\n  backend-name:\n    name: backend-name\n    template: &gt;-\n      ing_{{ ingress.metadata.namespace }}_{{ ingress.metadata.name }}_{{ path.backend.service.name }}\n\n  backend-servers:\n    name: backend-servers\n    template: |\n      {% set service_name = path.backend.service.name %}\n      {% set port = path.backend.service.port.number %}\n      {% include \"reserved-server-slots\" %}\n</code></pre> <p>Use snippets:</p> <pre><code>{# Include snippet inline #}\nbackend {% include \"backend-name\" %}\n    balance roundrobin\n    {%- filter indent(4) %}\n    {% include \"backend-servers\" %}\n    {%- endfilter %}\n</code></pre> <p>Pass variables to snippets:</p> <pre><code>{# Set variables before including snippet #}\n{% set service_name = \"my-service\" %}\n{% set port = 8080 %}\n{% include \"backend-servers\" %}\n</code></pre>"},{"location":"templating/#indentation-control","title":"Indentation Control","text":"<p>Control whitespace in generated output using the <code>-</code> marker to strip whitespace.</p> <pre><code>{# Strip whitespace before tag #}\n{%- for item in items %}\n  {{ item }}\n{%- endfor %}\n\n{# Strip whitespace after tag #}\n{% for item in items -%}\n  {{ item }}\n{% endfor -%}\n\n{# Indent included content #}\nbackend my_backend\n    {%- filter indent(4, first=True) %}\n    {% include \"server-list\" %}\n    {%- endfilter %}\n</code></pre> <p>Result with indentation: <pre><code>backend my_backend\n    server srv1 192.168.1.10:80\n    server srv2 192.168.1.11:80\n</code></pre></p>"},{"location":"templating/#complete-examples","title":"Complete Examples","text":""},{"location":"templating/#example-1-simple-host-based-routing","title":"Example 1: Simple Host-Based Routing","text":"<p>Basic Ingress to backend mapping with map-based routing:</p> <pre><code># Configuration\nwatched_resources:\n  ingresses:\n    api_version: networking.k8s.io/v1\n    kind: Ingress\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n\nmaps:\n  host.map:\n    template: |\n      {%- for ingress in resources.ingresses.List() %}\n      {%- for rule in (ingress.spec.rules | default([])) %}\n      {{ rule.host }} backend_{{ ingress.metadata.name }}\n      {%- endfor %}\n      {%- endfor %}\n\nhaproxy_config:\n  template: |\n    global\n        daemon\n\n    defaults\n        mode http\n        timeout connect 5s\n        timeout client 50s\n        timeout server 50s\n\n    frontend http\n        bind *:80\n        # Use map for host-based routing\n        use_backend %[req.hdr(host),lower,map({{ pathResolver.GetPath(\"host.map\", \"map\") }})]\n\n    {% for ingress in resources.ingresses.List() %}\n    backend backend_{{ ingress.metadata.name }}\n        balance roundrobin\n        server srv1 192.168.1.10:80 check\n    {% endfor %}\n</code></pre>"},{"location":"templating/#example-2-path-based-routing-with-maps","title":"Example 2: Path-Based Routing with Maps","text":"<p>Using map files for both host and path-based routing:</p> <pre><code>maps:\n  host.map:\n    template: |\n      {%- for ingress in resources.ingresses.List() %}\n      {%- for rule in (ingress.spec.rules | default([])) %}\n      {{ rule.host }} {{ rule.host }}\n      {%- endfor %}\n      {%- endfor %}\n\n  path-prefix.map:\n    template: |\n      {%- for ingress in resources.ingresses.List() %}\n      {%- for rule in (ingress.spec.rules | default([])) %}\n      {%- for path in (rule.http.paths | default([]) | selectattr(\"pathType\", \"equalto\", \"Prefix\")) %}\n      {{ rule.host }}{{ path.path }}/ backend_{{ ingress.metadata.name }}_{{ path.backend.service.name }}\n      {%- endfor %}\n      {%- endfor %}\n      {%- endfor %}\n\nhaproxy_config:\n  template: |\n    frontend http\n        bind *:80\n\n        # Normalize host header\n        http-request set-var(txn.host) req.hdr(Host),field(1,:),lower\n        http-request set-var(txn.host_match) var(txn.host),map({{ pathResolver.GetPath(\"host.map\", \"map\") }})\n\n        # Path-based routing with prefix matching\n        http-request set-var(txn.backend) var(txn.host_match),concat(,txn.path,),map_beg({{ pathResolver.GetPath(\"path-prefix.map\", \"map\") }})\n\n        use_backend %[var(txn.backend)]\n        default_backend default_backend\n\n    {% for ingress in resources.ingresses.List() %}\n    {% for rule in (ingress.spec.rules | default([])) %}\n    {% for path in (rule.http.paths | default([])) %}\n    backend backend_{{ ingress.metadata.name }}_{{ path.backend.service.name }}\n        balance roundrobin\n        server srv1 192.168.1.10:80 check\n    {% endfor %}\n    {% endfor %}\n    {% endfor %}\n\n    backend default_backend\n        http-request return status 404\n</code></pre>"},{"location":"templating/#example-3-dynamic-backend-servers-with-cross-resource-lookups","title":"Example 3: Dynamic Backend Servers with Cross-Resource Lookups","text":"<p>Complete ingress \u2192 service \u2192 endpoints chain using reserved slots pattern:</p> <pre><code>watched_resources:\n  ingresses:\n    api_version: networking.k8s.io/v1\n    kind: Ingress\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n\n  endpoints:\n    api_version: discovery.k8s.io/v1\n    kind: EndpointSlice\n    index_by: [\"metadata.labels.kubernetes\\\\.io/service-name\"]\n\ntemplate_snippets:\n  backend-servers-with-slots:\n    name: backend-servers-with-slots\n    template: |\n      {%- set initial_slots = 10 %}\n\n      {# Collect active endpoints using indexed lookup #}\n      {%- set ns = namespace(active_endpoints=[]) %}\n      {%- for endpoint_slice in resources.endpoints.Fetch(service_name) %}\n        {%- for endpoint in (endpoint_slice.endpoints | default([])) %}\n          {%- for address in (endpoint.addresses | default([])) %}\n            {%- set ns.active_endpoints = ns.active_endpoints + [{'name': endpoint.targetRef.name, 'address': address, 'port': port}] %}\n          {%- endfor %}\n        {%- endfor %}\n      {%- endfor %}\n\n      {# Generate fixed server slots #}\n      {%- for i in range(1, initial_slots + 1) %}\n        {%- if loop.index0 &lt; ns.active_endpoints|length %}\n          {%- set endpoint = ns.active_endpoints[loop.index0] %}\n      server SRV_{{ i }} {{ endpoint.address }}:{{ endpoint.port }} check\n        {%- else %}\n      server SRV_{{ i }} 127.0.0.1:1 disabled\n        {%- endif %}\n      {%- endfor %}\n\nhaproxy_config:\n  template: |\n    global\n        daemon\n        maxconn 4096\n\n    defaults\n        mode http\n        timeout connect 5s\n        timeout client 50s\n        timeout server 50s\n        option httpchk GET /healthz\n\n    frontend http\n        bind *:80\n        use_backend %[req.hdr(host),lower,map({{ pathResolver.GetPath(\"host.map\", \"map\") }})]\n\n    {% for ingress in resources.ingresses.List() %}\n    {% for rule in (ingress.spec.rules | default([])) %}\n    {% for path in (rule.http.paths | default([])) %}\n    {%- set service_name = path.backend.service.name %}\n    {%- set port = path.backend.service.port.number | default(80) %}\n\n    backend ing_{{ ingress.metadata.name }}_{{ service_name }}\n        balance roundrobin\n        default-server check inter 2s\n        {%- filter indent(4, first=True) %}\n        {% include \"backend-servers-with-slots\" %}\n        {%- endfilter %}\n\n    {% endfor %}\n    {% endfor %}\n    {% endfor %}\n</code></pre> <p>Result: Endpoint changes only update server addresses via HAProxy runtime API without reloads. Server slot names (<code>SRV_1</code>, <code>SRV_2</code>, etc.) remain stable.</p>"},{"location":"templating/#see-also","title":"See Also","text":"<ul> <li>Template Engine Reference - Detailed templating API and error handling</li> <li>Configuration Reference - Complete configuration schema</li> <li>Gonja Documentation - Full template syntax reference</li> <li>Helm Chart Values - Production-ready template examples</li> <li>HAProxy Configuration Manual - HAProxy configuration reference</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>This guide covers common issues and their solutions when running the HAProxy Template Ingress Controller.</p>"},{"location":"troubleshooting/#controller-issues","title":"Controller Issues","text":""},{"location":"troubleshooting/#controller-not-starting","title":"Controller Not Starting","text":"<p>Symptoms: - Controller pods in CrashLoopBackOff - Pod repeatedly restarting - Logs show initialization errors</p> <p>Diagnosis:</p> <pre><code># Check pod status\nkubectl get pods -l app.kubernetes.io/name=haproxy-template-ic\n\n# View recent logs\nkubectl logs -l app.kubernetes.io/name=haproxy-template-ic --tail=100\n\n# Describe pod for events\nkubectl describe pod -l app.kubernetes.io/name=haproxy-template-ic\n</code></pre> <p>Common Causes:</p> <p>1. Missing HAProxyTemplateConfig CRD resource: <pre><code># Check if config exists\nkubectl get haproxytemplateconfig\n\n# Check config content\nkubectl get haproxytemplateconfig haproxy-template-ic-config -o yaml\n</code></pre></p> <p>Solution: <pre><code># Reinstall Helm chart or manually create config\nhelm upgrade haproxy-ic ./charts/haproxy-template-ic --reuse-values\n</code></pre></p> <p>2. Missing or invalid credentials Secret: <pre><code># Check if secret exists\nkubectl get secret haproxy-credentials\n\n# Verify secret has required keys\nkubectl get secret haproxy-credentials -o jsonpath='{.data}' | jq 'keys'\n# Should include: dataplane_username, dataplane_password\n</code></pre></p> <p>Solution: <pre><code># Recreate secret with correct credentials\nkubectl create secret generic haproxy-credentials \\\n  --from-literal=dataplane_username=admin \\\n  --from-literal=dataplane_password=your-password \\\n  --dry-run=client -o yaml | kubectl apply -f -\n</code></pre></p> <p>3. RBAC permissions missing: <pre><code># Check if controller can list resources\nkubectl auth can-i list haproxytemplateconfigs \\\n  --as=system:serviceaccount:default:haproxy-template-ic\n\nkubectl auth can-i list ingresses --all-namespaces \\\n  --as=system:serviceaccount:default:haproxy-template-ic\n</code></pre></p> <p>Solution: <pre><code># Verify ClusterRole and ClusterRoleBinding exist\nkubectl get clusterrole haproxy-template-ic\nkubectl get clusterrolebinding haproxy-template-ic\n\n# Reinstall RBAC if missing\nhelm upgrade haproxy-ic ./charts/haproxy-template-ic --reuse-values\n</code></pre></p>"},{"location":"troubleshooting/#controller-running-but-not-processing-resources","title":"Controller Running But Not Processing Resources","text":"<p>Symptoms: - Controller pods running (not restarting) - No reconciliation happening - Logs show controller started but no activity</p> <p>Diagnosis:</p> <pre><code># Check controller logs for \"watching\" messages\nkubectl logs -l app.kubernetes.io/name=haproxy-template-ic | grep -i \"watch\"\n\n# Check if initial sync completed\nkubectl logs -l app.kubernetes.io/name=haproxy-template-ic | grep -i \"sync complete\"\n</code></pre> <p>Common Causes:</p> <p>1. Informers not syncing:</p> <p>Check logs for errors like: <pre><code>failed to sync cache for ingresses\ntimeout waiting for cache sync\n</code></pre></p> <p>Solution: - Verify API server connectivity - Check network policies aren't blocking controller \u2192 API server - Increase timeout if cluster is slow</p> <p>2. No watched resources match configuration:</p> <pre><code># Check what resources exist\nkubectl get ingresses -A\nkubectl get services -A\n\n# Compare with watched resources configuration\nkubectl get haproxytemplateconfig haproxy-template-ic-config \\\n  -o jsonpath='{.spec.watchedResources}'\n</code></pre> <p>Solution: - Ensure resources exist in watched namespaces - Verify label selectors match actual resource labels - Check namespace configuration isn't too restrictive</p> <p>3. Leader election preventing action (HA mode):</p> <pre><code># Check which pod is leader\nkubectl get lease haproxy-template-ic-leader -o yaml\n\n# Check leader election metrics\nkubectl port-forward deployment/haproxy-template-ic 9090:9090\ncurl http://localhost:9090/metrics | grep leader_election_is_leader\n</code></pre> <p>Solution: - Verify exactly one pod shows <code>is_leader=1</code> - If no leader, check logs for election failures - See High Availability Troubleshooting</p>"},{"location":"troubleshooting/#configuration-issues","title":"Configuration Issues","text":""},{"location":"troubleshooting/#invalid-template-syntax","title":"Invalid Template Syntax","text":"<p>Symptoms: - Error logs mentioning \"template rendering failed\" - Reconciliation errors - Configuration not deploying</p> <p>Diagnosis:</p> <pre><code># Check logs for template errors\nkubectl logs -l app.kubernetes.io/name=haproxy-template-ic | grep -i \"template\\|render\"\n</code></pre> <p>Example error: <pre><code>template rendering failed: function \"unknown_function\" not defined\n</code></pre></p> <p>Solution: 1. Review template syntax in HAProxyTemplateConfig 2. Check for typos in filter names or function calls 3. Use debug server to see rendered output:    <pre><code>kubectl port-forward deployment/haproxy-template-ic 6060:6060\ncurl http://localhost:6060/debug/vars/rendered\n</code></pre> 4. See Templating Guide for correct syntax</p>"},{"location":"troubleshooting/#configuration-validation-failures","title":"Configuration Validation Failures","text":"<p>Symptoms: - Error logs showing \"validation failed\" - HAProxy binary errors in logs - Config not applying to HAProxy pods</p> <p>Diagnosis:</p> <pre><code># Check for validation errors in logs\nkubectl logs -l app.kubernetes.io/name=haproxy-template-ic | grep -i \"validation\"\n</code></pre> <p>Common validation errors:</p> <p>1. Invalid HAProxy syntax: <pre><code>line 42, 'backend' expects &lt;name&gt; as an argument\n</code></pre></p> <p>Solution: - Fix syntax in haproxyConfig template - Refer to HAProxy Documentation - Test configuration locally with <code>haproxy -c -f config.cfg</code></p> <p>2. Missing map files or certificates: <pre><code>unable to load file '/etc/haproxy/maps/host.map'\n</code></pre></p> <p>Solution: - Ensure map files are defined in <code>maps</code> section - Use <code>pathResolver.GetPath()</code> method correctly: <code>{{ pathResolver.GetPath(\"host.map\", \"map\") }}</code> - Check paths match dataplane configuration</p> <p>3. Invalid server addresses: <pre><code>'server' : invalid address: 'invalid-hostname'\n</code></pre></p> <p>Solution: - Verify EndpointSlice resources exist - Check service names are correct - Ensure DNS resolution works for service names</p>"},{"location":"troubleshooting/#validation-test-failures","title":"Validation Test Failures","text":"<p>Symptoms: - <code>controller validate</code> command fails - Webhook rejecting HAProxyTemplateConfig updates - Assertion failures in validation tests - Template rendering errors in tests</p> <p>Diagnosis:</p> <pre><code># Run validation tests with default output\ncontroller validate -f config.yaml\n</code></pre> <p>Common Causes:</p> <p>1. Pattern not found in rendered output:</p> <pre><code>\u2717 Backend pattern test\n  Error: pattern \"backend api-.*\" not found in haproxy.cfg (target size: 1234 bytes).\n         Hint: Use --verbose to see content preview\n</code></pre> <p>Solution: <pre><code># Step 1: See what was actually rendered\ncontroller validate -f config.yaml --verbose\n\n# Example output:\n# \u2717 Backend pattern test\n#   Error: pattern \"backend api-.*\" not found in haproxy.cfg (target size: 1234 bytes)\n#   Target: haproxy.cfg (1234 bytes)\n#   Content preview:\n#     global\n#       daemon\n#     defaults\n#       mode http\n\n# Step 2: If preview isn't enough, see full content\ncontroller validate -f config.yaml --dump-rendered\n\n# Step 3: Check which templates executed\ncontroller validate -f config.yaml --trace-templates\n\n# Example trace:\n# Rendering: haproxy.cfg\n# Completed: haproxy.cfg (0.007ms)\n# Rendering: backends.cfg\n# Completed: backends.cfg (0.005ms)\n</code></pre></p> <p>2. Empty map files or missing content:</p> <pre><code>\u2717 Map contains routing entry\n  Error: pattern \"api.example.com\" not found in map:host-routing.map (target size: 0 bytes)\n</code></pre> <p>Solution: <pre><code># Check what was rendered in the map\ncontroller validate -f config.yaml --dump-rendered\n\n# Look for the map file section:\n# ### Map Files\n# #### map:host-routing.map\n# (empty)\n\n# Common causes of empty maps:\n# - Loop condition never true (no resources match)\n# - Missing `| default([])` on array variables\n# - Incorrect template logic\n</code></pre></p> <p>3. Template execution errors:</p> <pre><code>\u2717 Basic rendering\n  Error: Service 'nonexistent' not found in namespace 'default'\n</code></pre> <p>Solution: <pre><code># Check test fixtures - ensure required resources are defined\n# In config.yaml:\nvalidationTests:\n  - name: \"basic rendering\"\n    fixtures:\n      services:\n        - metadata:\n            name: nonexistent\n            namespace: default\n          spec:\n            clusterIP: 10.0.0.1\n</code></pre></p> <p>4. Slow templates affecting validation:</p> <pre><code># Identify slow templates\ncontroller validate -f config.yaml --trace-templates\n\n# Example output showing slow template:\n# Rendering: haproxy.cfg (0.005ms)\n# Rendering: complex-backends.cfg (45.123ms)  \u2190 Needs optimization\n</code></pre> <p>Solution: - Simplify loops in slow templates - Reduce nested includes - Cache repeated computations with <code>{% set %}</code> - See Templating Guide</p> <p>5. HAProxy syntax validation failures:</p> <pre><code>\u2717 Config must be syntactically valid\n  Error: HAProxy validation failed (config size: 1234 bytes):\n         maxconn: integer expected, got 'invalid' (line 15)\n</code></pre> <p>Solution: <pre><code># See the problematic line\ncontroller validate -f config.yaml --dump-rendered | grep -A2 -B2 \"line 15\"\n\n# Test HAProxy config locally\nhaproxy -c -f /tmp/haproxy.cfg\n</code></pre></p> <p>Debugging Workflow:</p> <pre><code># 1. Start with enhanced error messages (no flags)\ncontroller validate -f config.yaml\n# Output: \"pattern X not found in map:foo.map (target size: 61 bytes). Hint: Use --verbose\"\n\n# 2. Enable verbose for content preview\ncontroller validate -f config.yaml --verbose\n# Output: Shows first 200 chars of failing target\n\n# 3. Dump full content if preview isn't enough\ncontroller validate -f config.yaml --dump-rendered\n# Output: Complete haproxy.cfg, all maps, files, certs\n\n# 4. Check template execution\ncontroller validate -f config.yaml --trace-templates\n# Output: Template names and timing\n\n# 5. Combine flags for comprehensive debugging\ncontroller validate -f config.yaml --verbose --dump-rendered --trace-templates\n</code></pre> <p>Output Formats:</p> <p>For CI/CD integration, use structured output:</p> <pre><code># JSON output\ncontroller validate -f config.yaml --output json &gt; results.json\n\n# YAML output\ncontroller validate -f config.yaml --output yaml &gt; results.yaml\n\n# Both formats include:\n# - Test results (pass/fail)\n# - Assertion details\n# - Rendered content\n# - Error messages\n</code></pre>"},{"location":"troubleshooting/#haproxy-pod-issues","title":"HAProxy Pod Issues","text":""},{"location":"troubleshooting/#cannot-connect-to-haproxy-dataplane-api","title":"Cannot Connect to HAProxy Dataplane API","text":"<p>Symptoms: - Controller logs show \"connection refused\" or \"timeout\" - Deployment operations failing - Configuration not reaching HAProxy</p> <p>Diagnosis:</p> <pre><code># Test direct connection to Dataplane API\nHAPROXY_POD=$(kubectl get pods -l app=haproxy -o jsonpath='{.items[0].metadata.name}')\nkubectl port-forward $HAPROXY_POD 5555:5555\n\n# In another terminal\ncurl -u admin:password http://localhost:5555/v2/info\n</code></pre> <p>Common Causes:</p> <p>1. Dataplane API not running:</p> <pre><code># Check both containers are running\nkubectl get pod $HAPROXY_POD -o jsonpath='{.status.containerStatuses[*].name}'\n# Should show: haproxy dataplane\n\n# Check dataplane container logs\nkubectl logs $HAPROXY_POD -c dataplane\n</code></pre> <p>Solution: - Verify dataplane container started correctly - Check for port conflicts - Ensure master socket exists (see HAProxy container logs)</p> <p>2. Wrong credentials:</p> <pre><code># Compare credentials in controller Secret and HAProxy config\nkubectl get secret haproxy-credentials -o jsonpath='{.data.dataplane_username}' | base64 -d\nkubectl exec $HAPROXY_POD -c dataplane -- cat /etc/haproxy/dataplaneapi.yaml | grep -A2 user\n</code></pre> <p>Solution: <pre><code># Update credentials to match\nkubectl create secret generic haproxy-credentials \\\n  --from-literal=dataplane_username=admin \\\n  --from-literal=dataplane_password=adminpass \\\n  --dry-run=client -o yaml | kubectl apply -f -\n\n# Restart controller to reload credentials\nkubectl rollout restart deployment haproxy-template-ic\n</code></pre></p> <p>3. Network policy blocking access:</p> <pre><code># Check if network policy exists\nkubectl get networkpolicy\n\n# Test connectivity from controller pod\nkubectl exec -it deployment/haproxy-template-ic -- \\\n  wget -qO- http://$HAPROXY_POD_IP:5555/v2/info\n</code></pre> <p>Solution: - Update NetworkPolicy to allow controller \u2192 HAProxy traffic - Check NetworkPolicy egress rules include HAProxy pod selector</p>"},{"location":"troubleshooting/#haproxy-configuration-not-updating","title":"HAProxy Configuration Not Updating","text":"<p>Symptoms: - Controller logs show successful deployment - <code>kubectl exec</code> shows old configuration in HAProxy pod - Changes not taking effect</p> <p>Diagnosis:</p> <pre><code># Check HAProxy config file timestamp\nkubectl exec $HAPROXY_POD -c haproxy -- ls -lh /etc/haproxy/haproxy.cfg\n\n# Compare with last deployment time in controller logs\nkubectl logs -l app.kubernetes.io/name=haproxy-template-ic | grep -i \"deployment.*succeeded\"\n</code></pre> <p>Common Causes:</p> <p>1. Volume mount issues:</p> <p>Verify HAProxy and Dataplane share the same volume:</p> <pre><code># Check volume mounts\nkubectl get pod $HAPROXY_POD -o yaml | grep -A5 volumeMounts\n</code></pre> <p>Solution: - Ensure both containers mount the same <code>haproxy-config</code> volume - Restart pod if volume mount is missing</p> <p>2. HAProxy not reloading:</p> <pre><code># Check HAProxy master process\nkubectl exec $HAPROXY_POD -c haproxy -- ps aux | grep haproxy\n\n# Try manual reload\nkubectl exec $HAPROXY_POD -c haproxy -- \\\n  sh -c \"echo 'reload' | socat stdio unix-connect:/etc/haproxy/haproxy-master.sock\"\n</code></pre> <p>Solution: - Verify master socket is accessible - Check reload command in dataplaneapi.yaml - Review dataplane logs for reload failures</p>"},{"location":"troubleshooting/#routing-issues","title":"Routing Issues","text":""},{"location":"troubleshooting/#requests-not-reaching-backend","title":"Requests Not Reaching Backend","text":"<p>Symptoms: - 503 Service Unavailable errors - Requests timing out - No backend servers in HAProxy stats</p> <p>Diagnosis:</p> <pre><code># Check HAProxy configuration for backend\nkubectl exec $HAPROXY_POD -c haproxy -- cat /etc/haproxy/haproxy.cfg | grep -A10 \"backend\"\n\n# Check if backend has servers\nkubectl exec $HAPROXY_POD -c haproxy -- \\\n  echo \"show servers state\" | socat stdio /etc/haproxy/haproxy-master.sock\n</code></pre> <p>Common Causes:</p> <p>1. No endpoints for service:</p> <pre><code># Check EndpointSlices exist\nkubectl get endpointslices -l kubernetes.io/service-name=echo\n\n# Check pods are ready\nkubectl get pods -l app=echo\n</code></pre> <p>Solution: - Verify backend pods are running and ready - Check service selector matches pod labels - Ensure ports are correctly configured</p> <p>2. Backend not created in HAProxy:</p> <p>Check controller logs for errors during backend creation:</p> <pre><code>kubectl logs -l app.kubernetes.io/name=haproxy-template-ic | grep -i \"backend.*error\"\n</code></pre> <p>Solution: - Review template logic for backend generation - Ensure Ingress references valid service - Check watched resources include Services and EndpointSlices</p> <p>3. Routing rules not matching:</p> <pre><code># Test routing manually\nkubectl port-forward $HAPROXY_POD 8080:80\n\n# Try request with different Host headers\ncurl -v -H \"Host: your-host.example.com\" http://localhost:8080/\n</code></pre> <p>Solution: - Verify Host header matches Ingress rules - Check ACL rules in HAProxy config - Review map files if using map-based routing</p>"},{"location":"troubleshooting/#ssltls-issues","title":"SSL/TLS Issues","text":"<p>Symptoms: - SSL handshake failures - Certificate errors - HTTPS not working</p> <p>Diagnosis:</p> <pre><code># Check SSL certificates exist\nkubectl exec $HAPROXY_POD -c haproxy -- ls -lh /etc/haproxy/ssl/\n\n# Test SSL connection\nopenssl s_client -connect localhost:443 -servername your-host.example.com &lt; /dev/null\n</code></pre> <p>Common Causes:</p> <p>1. Certificate not deployed:</p> <pre><code># Check if certificate template is defined\nkubectl get haproxytemplateconfig haproxy-template-ic-config \\\n  -o jsonpath='{.spec.sslCertificates}'\n\n# Check Secret exists\nkubectl get secret your-tls-secret\n</code></pre> <p>Solution: - Define certificate template in <code>sslCertificates</code> section - Ensure Secret is watched (configure in <code>watchedResources.secrets</code>) - Use <code>b64decode</code> filter for Secret data</p> <p>2. Wrong certificate path in bind:</p> <pre><code># Check bind line in config\nkubectl exec $HAPROXY_POD -c haproxy -- \\\n  grep -i \"bind.*ssl.*crt\" /etc/haproxy/haproxy.cfg\n</code></pre> <p>Solution: - Use absolute paths: <code>/etc/haproxy/ssl/cert.pem</code> - Use <code>pathResolver.GetPath()</code> method: <code>{{ pathResolver.GetPath(\"cert.pem\", \"cert\") }}</code> - Verify path matches <code>dataplane.sslCertsDir</code></p>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#slow-reconciliation","title":"Slow Reconciliation","text":"<p>Symptoms: - Configuration changes taking minutes to apply - High CPU usage in controller - Template rendering timeouts</p> <p>Diagnosis:</p> <pre><code># Check reconciliation duration metrics\nkubectl port-forward deployment/haproxy-template-ic 9090:9090\ncurl http://localhost:9090/metrics | grep reconciliation_duration_seconds\n\n# Enable debug logging\nkubectl set env deployment/haproxy-template-ic VERBOSE=2\nkubectl logs -f deployment/haproxy-template-ic\n</code></pre> <p>Common Causes:</p> <p>1. Large number of resources:</p> <pre><code># Count watched resources\nkubectl get ingresses --all-namespaces | wc -l\nkubectl get services --all-namespaces | wc -l\n</code></pre> <p>Solution: - Use namespace restrictions in <code>watchedResources</code> - Add label selectors to filter resources - Consider memory store vs cached store (see Watching Resources)</p> <p>2. Inefficient templates:</p> <p>Look for: - Nested loops over large collections - Repeated expensive operations - Missing use of variables for cached values</p> <p>Solution: - Use template snippets for reusable logic - Cache computed values with <code>{% set %}</code> - See Templating Guide</p> <p>3. Memory constraints:</p> <pre><code># Check memory usage\nkubectl top pod -l app.kubernetes.io/name=haproxy-template-ic\n\n# Check for OOMKilled events\nkubectl get events --sort-by='.lastTimestamp' | grep OOM\n</code></pre> <p>Solution: <pre><code># Increase memory limits\nhelm upgrade haproxy-ic ./charts/haproxy-template-ic \\\n  --reuse-values \\\n  --set resources.limits.memory=1Gi\n</code></pre></p>"},{"location":"troubleshooting/#high-memory-usage","title":"High Memory Usage","text":"<p>Symptoms: - Controller pod using excessive memory - OOMKilled events - Gradual memory growth</p> <p>Diagnosis:</p> <pre><code># Monitor memory over time\nkubectl top pod -l app.kubernetes.io/name=haproxy-template-ic --containers\n\n# Enable memory profiling (if debug server enabled)\nkubectl port-forward deployment/haproxy-template-ic 6060:6060\ncurl http://localhost:6060/debug/pprof/heap &gt; heap.prof\ngo tool pprof heap.prof\n</code></pre> <p>Solution:</p> <p>1. Use field filtering: <pre><code># In HAProxyTemplateConfig\nspec:\n  watchedResourcesIgnoreFields:\n    - metadata.managedFields\n    - metadata.annotations[\"kubectl.kubernetes.io/last-applied-configuration\"]\n</code></pre></p> <p>2. Switch to cached store for large resources: <pre><code>spec:\n  watchedResources:\n    secrets:\n      store: on-demand\n      cacheTTL: 2m\n</code></pre></p> <p>3. Limit watch scope: <pre><code>spec:\n  watchedResources:\n    ingresses:\n      namespace: production  # Watch only one namespace\n      labelSelector:\n        app: myapp  # Filter by labels\n</code></pre></p>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#collect-diagnostic-information","title":"Collect Diagnostic Information","text":"<p>When reporting issues, gather this information:</p> <pre><code># Controller version\nkubectl get deployment haproxy-template-ic \\\n  -o jsonpath='{.spec.template.spec.containers[0].image}'\n\n# Controller logs (last 500 lines)\nkubectl logs -l app.kubernetes.io/name=haproxy-template-ic --tail=500 &gt; controller-logs.txt\n\n# Configuration\nkubectl get haproxytemplateconfig haproxy-template-ic-config -o yaml &gt; config.yaml\n\n# Resource counts\nkubectl get ingresses,services,endpointslices --all-namespaces | wc -l\n\n# Metrics\nkubectl port-forward deployment/haproxy-template-ic 9090:9090\ncurl http://localhost:9090/metrics &gt; metrics.txt\n\n# HAProxy config (sanitize sensitive data!)\nkubectl exec $HAPROXY_POD -c haproxy -- cat /etc/haproxy/haproxy.cfg &gt; haproxy.cfg\n</code></pre>"},{"location":"troubleshooting/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code># Temporary: Update environment variable\nkubectl set env deployment/haproxy-template-ic VERBOSE=2\n\n# Permanent: Update Helm values\nhelm upgrade haproxy-ic ./charts/haproxy-template-ic \\\n  --reuse-values \\\n  --set controller.config.logging.verbose=2\n</code></pre> <p>Debug logging includes: - Detailed reconciliation steps - Template rendering context - Dataplane API requests/responses - Configuration diff details</p>"},{"location":"troubleshooting/#enable-debug-server","title":"Enable Debug Server","text":"<p>For deeper investigation, enable the debug HTTP server:</p> <pre><code>helm upgrade haproxy-ic ./charts/haproxy-template-ic \\\n  --reuse-values \\\n  --set controller.debugPort=6060\n\n# Access debug endpoints\nkubectl port-forward deployment/haproxy-template-ic 6060:6060\n</code></pre> <p>Available endpoints: - <code>/debug/vars</code> - Internal state - <code>/debug/vars/rendered</code> - Last rendered config - <code>/debug/vars/resources</code> - Resource counts - <code>/debug/vars/events</code> - Recent events - <code>/debug/pprof/</code> - Go profiling</p> <p>See Debugging Guide for details.</p>"},{"location":"troubleshooting/#see-also","title":"See Also","text":"<ul> <li>Getting Started Guide - Initial setup and verification</li> <li>Configuration Reference - Complete configuration options</li> <li>High Availability Guide - HA-specific troubleshooting</li> <li>Debugging Guide - Advanced debugging techniques</li> <li>GitHub Issues - Report bugs and request features</li> </ul>"},{"location":"validation-tests/","title":"Validation Tests","text":"<p>This guide explains how to write and run validation tests for HAProxyTemplateConfig templates.</p>"},{"location":"validation-tests/#overview","title":"Overview","text":"<p>Validation tests allow you to verify that your templates render correctly and produce valid HAProxy configurations. Tests are embedded directly in the HAProxyTemplateConfig CRD and can be executed locally using the CLI.</p> <p>Benefits:</p> <ul> <li>Catch template errors before deployment</li> <li>Verify configuration changes don't break existing functionality</li> <li>Document expected behavior with concrete examples</li> <li>Test edge cases and error conditions</li> </ul>"},{"location":"validation-tests/#quick-start","title":"Quick Start","text":"<p>Add a <code>validationTests</code> section to your HAProxyTemplateConfig:</p> <pre><code>apiVersion: haproxy-template-ic.github.io/v1alpha1\nkind: HAProxyTemplateConfig\nmetadata:\n  name: my-config\nspec:\n  # ... template configuration ...\n\n  validationTests:\n    test-basic-frontend:\n      description: Frontend should be created with correct settings\n      fixtures:\n        services:\n          - apiVersion: v1\n            kind: Service\n            metadata:\n              name: my-service\n              namespace: default\n            spec:\n              ports:\n                - port: 80\n                  targetPort: 8080\n      assertions:\n        - type: haproxy_valid\n          description: Configuration must be syntactically valid\n\n        - type: contains\n          target: haproxy.cfg\n          pattern: \"frontend.*default\"\n          description: Must have default frontend\n</code></pre> <p>Run the tests:</p> <pre><code>controller validate -f my-config.yaml\n</code></pre>"},{"location":"validation-tests/#test-structure","title":"Test Structure","text":"<p>Each validation test consists of:</p> <ol> <li>Name: Unique identifier for the test</li> <li>Description: Human-readable explanation of what the test verifies</li> <li>Fixtures: Test data (Kubernetes resources) to use during rendering</li> <li>Assertions: Checks to perform on the rendered output</li> </ol>"},{"location":"validation-tests/#test-name","title":"Test Name","text":"<ul> <li>Must be unique within the config</li> <li>Use lowercase with hyphens (kebab-case)</li> <li>Be descriptive: <code>test-ingress-tls-routing</code> not <code>test1</code></li> </ul>"},{"location":"validation-tests/#fixtures","title":"Fixtures","text":"<p>Fixtures simulate Kubernetes resources that would be watched by the controller:</p> <pre><code>fixtures:\n  services: # Resource type must match watchedResources config\n    - apiVersion: v1\n      kind: Service\n      metadata:\n        name: api\n        namespace: production\n      spec:\n        ports:\n          - port: 80\n            targetPort: 8080\n\n  ingresses:\n    - apiVersion: networking.k8s.io/v1\n      kind: Ingress\n      metadata:\n        name: main-ingress\n        namespace: production\n      spec:\n        rules:\n          - host: api.example.com\n            http:\n              paths:\n                - path: /\n                  pathType: Prefix\n                  backend:\n                    service:\n                      name: api\n                      port:\n                        number: 80\n</code></pre> <p>Fixture Guidelines:</p> <ul> <li>Include only resources needed for the test</li> <li>Use realistic data that represents actual use cases</li> <li>Test both common cases and edge cases</li> <li>Keep fixtures minimal but complete</li> </ul>"},{"location":"validation-tests/#http-fixtures","title":"HTTP Fixtures","text":"<p>For templates that use <code>http.Fetch()</code> to retrieve external content, you can provide mock HTTP responses using <code>httpResources</code>:</p> <pre><code>validationTests:\n  test-http-blocklist:\n    description: Template should generate blocklist from HTTP-fetched content\n    fixtures:\n      ingresses:\n        - apiVersion: networking.k8s.io/v1\n          kind: Ingress\n          metadata:\n            name: my-ingress\n            namespace: default\n          # ...\n    httpResources:\n      - url: \"http://blocklist.example.com/list.txt\"\n        content: |\n          blocked-value-1\n          blocked-value-2\n          blocked-value-3\n    assertions:\n      - type: contains\n        target: map:blocklist.map\n        pattern: \"blocked-value-1\"\n        description: Blocklist map should contain fetched values\n</code></pre> <p>When a template calls <code>http.Fetch(\"http://blocklist.example.com/list.txt\")</code>, it receives the fixture content instead of making an actual HTTP request.</p> <p>Missing Fixture Error:</p> <p>If a template calls <code>http.Fetch()</code> for a URL that doesn't have a matching fixture, the test fails with an error:</p> <pre><code>Error: http.Fetch: no fixture defined for URL: http://example.com/data.txt\n       (add an httpResources fixture for this URL)\n</code></pre> <p>This ensures all HTTP dependencies are explicitly mocked in tests.</p> <p>Global HTTP Fixtures:</p> <p>You can define HTTP fixtures in the <code>_global</code> test to share them across all tests:</p> <pre><code>validationTests:\n  _global:\n    fixtures: {}\n    httpResources:\n      - url: \"http://shared-config.example.com/config.json\"\n        content: |\n          {\"setting\": \"value\"}\n    assertions: []\n\n  test-uses-shared-config:\n    description: Test that uses globally defined HTTP fixture\n    fixtures:\n      services:\n        - # ...\n    # httpResources from _global are automatically available\n    assertions:\n      - type: haproxy_valid\n</code></pre> <p>Test-specific HTTP fixtures override global fixtures for the same URL.</p> <p>HTTP Fixture Properties:</p> Property Required Description <code>url</code> Yes The HTTP URL that will be matched when templates call <code>http.Fetch()</code> <code>content</code> Yes The response body content to return <p>Notes:</p> <ul> <li>HTTP fixtures are matched by exact URL (no pattern matching)</li> <li>Options passed to <code>http.Fetch()</code> (delay, timeout, auth) are ignored in test mode</li> <li>Empty content is valid: <code>content: \"\"</code></li> </ul>"},{"location":"validation-tests/#assertions","title":"Assertions","text":"<p>Five assertion types are available:</p>"},{"location":"validation-tests/#1-haproxy_valid","title":"1. haproxy_valid","text":"<p>Validates that the rendered HAProxy configuration is syntactically valid using the HAProxy binary.</p> <pre><code>assertions:\n  - type: haproxy_valid\n    description: HAProxy configuration must be syntactically valid\n</code></pre> <p>When to use:</p> <ul> <li>Every test should include this assertion</li> <li>Catches syntax errors, invalid directives, missing sections</li> </ul> <p>Requirements:</p> <ul> <li>HAProxy binary must be in PATH or specified with <code>--haproxy-binary</code></li> </ul>"},{"location":"validation-tests/#2-contains","title":"2. contains","text":"<p>Verifies that the target content matches a regex pattern.</p> <pre><code>assertions:\n  - type: contains\n    target: haproxy.cfg\n    pattern: \"backend api-production\"\n    description: Must create backend for API service\n\n  - type: contains\n    target: haproxy.cfg\n    pattern: \"bind :443 ssl crt /etc/haproxy/ssl/cert.pem\"\n    description: HTTPS frontend must use SSL certificate\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code>: What to check (<code>haproxy.cfg</code>, <code>map:&lt;name&gt;</code>, <code>file:&lt;name&gt;</code>, <code>cert:&lt;name&gt;</code>)</li> <li><code>pattern</code>: Regular expression to match</li> <li><code>description</code>: Why this pattern should exist</li> </ul> <p>Pattern Tips:</p> <ul> <li>Use <code>.*</code> for wildcards: <code>frontend.*production</code></li> <li>Escape special regex characters: <code>\\.</code> for literal dot</li> <li>Keep patterns specific enough to catch regressions</li> </ul>"},{"location":"validation-tests/#3-not_contains","title":"3. not_contains","text":"<p>Verifies that the target content does NOT match a pattern.</p> <pre><code>assertions:\n  - type: not_contains\n    target: haproxy.cfg\n    pattern: \"server.*127.0.0.1\"\n    description: Should not have localhost servers in production\n\n  - type: not_contains\n    target: haproxy.cfg\n    pattern: \"ssl-verify none\"\n    description: Must not disable SSL verification\n</code></pre> <p>Use cases:</p> <ul> <li>Verify deprecated patterns are removed</li> <li>Ensure security-sensitive config is not present</li> <li>Check that test/debug settings don't leak to production</li> </ul>"},{"location":"validation-tests/#4-equals","title":"4. equals","text":"<p>Checks that the entire target content exactly matches the expected value.</p> <pre><code>assertions:\n  - type: equals\n    target: map:hostnames.map\n    expected: |\n      api.example.com backend-api\n      www.example.com backend-web\n    description: Hostname map must contain exactly these entries\n</code></pre> <p>When to use:</p> <ul> <li>Small, deterministic files (maps, simple configs)</li> <li>When order and whitespace matter</li> <li>Verifying complete file contents</li> </ul> <p>Not recommended for:</p> <ul> <li>Large HAProxy configs (too brittle)</li> <li>Content with timestamps or dynamic values</li> </ul>"},{"location":"validation-tests/#5-jsonpath","title":"5. jsonpath","text":"<p>Queries the template rendering context using JSONPath expressions.</p> <pre><code>assertions:\n  - type: jsonpath\n    jsonpath: \"{.resources.services.List()[0].metadata.name}\"\n    expected: \"my-service\"\n    description: First service should be my-service\n\n  - type: jsonpath\n    jsonpath: \"{.template_snippets[0]}\"\n    expected: \"logging\"\n    description: First snippet should be logging\n</code></pre> <p>Template Context Structure:</p> <pre><code>{\n  \"resources\": {\n    \"services\": &lt;StoreWrapper\n    with\n    .List()\n    method&gt;,\n    \"ingresses\": &lt;StoreWrapper&gt;\n  },\n  \"template_snippets\": [\n    \"snippet1\",\n    \"snippet2\"\n  ]\n}\n</code></pre> <p>JSONPath Examples:</p> <pre><code># Count resources\n- jsonpath: \"{.resources.services.List() | length}\"\n  expected: \"3\"\n\n# Check resource field\n- jsonpath: \"{.resources.ingresses.List()[0].spec.rules[0].host}\"\n  expected: \"api.example.com\"\n\n# Verify snippet order\n- jsonpath: \"{.template_snippets[0]}\"\n  expected: \"logging\"\n</code></pre>"},{"location":"validation-tests/#testing-strategies","title":"Testing Strategies","text":""},{"location":"validation-tests/#test-organization","title":"Test Organization","text":"<p>Group related tests by feature or scenario:</p> <pre><code>validationTests:\n  # Basic functionality\n  test-basic-http-routing:\n    description: HTTP routing for simple service\n    # ...\n\n  test-basic-load-balancing:\n    description: Load balancing across multiple servers\n    # ...\n\n  # TLS/SSL\n  test-tls-termination:\n    description: TLS termination with certificate\n    # ...\n\n  test-tls-passthrough:\n    description: TLS passthrough for end-to-end encryption\n    # ...\n\n  # Edge cases\n  test-empty-services:\n    description: Handle case with no backend services\n    # ...\n\n  test-invalid-port:\n    description: Gracefully handle invalid port numbers\n    # ...\n</code></pre>"},{"location":"validation-tests/#testing-template-errors","title":"Testing Template Errors","text":"<p>Test that templates fail gracefully with <code>fail()</code> function:</p> <pre><code>{% if not services.List() %}\n  {{ fail(\"At least one service is required\") }}\n{% endif %}\n</code></pre> <p>Test file:</p> <pre><code>validationTests:\n  test-no-services-error:\n    description: Should fail gracefully when no services exist\n    fixtures:\n      services: [ ]  # Empty services\n    assertions:\n    # This test will fail at rendering stage\n    # The test runner will capture the fail() message\n</code></pre> <p>Expected output:</p> <pre><code>\u2717 test-no-services-error\n  \u2717 Template rendering failed\n    Error: At least one service is required\n</code></pre>"},{"location":"validation-tests/#testing-multiple-scenarios","title":"Testing Multiple Scenarios","text":"<p>Use fixtures to test different deployment configurations:</p> <pre><code>validationTests:\n  # Production scenario\n  test-production-setup:\n    fixtures:\n      services:\n        - name: api\n          namespace: production\n        - name: web\n          namespace: production\n    assertions:\n      - type: contains\n        pattern: \"production\"\n\n  # Staging scenario\n  test-staging-setup:\n    fixtures:\n      services:\n        - name: api\n          namespace: staging\n    assertions:\n      - type: contains\n        pattern: \"staging\"\n\n  # Development scenario\n  test-dev-setup:\n    fixtures:\n      services:\n        - name: api\n          namespace: dev\n    assertions:\n      - type: contains\n        pattern: \"maxconn 100\"  # Lower limits for dev\n</code></pre>"},{"location":"validation-tests/#testing-auxiliary-files","title":"Testing Auxiliary Files","text":"<p>Test maps, general files, and SSL certificates:</p> <pre><code>validationTests:\n  - name: test-hostname-map\n    description: Hostname map should contain all ingress hosts\n    fixtures:\n      ingresses:\n        - metadata:\n            name: main\n          spec:\n            rules:\n              - host: api.example.com\n              - host: www.example.com\n    assertions:\n      - type: contains\n        target: map:hostnames.map\n        pattern: \"api.example.com\"\n\n      - type: contains\n        target: map:hostnames.map\n        pattern: \"www.example.com\"\n\n  - name: test-error-page\n    description: Custom error page should be generated\n    fixtures:\n      services: [ ]\n    assertions:\n      - type: contains\n        target: file:500.http\n        pattern: \"Internal Server Error\"\n</code></pre>"},{"location":"validation-tests/#running-tests","title":"Running Tests","text":""},{"location":"validation-tests/#cli-usage","title":"CLI Usage","text":"<pre><code># Run all tests\ncontroller validate -f config.yaml\n\n# Run specific test\ncontroller validate -f config.yaml --test test-basic-routing\n\n# Output as JSON (for CI/CD)\ncontroller validate -f config.yaml --output json\n\n# Output as YAML\ncontroller validate -f config.yaml --output yaml\n\n# Use custom HAProxy binary\ncontroller validate -f config.yaml --haproxy-binary /usr/local/bin/haproxy\n</code></pre>"},{"location":"validation-tests/#exit-codes","title":"Exit Codes","text":"<ul> <li>0: All tests passed</li> <li>Non-zero: One or more tests failed</li> </ul> <p>Use in CI/CD pipelines:</p> <pre><code>#!/bin/bash\ncontroller validate -f config.yaml\nif [ $? -ne 0 ]; then\n  echo \"Validation tests failed!\"\n  exit 1\nfi\n</code></pre>"},{"location":"validation-tests/#output-formats","title":"Output Formats","text":"<p>Summary (default):</p> <pre><code>\u2713 test-basic-routing (0.125s)\n  Basic HTTP routing configuration\n  \u2713 HAProxy configuration must be syntactically valid\n  \u2713 Must have frontend\n  \u2713 Must have backend\n\n\u2717 test-tls-config (0.089s)\n  TLS configuration\n  \u2717 Must have SSL certificate\n    Error: pattern \"ssl crt\" not found in haproxy.cfg\n\nTests: 1 passed, 1 failed, 2 total (0.214s)\n</code></pre> <p>JSON:</p> <pre><code>{\n  \"totalTests\": 2,\n  \"passedTests\": 1,\n  \"failedTests\": 1,\n  \"duration\": 0.214,\n  \"tests\": [\n    {\n      \"testName\": \"test-basic-routing\",\n      \"description\": \"Basic HTTP routing configuration\",\n      \"passed\": true,\n      \"duration\": 0.125,\n      \"assertions\": [\n        ...\n      ]\n    }\n  ]\n}\n</code></pre> <p>YAML:</p> <pre><code>totalTests: 2\npassedTests: 1\nfailedTests: 1\nduration: 0.214\ntests:\n  - testName: test-basic-routing\n    description: Basic HTTP routing configuration\n    passed: true\n    duration: 0.125\n    assertions: [ ... ]\n</code></pre>"},{"location":"validation-tests/#debugging-failed-tests","title":"Debugging Failed Tests","text":"<p>When validation tests fail, the <code>controller validate</code> command provides several flags to help diagnose issues quickly.</p>"},{"location":"validation-tests/#quick-debugging-with-verbose","title":"Quick Debugging with --verbose","text":"<p>The <code>--verbose</code> flag shows rendered content preview for failed assertions:</p> <pre><code>controller validate -f config.yaml --verbose\n</code></pre> <p>Example output:</p> <pre><code>\u2717 test-gateway-routing (0.004s)\n  \u2717 Path map must use MULTIBACKEND qualifier with total weight 100\n    Error: pattern \"split.example.com/app MULTIBACKEND:100:default_split-route_0/\" not found in map:path-prefix.map (target size: 61 bytes). Hint: Use --verbose to see content preview\n    Target: map:path-prefix.map (61 bytes)\n    Content preview:\n      split.example.com/app MULTIBACKEND:0:default_split-route_0/\n\n    Hint: Use --dump-rendered to see full content\n</code></pre> <p>What it shows: - Target file name and size (61 bytes) - First 200 characters of actual rendered content - Hint about <code>--dump-rendered</code> for full content</p> <p>When to use: - First step when tests fail - Quick check of what was actually rendered vs expected - Debugging pattern mismatches or unexpected values</p>"},{"location":"validation-tests/#complete-output-with-dump-rendered","title":"Complete Output with --dump-rendered","text":"<p>The <code>--dump-rendered</code> flag outputs all rendered content after test results:</p> <pre><code>controller validate -f config.yaml --dump-rendered\n</code></pre> <p>Output structure:</p> <pre><code>Tests: 0 passed, 1 failed, 1 total (0.003s)\n\n================================================================================\nRENDERED CONTENT\n================================================================================\n\n## Test: test-gateway-routing\n\n### haproxy.cfg\n--------------------------------------------------------------------------------\nglobal\n  daemon\n  maxconn 4000\n\ndefaults\n  mode http\n  timeout connect 5s\n--------------------------------------------------------------------------------\n\n### Map Files\n\n#### path-prefix.map\n--------------------------------------------------------------------------------\nsplit.example.com/app MULTIBACKEND:0:default_split-route_0/\n\n--------------------------------------------------------------------------------\n</code></pre> <p>What it shows: - Complete HAProxy configuration - All map files with names - General files (error pages, etc.) - SSL certificates</p> <p>When to use: - Need to see complete rendered output - Debugging complex template logic - Verifying exact output format - Creating bug reports with full context</p>"},{"location":"validation-tests/#template-execution-trace","title":"Template Execution Trace","text":"<p>The <code>--trace-templates</code> flag shows which templates were rendered and timing information:</p> <pre><code>controller validate -f config.yaml --trace-templates\n</code></pre> <p>Output:</p> <pre><code>TEMPLATE EXECUTION TRACE\n================================================================================\nRendering: haproxy.cfg\nCompleted: haproxy.cfg (0.007ms)\nRendering: path-prefix.map\nCompleted: path-prefix.map (3.347ms)\nRendering: weighted-multi-backend.map\nCompleted: weighted-multi-backend.map (2.105ms)\n</code></pre> <p>What it shows: - Order of template rendering - Duration for each template in milliseconds - Nesting depth for includes (shown via indentation)</p> <p>When to use: - Understanding template execution order - Performance debugging (identify slow templates) - Verifying template includes work correctly - Debugging missing or unexpected renders</p>"},{"location":"validation-tests/#enhanced-default-error-messages","title":"Enhanced Default Error Messages","text":"<p>All error messages now include helpful context without requiring any flags:</p> <p>Before (old format): <pre><code>Error: pattern \"X\" not found in map:path-prefix.map\n</code></pre></p> <p>After (enhanced format): <pre><code>Error: pattern \"X\" not found in map:path-prefix.map (target size: 61 bytes). Hint: Use --verbose to see content preview\n</code></pre></p> <p>Benefits: - Immediate visibility into target size - Clear hints about available debugging options - No need to re-run with flags for basic info</p>"},{"location":"validation-tests/#combining-flags","title":"Combining Flags","text":"<p>Flags can be combined for comprehensive debugging:</p> <pre><code>controller validate -f config.yaml --verbose --dump-rendered --trace-templates\n</code></pre> <p>This provides: 1. Content previews for all failed assertions (<code>--verbose</code>) 2. Complete rendered files (<code>--dump-rendered</code>) 3. Template execution trace (<code>--trace-templates</code>)</p> <p>Recommended workflow: 1. Start with <code>--verbose</code> for quick diagnosis 2. Add <code>--dump-rendered</code> if you need full content 3. Add <code>--trace-templates</code> for performance or execution flow issues</p>"},{"location":"validation-tests/#structured-output-with-rendered-content","title":"Structured Output with Rendered Content","text":"<p>JSON and YAML output formats now include rendered content and target metadata:</p> <pre><code>controller validate -f config.yaml --output yaml\n</code></pre> <p>New fields in output:</p> <pre><code>tests:\n  - testName: test-gateway-routing\n    passed: false\n    # Rendered content (available for all tests)\n    renderedConfig: |\n      global\n        daemon\n    renderedMaps:\n      path-prefix.map: |\n        split.example.com/app MULTIBACKEND:0:default_split-route_0/\n    renderedFiles: {}\n    renderedCerts: {}\n\n    assertions:\n      - type: contains\n        passed: false\n        error: \"pattern not found\"\n        # Target metadata (available for all assertions)\n        target: \"map:path-prefix.map\"\n        targetSize: 61\n        targetPreview: \"split.example.com/app MULTIBACKEND:0:...\"\n</code></pre> <p>Use cases: - CI/CD integration with detailed failure context - Automated debugging scripts - Archiving test results with full rendered content</p>"},{"location":"validation-tests/#troubleshooting-tips","title":"Troubleshooting Tips","text":"<p>Map file appears empty or has unexpected content:</p> <pre><code># See exactly what was rendered\ncontroller validate -f config.yaml --dump-rendered\n</code></pre> <p>Check for: - Missing template logic - Empty loops (no resources match) - Incorrect variable names</p> <p>Template not generating expected output:</p> <pre><code># See which templates were actually rendered\ncontroller validate -f config.yaml --trace-templates\n</code></pre> <p>If a template is missing from the trace: - Check template name spelling - Verify includes are correct - Check conditional logic preventing execution</p> <p>Pattern not matching rendered content:</p> <pre><code># See actual content vs expected pattern\ncontroller validate -f config.yaml --verbose\n</code></pre> <p>Common issues: - Whitespace differences (extra newlines, spaces) - Case sensitivity in patterns - Regex special characters need escaping - Multiline patterns require <code>(?m)</code> flag</p> <p>Slow validation or timeouts:</p> <pre><code># Identify slow templates\ncontroller validate -f config.yaml --trace-templates\n</code></pre> <p>Templates taking &gt;10ms may need optimization: - Simplify complex loops - Reduce nested includes - Cache expensive computations - Split large templates</p>"},{"location":"validation-tests/#best-practices","title":"Best Practices","text":""},{"location":"validation-tests/#1-test-early-and-often","title":"1. Test Early and Often","text":"<p>Add tests as you develop templates:</p> <pre><code># Development workflow\nvim config.yaml  # Edit templates\ncontroller validate -f config.yaml  # Run tests\n# Fix any failures, repeat\n</code></pre>"},{"location":"validation-tests/#2-keep-tests-fast","title":"2. Keep Tests Fast","text":"<ul> <li>Use minimal fixtures (only what's needed)</li> <li>Avoid excessive pattern matching</li> <li>Group related assertions in single tests</li> </ul>"},{"location":"validation-tests/#3-make-tests-readable","title":"3. Make Tests Readable","text":"<pre><code># Good: Clear, descriptive\n- name: test-ingress-tls-routing\n  description: Ingress with TLS should create HTTPS frontend and route to backend\n  assertions:\n    - type: contains\n      pattern: \"bind :443 ssl\"\n      description: HTTPS frontend must bind to port 443 with SSL\n\n# Bad: Unclear\n- name: test1\n  description: Test\n  assertions:\n    - type: contains\n      pattern: \"443\"\n</code></pre>"},{"location":"validation-tests/#4-test-edge-cases","title":"4. Test Edge Cases","text":"<pre><code>validationTests:\n  # Normal case\n  - name: test-single-service\n    fixtures:\n      services: [ ... ]\n    # ...\n\n  # Edge case: no services\n  - name: test-no-services\n    fixtures:\n      services: [ ]\n    # ...\n\n  # Edge case: many services\n  - name: test-many-services\n    fixtures:\n      services: [ ... 50 services ... ]\n    # ...\n</code></pre>"},{"location":"validation-tests/#5-document-expected-behavior","title":"5. Document Expected Behavior","text":"<p>Use test descriptions to document template behavior:</p> <pre><code>- name: test-weighted-load-balancing\n  description: |\n    Services with weight annotation should use weighted round-robin.\n    Weight is specified via 'haproxy.weight' annotation.\n    Default weight is 1 if not specified.\n</code></pre>"},{"location":"validation-tests/#troubleshooting","title":"Troubleshooting","text":""},{"location":"validation-tests/#test-fails-with-haproxy-command-not-found","title":"Test Fails with \"haproxy: command not found\"","text":"<p>Problem: HAProxy binary not in PATH.</p> <p>Solution: Specify binary location:</p> <pre><code>controller validate -f config.yaml --haproxy-binary /usr/local/bin/haproxy\n</code></pre>"},{"location":"validation-tests/#test-fails-with-template-rendering-failed","title":"Test Fails with \"template rendering failed\"","text":"<p>Problem: Template syntax error or failed assertion from <code>fail()</code>.</p> <p>Solution: Check the error message for details. Common issues:</p> <ul> <li>Undefined variables: <code>{{ undefined_var }}</code></li> <li>Missing filters: <code>{{ value | missing_filter }}</code></li> <li>Logic errors in conditionals</li> </ul>"},{"location":"validation-tests/#pattern-not-matching","title":"Pattern Not Matching","text":"<p>Problem: <code>contains</code> assertion fails but content looks correct.</p> <p>Solution:</p> <ol> <li>Check regex syntax (escape special characters)</li> <li>Check for whitespace differences</li> <li>Use simpler patterns: <code>api</code> instead of <code>backend\\s+api.*</code></li> <li>Add <code>-o json</code> to see full rendered config</li> </ol>"},{"location":"validation-tests/#jsonpath-returns-no-results","title":"JSONPath Returns No Results","text":"<p>Problem: JSONPath assertion fails with \"returned no results\".</p> <p>Solution:</p> <ul> <li>Verify the path syntax: <code>{.resources.services}</code></li> <li>Check if the resource type exists in fixtures</li> <li>Use <code>List()</code> method to access resources: <code>{.resources.services.List()}</code></li> </ul>"},{"location":"validation-tests/#examples","title":"Examples","text":""},{"location":"validation-tests/#complete-example-ingress-routing","title":"Complete Example: Ingress Routing","text":"<pre><code>apiVersion: haproxy-template-ic.github.io/v1alpha1\nkind: HAProxyTemplateConfig\nmetadata:\n  name: ingress-routing\nspec:\n  watchedResources:\n    services:\n      apiVersion: v1\n      resources: services\n      indexBy: [ \"metadata.namespace\", \"metadata.name\" ]\n    ingresses:\n      apiVersion: networking.k8s.io/v1\n      resources: ingresses\n      indexBy: [ \"metadata.namespace\", \"metadata.name\" ]\n\n  haproxyConfig:\n    template: |\n      global\n        daemon\n\n      defaults\n        mode http\n        timeout connect 5s\n        timeout client 30s\n        timeout server 30s\n\n      frontend http\n        bind :80\n        {% for ingress in resources.ingresses.List() %}\n        {% for rule in ingress.spec.rules %}\n        acl host_{{ rule.host | replace(\".\", \"_\") }} hdr(host) -i {{ rule.host }}\n        use_backend {{ rule.host | replace(\".\", \"_\") }}_backend if host_{{ rule.host | replace(\".\", \"_\") }}\n        {% endfor %}\n        {% endfor %}\n\n      {% for ingress in resources.ingresses.List() %}\n      {% for rule in ingress.spec.rules %}\n      backend {{ rule.host | replace(\".\", \"_\") }}_backend\n        balance roundrobin\n        {% set svc_name = rule.http.paths[0].backend.service.name %}\n        {% set svc = resources.services.Get(ingress.metadata.namespace, svc_name) %}\n        {% if svc %}\n        server svc1 {{ svc.spec.clusterIP }}:{{ svc.spec.ports[0].port }} check\n        {% endif %}\n      {% endfor %}\n      {% endfor %}\n\n  validationTests:\n    test-single-ingress:\n      description: Single ingress should create frontend ACL and backend\n      fixtures:\n        services:\n          - apiVersion: v1\n            kind: Service\n            metadata:\n              name: api\n              namespace: default\n            spec:\n              clusterIP: 10.0.0.100\n              ports:\n                - port: 80\n        ingresses:\n          - apiVersion: networking.k8s.io/v1\n            kind: Ingress\n            metadata:\n              name: main\n              namespace: default\n            spec:\n              rules:\n                - host: api.example.com\n                  http:\n                    paths:\n                      - path: /\n                        backend:\n                          service:\n                            name: api\n                            port:\n                              number: 80\n      assertions:\n        - type: haproxy_valid\n          description: Configuration must be valid\n\n        - type: contains\n          target: haproxy.cfg\n          pattern: \"acl host_api_example_com hdr\\\\(host\\\\) -i api.example.com\"\n          description: Must have ACL for api.example.com\n\n        - type: contains\n          target: haproxy.cfg\n          pattern: \"backend api_example_com_backend\"\n          description: Must have backend for api.example.com\n\n        - type: contains\n          target: haproxy.cfg\n          pattern: \"server svc1 10.0.0.100:80 check\"\n          description: Must have server pointing to service ClusterIP\n</code></pre>"},{"location":"validation-tests/#next-steps","title":"Next Steps","text":"<ul> <li>Read the Template Guide for template syntax</li> <li>See Supported Configuration for HAProxy directives</li> <li>Check Troubleshooting for common issues</li> </ul>"},{"location":"watching-resources/","title":"Watched Resources Configuration","text":""},{"location":"watching-resources/#overview","title":"Overview","text":"<p>The <code>watched_resources</code> configuration determines which Kubernetes resources the controller monitors and how it stores them in memory. Each resource type you configure creates a watcher that tracks changes and maintains an indexed store for template access.</p> <p>This configuration is critical for: - Performance: Choosing the right storage strategy affects memory usage and template rendering speed - Functionality: Resources must be watched to be accessible in templates - Scalability: Proper store selection allows operation in memory-constrained environments</p> <p>The controller supports two storage strategies: memory store (full in-memory storage) and cached store (on-demand API-backed storage). Understanding when to use each is essential for optimal controller performance.</p> <p>For complete configuration syntax, see Configuration Reference.</p>"},{"location":"watching-resources/#understanding-store-types","title":"Understanding Store Types","text":"<p>The controller provides two store implementations with different trade-offs:</p> Aspect Memory Store (<code>store: full</code> or default) Cached Store (<code>store: on-demand</code>) Storage Complete resources in memory Only references in memory, resources fetched from API Lookup Performance O(1), instant O(1) with cache hit, API latency on cache miss Memory Usage ~1KB per resource Minimal (only references + cache) API Load Initial list only Initial list + fetch on cache miss Best For Iterating over all resources Selective resource access Template Method <code>.List()</code> efficient <code>.Fetch()</code> efficient <p>How they work:</p> <p>Memory Store: When a resource changes, the watcher stores the complete resource object in memory. Template rendering accesses this in-memory copy directly.</p> <p>Cached Store: When a resource changes, the watcher stores only a reference (namespace + name + index keys). When a template accesses the resource via <code>.Fetch()</code>, the store checks its TTL cache. On cache miss, it fetches from the Kubernetes API and caches the result.</p>"},{"location":"watching-resources/#memory-store","title":"Memory Store","text":"<p>The memory store is the default storage strategy. It keeps complete resource objects in memory for fast access.</p>"},{"location":"watching-resources/#how-it-works","title":"How It Works","text":"<p>When you configure a resource with memory store:</p> <ol> <li>The watcher performs an initial list operation to load all existing resources</li> <li>Each resource is stored completely in memory after field filtering</li> <li>Resources are indexed using the <code>index_by</code> configuration for O(1) lookups</li> <li>Real-time changes (add/update/delete) update the in-memory store</li> <li>Template rendering accesses resources directly from memory</li> </ol>"},{"location":"watching-resources/#when-to-use-memory-store","title":"When to Use Memory Store","text":"<p>Use memory store when you will: - Iterate over all resources frequently (e.g., generating map files with all ingresses) - Access most resources during each template render - Need fastest template rendering (no API latency) - Work with small to medium resources (Ingress, Service, EndpointSlice)</p> <p>Example use cases: - Ingress resources for generating HAProxy routing rules - Service resources for backend configuration - EndpointSlice resources for dynamic server lists - ConfigMap resources for configuration snippets</p>"},{"location":"watching-resources/#memory-usage","title":"Memory Usage","text":"<p>Approximate memory usage per resource: - Ingress: ~1-2 KB (depends on number of rules and paths) - Service: ~1 KB (simple services) to ~5 KB (many ports/annotations) - EndpointSlice: ~2-5 KB (depends on number of endpoints) - ConfigMap/Secret: ~1 KB + data size</p> <p>Example: 1000 Ingress resources \u2248 1-2 MB memory</p>"},{"location":"watching-resources/#configuration","title":"Configuration","text":"<p>Configure memory store explicitly or use the default:</p> <pre><code>watched_resources:\n  ingresses:\n    api_version: networking.k8s.io/v1\n    kind: Ingress\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n    # Memory store is the default\n    store: full  # Optional - this is the default\n\n  services:\n    api_version: v1\n    kind: Service\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n    # Omitting 'store' uses memory store by default\n</code></pre> <p>See Configuration Reference for complete field descriptions.</p>"},{"location":"watching-resources/#cached-store","title":"Cached Store","text":"<p>The cached store minimizes memory usage by storing only resource references and fetching complete resources on-demand from the Kubernetes API.</p>"},{"location":"watching-resources/#how-it-works_1","title":"How It Works","text":"<p>When you configure a resource with cached store:</p> <ol> <li>The watcher performs an initial list operation to discover resources</li> <li>Only references are stored (namespace, name, and index keys)</li> <li>Resources are indexed using the <code>index_by</code> configuration</li> <li>When a template calls <code>.Fetch()</code>, the store checks its TTL cache</li> <li>On cache miss, the store fetches the resource from the Kubernetes API</li> <li>Fetched resources are cached for the configured TTL duration</li> <li>Real-time changes update the reference index but don't fetch the full resource</li> </ol>"},{"location":"watching-resources/#when-to-use-cached-store","title":"When to Use Cached Store","text":"<p>Use cached store when you will: - Access resources selectively (e.g., fetch specific secrets by name) - Work with large resources (Secrets with certificate data, large ConfigMaps) - Need to minimize memory usage (hundreds of secrets or large data) - Can tolerate API latency on cache misses</p> <p>Example use cases: - Secrets containing TLS certificates (large PEM data) - Secrets for authentication (accessed by annotation reference) - ConfigMaps with large configuration data - Resources accessed conditionally in templates</p>"},{"location":"watching-resources/#trade-offs","title":"Trade-offs","text":"<p>Benefits: - Minimal memory footprint (only references + cache) - Suitable for large resource types - Cache reduces API calls for frequently accessed resources</p> <p>Costs: - API latency on cache misses (typically 10-50ms) - Additional load on Kubernetes API server - Slightly slower template rendering when cache misses occur</p>"},{"location":"watching-resources/#configuration_1","title":"Configuration","text":"<p>Configure cached store with the <code>store: on-demand</code> setting:</p> <pre><code>watched_resources:\n  secrets:\n    api_version: v1\n    kind: Secret\n    store: on-demand  # Use cached store\n    cache_ttl: 2m10s  # Optional: cache duration (default: 2m10s)\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n</code></pre> <p>Cache TTL considerations: - Shorter TTL (1-2 minutes): More API calls, fresher data - Longer TTL (5-10 minutes): Fewer API calls, may show stale data between template renders - Default (2m10s): Balanced approach for most use cases</p> <p>See Configuration Reference for complete field descriptions.</p>"},{"location":"watching-resources/#indexing-with-index_by","title":"Indexing with index_by","text":"<p>The <code>index_by</code> configuration determines how resources are indexed for lookups and what parameters the <code>.Fetch()</code> method expects in templates.</p>"},{"location":"watching-resources/#understanding-indexing","title":"Understanding Indexing","text":"<p>Resources are indexed using composite keys constructed from JSONPath expressions. This enables O(1) lookup performance for specific resources.</p> <p>Example: Indexing by namespace and name: <pre><code>index_by: [\"metadata.namespace\", \"metadata.name\"]\n</code></pre></p> <p>Creates composite keys like: - <code>default/my-ingress</code> \u2192 [Ingress resource] - <code>production/api-gateway</code> \u2192 [Ingress resource]</p> <p>How lookups work: <pre><code>{# In templates, Fetch() parameters match index_by order #}\n{% for ingress in resources.ingresses.Fetch(\"default\", \"my-ingress\") %}\n  {# Returns the specific ingress #}\n{% endfor %}\n</code></pre></p>"},{"location":"watching-resources/#common-indexing-patterns","title":"Common Indexing Patterns","text":""},{"location":"watching-resources/#pattern-1-by-namespace-and-name","title":"Pattern 1: By Namespace and Name","text":"<p>Most common pattern for standard Kubernetes resources:</p> <pre><code>watched_resources:\n  ingresses:\n    api_version: networking.k8s.io/v1\n    kind: Ingress\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n</code></pre> <p>Use when: - Looking up specific resources by namespace and name - Standard resource types (Ingress, Service, ConfigMap, Secret)</p> <p>Template usage: <pre><code>{# Fetch specific ingress #}\n{% for ingress in resources.ingresses.Fetch(\"default\", \"my-app\") %}\n  {# Process specific ingress #}\n{% endfor %}\n\n{# Fetch all in namespace #}\n{% for ingress in resources.ingresses.Fetch(\"default\") %}\n  {# Process all ingresses in 'default' namespace #}\n{% endfor %}\n</code></pre></p> <p>See Configuration Reference for <code>index_by</code> syntax.</p>"},{"location":"watching-resources/#pattern-2-by-service-name-for-endpointslices","title":"Pattern 2: By Service Name (for EndpointSlices)","text":"<p>Essential pattern for mapping services to endpoints:</p> <pre><code>watched_resources:\n  endpoints:\n    api_version: discovery.k8s.io/v1\n    kind: EndpointSlice\n    index_by: [\"metadata.labels.kubernetes\\\\.io/service-name\"]\n</code></pre> <p>Why this works: EndpointSlices are labeled with <code>kubernetes.io/service-name</code>, allowing O(1) lookup of all endpoint slices for a service.</p> <p>Template usage: <pre><code>{# In ingress loop, extract service name #}\n{% set service_name = path.backend.service.name %}\n\n{# Fetch all endpoint slices for this service #}\n{% for endpoint_slice in resources.endpoints.Fetch(service_name) %}\n  {# Generate server entries from endpoints #}\n  {% for endpoint in (endpoint_slice.endpoints | default([])) %}\n    {% for address in (endpoint.addresses | default([])) %}\n      server {{ endpoint.targetRef.name }} {{ address }}:{{ port }} check\n    {% endfor %}\n  {% endfor %}\n{% endfor %}\n</code></pre></p> <p>This is the most important cross-resource lookup pattern in HAProxy configurations.</p> <p>See Configuration Reference for label selector syntax.</p>"},{"location":"watching-resources/#pattern-3-by-type-for-secrets","title":"Pattern 3: By Type (for Secrets)","text":"<p>Useful pattern for grouping secrets by type:</p> <pre><code>watched_resources:\n  secrets:\n    api_version: v1\n    kind: Secret\n    store: on-demand\n    index_by: [\"metadata.namespace\", \"type\"]\n</code></pre> <p>Template usage: <pre><code>{# Fetch all TLS secrets in namespace #}\n{% for secret in resources.secrets.Fetch(\"default\", \"kubernetes.io/tls\") %}\n  {# Generate SSL certificate files #}\n{% endfor %}\n</code></pre></p>"},{"location":"watching-resources/#pattern-4-single-key-index","title":"Pattern 4: Single Key Index","text":"<p>Simplified pattern when resources are uniquely identified by one field:</p> <pre><code>watched_resources:\n  configmaps:\n    api_version: v1\n    kind: ConfigMap\n    namespace: haproxy-system  # Watch single namespace\n    index_by: [\"metadata.name\"]\n</code></pre> <p>Template usage: <pre><code>{# Fetch by name only (namespace is implicit) #}\n{% for cm in resources.configmaps.Fetch(\"error-pages\") %}\n  {# Use configmap data #}\n{% endfor %}\n</code></pre></p>"},{"location":"watching-resources/#choosing-index-strategy","title":"Choosing Index Strategy","text":"<p>Consider these questions when choosing <code>index_by</code>:</p> <ol> <li>How will you look up resources in templates?</li> <li>By namespace + name \u2192 Use both in <code>index_by</code></li> <li>By label value \u2192 Use label in <code>index_by</code></li> <li> <p>By type \u2192 Include type in <code>index_by</code></p> </li> <li> <p>What cross-resource relationships exist?</p> </li> <li>Service \u2192 Endpoints: Index endpoints by service name label</li> <li> <p>Ingress \u2192 Secret: Index secrets by namespace + name</p> </li> <li> <p>Is the index unique or non-unique?</p> </li> <li>Unique (namespace + name): Returns 0 or 1 resource</li> <li>Non-unique (service name): Returns 0+ resources</li> </ol> <p>Example decision: For EndpointSlices, you need to find all slices for a service name. Index by the service name label (non-unique) rather than namespace + name (unique).</p> <p>See Configuration Reference for additional indexing examples.</p>"},{"location":"watching-resources/#non-unique-indexes","title":"Non-Unique Indexes","text":"<p>Some index configurations intentionally map multiple resources to the same key:</p> <pre><code># Multiple endpoint slices can have the same service-name label\nindex_by: [\"metadata.labels.kubernetes\\\\.io/service-name\"]\n</code></pre> <p>Result: <code>.Fetch(\"my-service\")</code> returns all endpoint slices labeled with <code>my-service</code>.</p> <p>This is the correct behavior for one-to-many relationships.</p>"},{"location":"watching-resources/#accessing-resources-in-templates","title":"Accessing Resources in Templates","text":"<p>Resources are accessed in templates through the <code>resources</code> variable, which contains stores for all configured resource types.</p>"},{"location":"watching-resources/#the-resources-variable","title":"The resources Variable","text":"<p>The <code>resources</code> variable structure matches your <code>watched_resources</code> configuration:</p> <pre><code># Configuration\nwatched_resources:\n  ingresses: {...}\n  services: {...}\n  endpoints: {...}\n  secrets: {...}\n</code></pre> <pre><code>{# Template access #}\n{{ resources.ingresses }}  {# Store for ingresses #}\n{{ resources.services }}   {# Store for services #}\n{{ resources.endpoints }}  {# Store for endpoints #}\n{{ resources.secrets }}    {# Store for secrets #}\n</code></pre> <p>Each store provides two methods: <code>.List()</code> and <code>.Fetch()</code>.</p>"},{"location":"watching-resources/#using-list-method","title":"Using List() Method","text":"<p>The <code>.List()</code> method returns all resources in the store.</p> <p>Best for: - Iterating over all resources - Generating configuration for every resource - Counting total resources - Memory store (efficient)</p> <p>Example - Generate routing map for all ingresses: <pre><code>{# maps/host.map template #}\n{% for ingress in resources.ingresses.List() %}\n  {% for rule in (ingress.spec.rules | default([])) %}\n    {{ rule.host }} backend_{{ ingress.metadata.name }}\n  {% endfor %}\n{% endfor %}\n</code></pre></p> <p>Example - Count resources: <pre><code>{# Total ingresses: {{ resources.ingresses.List() | length }} #}\n</code></pre></p> <p>Performance note: With memory store, <code>.List()</code> returns in-memory objects (fast). With cached store, <code>.List()</code> still returns references, but template rendering will trigger fetches for each resource (slower).</p>"},{"location":"watching-resources/#using-fetch-method","title":"Using Fetch() Method","text":"<p>The <code>.Fetch()</code> method returns resources matching index keys.</p> <p>Best for: - Looking up specific resources - Cross-resource relationships (service \u2192 endpoints) - Conditional resource access - Cached store (efficient)</p> <p>Parameters: Match the <code>index_by</code> configuration order.</p> <p>Example - Fetch specific ingress: <pre><code># Configuration\nindex_by: [\"metadata.namespace\", \"metadata.name\"]\n</code></pre></p> <pre><code>{# Template #}\n{% for ingress in resources.ingresses.Fetch(\"default\", \"my-app\") %}\n  {# Usually returns 0 or 1 items #}\n{% endfor %}\n</code></pre> <p>Example - Fetch all in namespace: <pre><code>{# Partial key match - returns all in namespace #}\n{% for ingress in resources.ingresses.Fetch(\"default\") %}\n  {# All ingresses in 'default' namespace #}\n{% endfor %}\n</code></pre></p> <p>Example - Cross-resource lookup: <pre><code>{# Get endpoints for a service #}\n{% set service_name = path.backend.service.name %}\n{% for endpoint_slice in resources.endpoints.Fetch(service_name) %}\n  {# All endpoint slices for this service #}\n{% endfor %}\n</code></pre></p> <p>Performance note: With cached store, <code>.Fetch()</code> uses the cache effectively. Only fetches from API on cache miss.</p>"},{"location":"watching-resources/#method-comparison","title":"Method Comparison","text":"Method Memory Store Cached Store Use Case <code>.List()</code> Fast (in-memory) Slow (fetches all) Iterate over all resources <code>.Fetch(keys)</code> Fast (indexed lookup) Fast (cached/on-demand) Lookup specific resources <p>Rule of thumb: - Use <code>.List()</code> when you need most or all resources - Use <code>.Fetch()</code> when you need specific resources - With memory store, both are fast - With cached store, prefer <code>.Fetch()</code> for selective access</p>"},{"location":"watching-resources/#getsingle-helper","title":"GetSingle() Helper","text":"<p>For convenience, stores also provide <code>.GetSingle()</code> which returns a single resource or <code>null</code>:</p> <pre><code>{# Instead of looping #}\n{% set secret = resources.secrets.GetSingle(\"default\", \"my-secret\") %}\n{% if secret %}\n  {{ secret.data.password | b64decode }}\n{% endif %}\n</code></pre> <p>This is equivalent to: <pre><code>{% for secret in resources.secrets.Fetch(\"default\", \"my-secret\") %}\n  {{ secret.data.password | b64decode }}\n{% endfor %}\n</code></pre></p> <p>See Templating Guide for more template patterns.</p>"},{"location":"watching-resources/#performance-implications","title":"Performance Implications","text":"<p>The choice between memory and cached stores affects multiple performance dimensions:</p>"},{"location":"watching-resources/#memory-usage_1","title":"Memory Usage","text":"<p>Memory Store: - Per-resource overhead: 1-10 KB depending on resource type - Total usage: Number of resources \u00d7 average size - Example: 1000 ingresses \u00d7 1.5 KB = 1.5 MB</p> <p>Cached Store: - Per-reference overhead: ~100 bytes (namespace, name, index keys) - Cache overhead: Cache size \u00d7 average resource size - Example: 1000 secret references \u00d7 100 bytes = 100 KB (plus cache)</p> <p>Memory savings example: - 1000 secrets with 10 KB certificates each - Memory store: 10 MB - Cached store: 100 KB + cache (bounded by cache_ttl and access patterns)</p>"},{"location":"watching-resources/#template-rendering-time","title":"Template Rendering Time","text":"<p>Memory Store: - <code>.List()</code>: &lt;1ms (return in-memory slice) - <code>.Fetch()</code>: &lt;1ms (indexed lookup) - No API calls during rendering</p> <p>Cached Store: - <code>.List()</code>: Triggers fetch for all resources (slow) - <code>.Fetch()</code> with cache hit: &lt;1ms (cached lookup) - <code>.Fetch()</code> with cache miss: 10-50ms (API call) - API calls add latency</p> <p>Rendering time example: - Template accessing 10 secrets - Memory store: ~1ms total - Cached store (all cached): ~1ms total - Cached store (all misses): ~100-500ms total</p>"},{"location":"watching-resources/#kubernetes-api-load","title":"Kubernetes API Load","text":"<p>Memory Store: - Initial sync: Single list operation per resource type - Updates: Watch events only (efficient) - Template rendering: No API calls</p> <p>Cached Store: - Initial sync: Single list operation (same as memory) - Updates: Watch events only (same as memory) - Template rendering: API calls on cache misses - Additional load: Depends on cache hit rate</p> <p>API call estimation: - 100 template renders per minute - 10 secrets accessed per render - 50% cache hit rate - API calls: 100 \u00d7 10 \u00d7 0.5 = 500 per minute</p>"},{"location":"watching-resources/#performance-recommendations","title":"Performance Recommendations","text":"<p>Optimize for memory store: - Use field filtering to remove unnecessary data (see configuration.md#watched_resources_ignore_fields) - Watch only necessary namespaces - Use label selectors to limit resource count</p> <p>Optimize for cached store: - Set appropriate cache TTL (balance freshness vs API load) - Use <code>.Fetch()</code> instead of <code>.List()</code> in templates - Access resources conditionally (don't fetch unless needed) - Consider cache pre-warming for frequently accessed resources</p>"},{"location":"watching-resources/#choosing-the-right-store-type","title":"Choosing the Right Store Type","text":"<p>Use this decision guide to select the appropriate store type for each resource:</p>"},{"location":"watching-resources/#use-memory-store-when","title":"Use Memory Store When:","text":"<ul> <li>\u2705 Resources are small to medium size (&lt;10 KB)</li> <li>\u2705 You iterate over most/all resources in templates</li> <li>\u2705 You need fastest possible template rendering</li> <li>\u2705 Memory is not constrained</li> <li>\u2705 Resource count is manageable (&lt;10,000)</li> </ul> <p>Common resources for memory store: - Ingress resources - Service resources - EndpointSlice resources - Small ConfigMap resources - Deployment/Pod resources (if watching)</p>"},{"location":"watching-resources/#use-cached-store-when","title":"Use Cached Store When:","text":"<ul> <li>\u2705 Resources are large (&gt;10 KB)</li> <li>\u2705 You access resources selectively by key</li> <li>\u2705 Memory is constrained</li> <li>\u2705 Resource count is high (&gt;1,000)</li> <li>\u2705 Can tolerate API latency on cache misses</li> </ul> <p>Common resources for cached store: - Secrets containing certificates (large PEM data) - Secrets for authentication (selective access via annotations) - Large ConfigMap resources - Resources with high volume but infrequent access</p>"},{"location":"watching-resources/#decision-matrix","title":"Decision Matrix","text":"Resource Type Typical Size Access Pattern Recommended Store Ingress 1-2 KB Iterate all Memory Service 1-5 KB Iterate all Memory EndpointSlice 2-5 KB Lookup by service Memory Secret (TLS) 5-20 KB Lookup by name Cached Secret (auth) &lt;1 KB Lookup by annotation Cached ConfigMap (small) &lt;5 KB Iterate all Memory ConfigMap (large) &gt;10 KB Selective access Cached"},{"location":"watching-resources/#mixed-store-configuration","title":"Mixed Store Configuration","text":"<p>You can use different stores for different resource types:</p> <pre><code>watched_resources:\n  # High-frequency access, small resources\n  ingresses:\n    api_version: networking.k8s.io/v1\n    kind: Ingress\n    store: full  # Memory store\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n\n  services:\n    api_version: v1\n    kind: Service\n    store: full  # Memory store\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n\n  endpoints:\n    api_version: discovery.k8s.io/v1\n    kind: EndpointSlice\n    store: full  # Memory store\n    index_by: [\"metadata.labels.kubernetes\\\\.io/service-name\"]\n\n  # Selective access, large resources\n  secrets:\n    api_version: v1\n    kind: Secret\n    store: on-demand  # Cached store\n    cache_ttl: 2m\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n</code></pre> <p>This is the recommended approach for most deployments.</p> <p>See Configuration Reference for complete configuration options.</p>"},{"location":"watching-resources/#configuration-examples","title":"Configuration Examples","text":""},{"location":"watching-resources/#example-1-standard-ingress-controller-configuration","title":"Example 1: Standard Ingress Controller Configuration","text":"<p>Typical configuration for an ingress controller with memory store:</p> <pre><code>watched_resources:\n  ingresses:\n    api_version: networking.k8s.io/v1\n    kind: Ingress\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n    # Memory store (default)\n\n  services:\n    api_version: v1\n    kind: Service\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n\n  endpoints:\n    api_version: discovery.k8s.io/v1\n    kind: EndpointSlice\n    index_by: [\"metadata.labels.kubernetes\\\\.io/service-name\"]\n</code></pre> <p>Template usage: <pre><code>{# Generate backends with dynamic servers #}\n{% for ingress in resources.ingresses.List() %}\n  {% for rule in (ingress.spec.rules | default([])) %}\n    {% for path in (rule.http.paths | default([])) %}\n      {% set service_name = path.backend.service.name %}\n      {% set port = path.backend.service.port.number %}\n\n      backend ing_{{ ingress.metadata.name }}_{{ service_name }}\n        balance roundrobin\n        {# Cross-resource lookup: service \u2192 endpoints #}\n        {% for endpoint_slice in resources.endpoints.Fetch(service_name) %}\n          {% for endpoint in (endpoint_slice.endpoints | default([])) %}\n            {% for address in (endpoint.addresses | default([])) %}\n              server {{ endpoint.targetRef.name }} {{ address }}:{{ port }} check\n            {% endfor %}\n          {% endfor %}\n        {% endfor %}\n    {% endfor %}\n  {% endfor %}\n{% endfor %}\n</code></pre></p>"},{"location":"watching-resources/#example-2-authentication-with-cached-secrets","title":"Example 2: Authentication with Cached Secrets","text":"<p>Configuration using cached store for authentication secrets:</p> <pre><code>watched_resources:\n  ingresses:\n    api_version: networking.k8s.io/v1\n    kind: Ingress\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n\n  # Secrets accessed selectively via annotations\n  secrets:\n    api_version: v1\n    kind: Secret\n    store: on-demand  # Cached store\n    cache_ttl: 5m\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n</code></pre> <p>Template usage: <pre><code>{# Generate userlist sections for basic auth #}\n{% set ns = namespace(processed=[]) %}\n{% for ingress in resources.ingresses.List() %}\n  {% set auth_type = ingress.metadata.annotations[\"haproxy.org/auth-type\"] | default(\"\") %}\n  {% if auth_type == \"basic-auth\" %}\n    {% set auth_secret = ingress.metadata.annotations[\"haproxy.org/auth-secret\"] | default(\"\") %}\n    {% if auth_secret and auth_secret not in ns.processed %}\n      {% set ns.processed = ns.processed + [auth_secret] %}\n\n      {# Parse namespace/name from annotation #}\n      {% if \"/\" in auth_secret %}\n        {% set secret_ns = auth_secret.split(\"/\")[0] %}\n        {% set secret_name = auth_secret.split(\"/\")[1] %}\n      {% else %}\n        {% set secret_ns = ingress.metadata.namespace %}\n        {% set secret_name = auth_secret %}\n      {% endif %}\n\n      {# Fetch specific secret (cached or API call) #}\n      {% set secret = resources.secrets.GetSingle(secret_ns, secret_name) %}\n      {% if secret %}\n        userlist auth_{{ secret_ns }}_{{ secret_name }}\n        {% for username in secret.data | default({}) %}\n          user {{ username }} password {{ secret.data[username] | b64decode }}\n        {% endfor %}\n      {% endif %}\n    {% endif %}\n  {% endif %}\n{% endfor %}\n</code></pre></p> <p>Why cached store here: Secrets are large and accessed selectively (only those referenced in annotations). Cached store minimizes memory usage.</p> <p>See Templating Guide for complete authentication examples.</p>"},{"location":"watching-resources/#example-3-tls-certificates-with-cached-store","title":"Example 3: TLS Certificates with Cached Store","text":"<p>Configuration for TLS certificates using cached store:</p> <pre><code>watched_resources:\n  ingresses:\n    api_version: networking.k8s.io/v1\n    kind: Ingress\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n\n  # Large TLS secrets accessed conditionally\n  secrets:\n    api_version: v1\n    kind: Secret\n    store: on-demand\n    cache_ttl: 10m  # Longer TTL for certificates\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n</code></pre> <p>Template usage: <pre><code>{# Generate SSL certificates from ingress TLS configuration #}\n{% for ingress in resources.ingresses.List() %}\n  {% if ingress.spec.tls %}\n    {% for tls in ingress.spec.tls %}\n      {% set secret_name = tls.secretName %}\n      {% set namespace = ingress.metadata.namespace %}\n\n      {# Fetch TLS secret only if ingress has TLS #}\n      {% set secret = resources.secrets.GetSingle(namespace, secret_name) %}\n      {% if secret %}\n        {# SSL certificate content for {{ namespace }}/{{ secret_name }} #}\n        {{ secret.data[\"tls.crt\"] | b64decode }}\n        {{ secret.data[\"tls.key\"] | b64decode }}\n      {% endif %}\n    {% endfor %}\n  {% endif %}\n{% endfor %}\n</code></pre></p> <p>Why cached store here: Certificate secrets are 5-20 KB each. Only ingresses with TLS need certificate data. Cached store avoids loading all certificates into memory.</p>"},{"location":"watching-resources/#example-4-namespace-restricted-watching","title":"Example 4: Namespace-Restricted Watching","text":"<p>Watch resources only in specific namespaces:</p> <pre><code>watched_resources:\n  ingresses:\n    api_version: networking.k8s.io/v1\n    kind: Ingress\n    namespace: production  # Only watch 'production' namespace\n    index_by: [\"metadata.name\"]  # Single key (namespace is implicit)\n\n  services:\n    api_version: v1\n    kind: Service\n    namespace: production\n    index_by: [\"metadata.name\"]\n</code></pre> <p>Benefits: - Reduced memory usage (fewer resources) - Reduced API load (narrower watch scope) - Simplified indexing (single key)</p> <p>Template usage: <pre><code>{# Fetch by name only (namespace is implicit) #}\n{% for ingress in resources.ingresses.Fetch(\"api-gateway\") %}\n  {# ... #}\n{% endfor %}\n</code></pre></p> <p>See Configuration Reference for namespace restrictions.</p>"},{"location":"watching-resources/#troubleshooting","title":"Troubleshooting","text":""},{"location":"watching-resources/#store-returns-empty-results","title":"Store Returns Empty Results","text":"<p>Symptoms: - <code>.List()</code> returns empty array - <code>.Fetch()</code> returns no resources - Templates generate empty output</p> <p>Diagnosis:</p> <ol> <li> <p>Check if watcher has completed initial sync: <pre><code># Check controller logs for sync completion\nkubectl logs deployment/haproxy-template-ic | grep \"initial sync complete\"\n</code></pre></p> </li> <li> <p>Verify resources exist matching the watch configuration: <pre><code># Check if resources exist\nkubectl get ingresses -A\n\n# Check if resources match label selector (if configured)\nkubectl get ingresses -A -l app=myapp\n</code></pre></p> </li> <li> <p>Verify RBAC permissions: <pre><code># Check if service account can list resources\nkubectl auth can-i list ingresses --all-namespaces \\\n  --as=system:serviceaccount:haproxy-system:haproxy-template-ic\n</code></pre></p> </li> <li> <p>Check index_by configuration matches template usage: <pre><code># Configuration\nindex_by: [\"metadata.namespace\", \"metadata.name\"]\n</code></pre> <pre><code>{# Template must match index order #}\n{% for ing in resources.ingresses.Fetch(\"default\", \"my-app\") %}\n</code></pre></p> </li> </ol> <p>Solutions: - Wait for initial sync to complete - Verify resource selectors (namespace, labels) - Fix RBAC permissions - Align template <code>.Fetch()</code> calls with <code>index_by</code> configuration</p>"},{"location":"watching-resources/#high-memory-usage","title":"High Memory Usage","text":"<p>Symptoms: - Controller pod OOMKilled - High resident memory (RSS) - Memory growth over time</p> <p>Diagnosis:</p> <ol> <li> <p>Count resources in each store: <pre><code># Enable debug port and check /debug/vars endpoint\nkubectl port-forward pod/haproxy-template-ic-xxx 6060:6060\ncurl http://localhost:6060/debug/vars | jq .\n</code></pre></p> </li> <li> <p>Identify large resource types:</p> </li> <li>Secrets with large data (certificates)</li> <li>ConfigMaps with large configuration</li> <li> <p>High resource counts (&gt;10,000)</p> </li> <li> <p>Check field filtering configuration: <pre><code># Are unnecessary fields being stored?\nwatched_resources_ignore_fields:\n  - metadata.managedFields\n  - metadata.annotations[\"kubectl.kubernetes.io/last-applied-configuration\"]\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ol> <li> <p>Switch large resources to cached store: <pre><code>secrets:\n  store: on-demand\n  cache_ttl: 2m\n</code></pre></p> </li> <li> <p>Add field filtering: <pre><code>watched_resources_ignore_fields:\n  - metadata.managedFields\n  - metadata.annotations[\"kubectl.kubernetes.io/last-applied-configuration\"]\n</code></pre>    See Configuration Reference for field filtering syntax.</p> </li> <li> <p>Limit watch scope: <pre><code>ingresses:\n  namespace: production  # Watch single namespace\n  label_selector:\n    app: myapp  # Filter by labels\n</code></pre></p> </li> <li> <p>Monitor memory with container limits: <pre><code>resources:\n  limits:\n    memory: 256Mi  # Adjust based on resource count\n</code></pre></p> </li> </ol>"},{"location":"watching-resources/#slow-template-rendering","title":"Slow Template Rendering","text":"<p>Symptoms: - Template rendering takes &gt;1 second - Deployment delays - High CPU usage during rendering</p> <p>Diagnosis:</p> <ol> <li> <p>Check if using cached store with <code>.List()</code>: <pre><code>{# This triggers fetch for ALL resources - slow with cached store #}\n{% for secret in resources.secrets.List() %}\n</code></pre></p> </li> <li> <p>Check for cache misses: <pre><code># Look for \"cache miss\" in logs with verbose logging\nkubectl logs deployment/haproxy-template-ic | grep \"cache miss\"\n</code></pre></p> </li> <li> <p>Profile template rendering:</p> </li> <li>Enable debug mode (verbose=2)</li> <li>Check rendering duration in logs</li> </ol> <p>Solutions:</p> <ol> <li> <p>Use cached store efficiently: <pre><code>{# Instead of List() #}\n{% for secret in resources.secrets.List() %}  {# SLOW #}\n\n{# Use Fetch() with specific keys #}\n{% set secret = resources.secrets.GetSingle(namespace, name) %}  {# FAST #}\n</code></pre></p> </li> <li> <p>Switch to memory store for frequently accessed resources: <pre><code>ingresses:\n  store: full  # Use memory store instead of cached\n</code></pre></p> </li> <li> <p>Increase cache TTL to reduce API calls: <pre><code>secrets:\n  cache_ttl: 5m  # Longer cache duration\n</code></pre></p> </li> <li> <p>Optimize template logic:</p> </li> <li>Avoid nested loops over large collections</li> <li>Cache computed values in variables</li> <li>Use <code>{% set %}</code> for repeated expressions</li> </ol>"},{"location":"watching-resources/#cache-miss-debugging","title":"Cache Miss Debugging","text":"<p>Symptoms: - Inconsistent rendering performance - Occasional slow template renders - High API call rate</p> <p>Diagnosis:</p> <ol> <li> <p>Enable debug logging: <pre><code>logging:\n  verbose: 2  # Debug level\n</code></pre></p> </li> <li> <p>Monitor cache hit/miss ratio in logs: <pre><code>kubectl logs deployment/haproxy-template-ic | grep -E \"cache (hit|miss)\"\n</code></pre></p> </li> <li> <p>Check cache TTL configuration: <pre><code>secrets:\n  cache_ttl: 2m10s  # Current TTL\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ol> <li> <p>Increase cache TTL if resources don't change frequently: <pre><code>secrets:\n  cache_ttl: 10m  # Longer TTL for stable resources\n</code></pre></p> </li> <li> <p>Pre-warm cache by accessing resources early: <pre><code>{# Access commonly used resources at template start #}\n{% set _ = resources.secrets.GetSingle(\"default\", \"common-secret\") %}\n</code></pre></p> </li> <li> <p>Switch to memory store if cache miss rate is high: <pre><code>secrets:\n  store: full  # Switch to memory store\n</code></pre></p> </li> </ol>"},{"location":"watching-resources/#common-misconfigurations","title":"Common Misconfigurations","text":"<p>Issue: index_by doesn't match template usage</p> <pre><code># Configuration\nindex_by: [\"metadata.namespace\", \"metadata.name\"]\n</code></pre> <pre><code>{# Template - WRONG: only provides one key #}\n{% for ing in resources.ingresses.Fetch(\"my-app\") %}\n</code></pre> <p>Solution: Match template parameters to index_by order: <pre><code>{# Correct: both namespace and name #}\n{% for ing in resources.ingresses.Fetch(\"default\", \"my-app\") %}\n</code></pre></p> <p>Issue: Using memory store for large resources</p> <pre><code># Bad: Large secrets in memory\nsecrets:\n  store: full  # Default, stores all in memory\n</code></pre> <p>Solution: Use cached store for large resources: <pre><code>secrets:\n  store: on-demand\n  cache_ttl: 2m\n</code></pre></p> <p>Issue: Using .List() with cached store</p> <pre><code>{# Slow: Fetches all resources from API #}\n{% for secret in resources.secrets.List() %}\n</code></pre> <p>Solution: Use <code>.Fetch()</code> for selective access: <pre><code>{# Fast: Fetches only required resources #}\n{% set secret = resources.secrets.GetSingle(namespace, name) %}\n</code></pre></p> <p>See Configuration Reference for additional configuration guidance.</p>"},{"location":"watching-resources/#see-also","title":"See Also","text":"<ul> <li>Configuration Reference - Complete configuration syntax and field descriptions</li> <li>Templating Guide - Template syntax and resource access patterns</li> <li>pkg/k8s README - Store implementation details and API documentation</li> <li>Architecture Design - System architecture and component interactions</li> </ul>"},{"location":"development/crd-validation-design/","title":"HAProxyTemplateConfig CRD Implementation Design","text":""},{"location":"development/crd-validation-design/#overview","title":"Overview","text":""},{"location":"development/crd-validation-design/#problem-statement","title":"Problem Statement","text":"<p>The HAProxy Template Ingress Controller uses a template-driven approach where users define HAProxy configurations using Gonja templates. While this provides flexibility, it introduces a confidence gap: unlike traditional ingress controllers where annotations are part of the code (and therefore tested), here the templates are part of the configuration and remain untested until deployment.</p> <p>This creates risk: - Invalid templates cause runtime failures - Configuration errors only discovered after applying changes - No validation feedback before deployment</p>"},{"location":"development/crd-validation-design/#solution","title":"Solution","text":"<p>Migrate from ConfigMap-based configuration to a Custom Resource Definition (CRD) that:</p> <ol> <li>Provides declarative configuration - Kubernetes-native resource with proper validation</li> <li>Embeds validation tests - Users define test fixtures and assertions inline</li> <li>Validates on admission - Webhook runs embedded tests before accepting changes</li> <li>Enables CLI validation - Pre-apply validation via <code>controller validate</code> subcommand</li> <li>Multi-layer defense - OpenAPI schema, webhook validation, runtime validation</li> </ol>"},{"location":"development/crd-validation-design/#design-principles","title":"Design Principles","text":"<ul> <li>Security first: Credentials stay in Secrets, never in CRDs</li> <li>Fail-open webhook: <code>failurePolicy: Ignore</code> prevents deadlock when controller is down</li> <li>Simple failure mode: Invalid config causes crash loop (visible in monitoring)</li> <li>No complex fallbacks: Keep it simple - let monitoring systems detect failures</li> <li>Namespace isolation: Webhook only validates resources with specific labels</li> </ul>"},{"location":"development/crd-validation-design/#phase-1-crd-api-design","title":"Phase 1: CRD API Design","text":""},{"location":"development/crd-validation-design/#api-group-and-version","title":"API Group and Version","text":"<ul> <li>Group: <code>haproxy-template-ic.github.io</code></li> <li>Version: <code>v1alpha1</code> (pre-release, API may change)</li> <li>Kind: <code>HAProxyTemplateConfig</code></li> <li>Plural: <code>haproxytemplateconfigs</code></li> <li>Short names: <code>htplcfg</code>, <code>haptpl</code></li> </ul>"},{"location":"development/crd-validation-design/#resource-structure","title":"Resource Structure","text":"<pre><code>apiVersion: haproxy-template-ic.github.io/v1alpha1\nkind: HAProxyTemplateConfig\nmetadata:\n  name: my-haproxy-config\n  namespace: default\n  labels:\n    app.kubernetes.io/name: haproxy-template-ic\n    app.kubernetes.io/instance: my-instance\nspec:\n  # Reference to Secret containing credentials\n  credentialsSecretRef:\n    name: haproxy-credentials\n    namespace: default  # optional, defaults to same namespace as config\n\n  # Pod selector for HAProxy instances\n  podSelector:\n    matchLabels:\n      app: haproxy\n      component: loadbalancer\n\n  # Controller settings\n  controller:\n    healthzPort: 8080\n    metricsPort: 9090\n    leaderElection:\n      enabled: true\n      leaseName: haproxy-template-ic-leader\n      leaseDuration: 60s\n      renewDeadline: 15s\n      retryPeriod: 5s\n\n  # Logging configuration\n  logging:\n    verbose: 1  # 0=WARNING, 1=INFO, 2=DEBUG\n\n  # Dataplane API configuration\n  dataplane:\n    port: 5555\n    minDeploymentInterval: 2s\n    driftPreventionInterval: 60s\n    mapsDir: /etc/haproxy/maps\n    sslCertsDir: /etc/haproxy/ssl\n    generalStorageDir: /etc/haproxy/general\n    configFile: /etc/haproxy/haproxy.cfg\n\n  # JSONPath fields to ignore across all resources\n  watchedResourcesIgnoreFields:\n    - metadata.managedFields\n    - metadata.resourceVersion\n\n  # Watched Kubernetes resources\n  watchedResources:\n    ingresses:\n      apiVersion: networking.k8s.io/v1\n      resources: ingresses\n      enableValidationWebhook: true\n      indexBy:\n        - metadata.namespace\n        - metadata.name\n      labelSelector: \"\"\n      fieldSelector: \"\"\n      namespaceSelector: \"\"\n\n    services:\n      apiVersion: v1\n      resources: services\n      indexBy:\n        - metadata.namespace\n        - metadata.name\n\n  # Template snippets\n  templateSnippets:\n    ssl_bind_options: |\n      ssl-min-ver TLSv1.2\n      ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256\n\n  # HAProxy map files\n  maps:\n    domain_to_backend:\n      template: |\n        {% for ingress in ingresses %}\n        {% for rule in ingress.spec.rules %}\n        {{ rule.host }} {{ rule.host }}_backend\n        {% endfor %}\n        {% endfor %}\n\n  # General files\n  files:\n    error_503:\n      template: |\n        HTTP/1.1 503 Service Unavailable\n        Content-Type: text/html\n\n        &lt;html&gt;&lt;body&gt;&lt;h1&gt;503 Service Unavailable&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\n      path: /etc/haproxy/errors/503.http\n\n  # SSL certificates\n  sslCertificates:\n    wildcard_example_com:\n      template: |\n        {{ secrets[\"tls-wildcard\"].data[\"tls.crt\"] | b64decode }}\n        {{ secrets[\"tls-wildcard\"].data[\"tls.key\"] | b64decode }}\n\n  # Main HAProxy configuration\n  haproxyConfig:\n    template: |\n      global\n          maxconn 4096\n          log stdout format raw local0 info\n\n      defaults\n          mode http\n          timeout connect 5s\n          timeout client 50s\n          timeout server 50s\n\n      frontend http-in\n          bind *:80\n          use_backend %[req.hdr(host),lower,map(/etc/haproxy/maps/domain_to_backend.map)]\n\n  # Embedded validation tests\n  validationTests:\n    - name: test_basic_ingress\n      description: Validate that a basic ingress generates valid HAProxy config\n      fixtures:\n        ingresses:\n          - apiVersion: networking.k8s.io/v1\n            kind: Ingress\n            metadata:\n              name: test-ingress\n              namespace: default\n            spec:\n              rules:\n                - host: example.com\n                  http:\n                    paths:\n                      - path: /\n                        pathType: Prefix\n                        backend:\n                          service:\n                            name: test-service\n                            port:\n                              number: 80\n        services:\n          - apiVersion: v1\n            kind: Service\n            metadata:\n              name: test-service\n              namespace: default\n            spec:\n              selector:\n                app: test\n              ports:\n                - port: 80\n                  targetPort: 8080\n      assertions:\n        - type: haproxy_valid\n          description: Generated config must pass HAProxy validation\n\n        - type: contains\n          description: Config must include frontend for example.com\n          target: haproxy_config\n          pattern: \"example.com\"\n\n        - type: contains\n          description: Map file must include domain mapping\n          target: maps.domain_to_backend\n          pattern: \"example.com example.com_backend\"\n\nstatus:\n  # Controller updates these fields\n  observedGeneration: 1\n  lastValidated: \"2025-01-27T10:00:00Z\"\n  validationStatus: Valid\n  validationMessage: \"All validation tests passed\"\n</code></pre>"},{"location":"development/crd-validation-design/#go-type-definitions","title":"Go Type Definitions","text":"<p>Location: <code>pkg/apis/haproxytemplate/v1alpha1/types.go</code></p> <pre><code>package v1alpha1\n\nimport (\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n    \"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\"\n)\n\n// +kubebuilder:object:root=true\n// +kubebuilder:subresource:status\n// +kubebuilder:resource:shortName=htplcfg;haptpl,scope=Namespaced\n// +kubebuilder:printcolumn:name=\"Status\",type=string,JSONPath=`.status.validationStatus`\n// +kubebuilder:printcolumn:name=\"Age\",type=date,JSONPath=`.metadata.creationTimestamp`\n\n// HAProxyTemplateConfig defines the configuration for the HAProxy Template Ingress Controller.\ntype HAProxyTemplateConfig struct {\n    metav1.TypeMeta   `json:\",inline\"`\n    metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n    Spec   HAProxyTemplateConfigSpec   `json:\"spec,omitempty\"`\n    Status HAProxyTemplateConfigStatus `json:\"status,omitempty\"`\n}\n\n// HAProxyTemplateConfigSpec defines the desired state of HAProxyTemplateConfig.\ntype HAProxyTemplateConfigSpec struct {\n    // CredentialsSecretRef references the Secret containing HAProxy Dataplane API credentials.\n    // The Secret must contain the following keys:\n    //   - dataplane_username: Username for production HAProxy Dataplane API\n    //   - dataplane_password: Password for production HAProxy Dataplane API\n    //   - validation_username: Username for validation HAProxy instance\n    //   - validation_password: Password for validation HAProxy instance\n    // +kubebuilder:validation:Required\n    CredentialsSecretRef SecretReference `json:\"credentialsSecretRef\"`\n\n    // PodSelector identifies which HAProxy pods to configure.\n    // +kubebuilder:validation:Required\n    PodSelector PodSelector `json:\"podSelector\"`\n\n    // Controller contains controller-level settings.\n    // +optional\n    Controller ControllerConfig `json:\"controller,omitempty\"`\n\n    // Logging configures logging behavior.\n    // +optional\n    Logging LoggingConfig `json:\"logging,omitempty\"`\n\n    // Dataplane configures the Dataplane API for production HAProxy instances.\n    // +optional\n    Dataplane DataplaneConfig `json:\"dataplane,omitempty\"`\n\n    // WatchedResourcesIgnoreFields specifies JSONPath expressions for fields\n    // to remove from all watched resources to reduce memory usage.\n    // +optional\n    WatchedResourcesIgnoreFields []string `json:\"watchedResourcesIgnoreFields,omitempty\"`\n\n    // WatchedResources maps resource type names to their watch configuration.\n    // +kubebuilder:validation:Required\n    // +kubebuilder:validation:MinProperties=1\n    WatchedResources map[string]WatchedResource `json:\"watchedResources\"`\n\n    // TemplateSnippets maps snippet names to reusable template fragments.\n    // +optional\n    TemplateSnippets map[string]TemplateSnippet `json:\"templateSnippets,omitempty\"`\n\n    // Maps maps map file names to their template definitions.\n    // +optional\n    Maps map[string]MapFile `json:\"maps,omitempty\"`\n\n    // Files maps file names to their template definitions.\n    // +optional\n    Files map[string]GeneralFile `json:\"files,omitempty\"`\n\n    // SSLCertificates maps certificate names to their template definitions.\n    // +optional\n    SSLCertificates map[string]SSLCertificate `json:\"sslCertificates,omitempty\"`\n\n    // HAProxyConfig contains the main HAProxy configuration template.\n    // +kubebuilder:validation:Required\n    HAProxyConfig HAProxyConfig `json:\"haproxyConfig\"`\n\n    // ValidationTests contains embedded validation test definitions.\n    // These tests are executed during admission webhook validation and\n    // via the \"controller validate\" CLI command.\n    // +optional\n    ValidationTests []ValidationTest `json:\"validationTests,omitempty\"`\n}\n\n// SecretReference references a Secret by name and optional namespace.\ntype SecretReference struct {\n    // Name is the name of the Secret.\n    // +kubebuilder:validation:Required\n    // +kubebuilder:validation:MinLength=1\n    Name string `json:\"name\"`\n\n    // Namespace is the namespace of the Secret.\n    // If empty, defaults to the same namespace as the HAProxyTemplateConfig.\n    // +optional\n    Namespace string `json:\"namespace,omitempty\"`\n}\n\n// ValidationTest defines a validation test with fixtures and assertions.\ntype ValidationTest struct {\n    // Name is a unique identifier for this test.\n    // +kubebuilder:validation:Required\n    // +kubebuilder:validation:MinLength=1\n    Name string `json:\"name\"`\n\n    // Description explains what this test validates.\n    // +optional\n    Description string `json:\"description,omitempty\"`\n\n    // Fixtures defines the Kubernetes resources to use for this test.\n    // Keys are resource type names (matching WatchedResources keys).\n    // Values are arrays of resources in unstructured format.\n    // +kubebuilder:validation:Required\n    Fixtures map[string][]unstructured.Unstructured `json:\"fixtures\"`\n\n    // Assertions defines the validation checks to perform.\n    // +kubebuilder:validation:Required\n    // +kubebuilder:validation:MinItems=1\n    Assertions []ValidationAssertion `json:\"assertions\"`\n}\n\n// ValidationAssertion defines a single validation check.\ntype ValidationAssertion struct {\n    // Type is the assertion type.\n    // Supported types:\n    //   - haproxy_valid: Validates that generated HAProxy config is syntactically valid\n    //   - contains: Checks if target contains pattern (regex)\n    //   - not_contains: Checks if target does not contain pattern (regex)\n    //   - equals: Checks if target equals expected value\n    //   - jsonpath: Evaluates JSONPath expression against target\n    // +kubebuilder:validation:Required\n    // +kubebuilder:validation:Enum=haproxy_valid;contains;not_contains;equals;jsonpath\n    Type string `json:\"type\"`\n\n    // Description explains what this assertion validates.\n    // +optional\n    Description string `json:\"description,omitempty\"`\n\n    // Target specifies what to validate.\n    // For haproxy_valid: not used\n    // For contains/not_contains/equals: \"haproxy_config\", \"maps.&lt;name&gt;\", \"files.&lt;name&gt;\", \"sslCertificates.&lt;name&gt;\"\n    // For jsonpath: the resource to query (e.g., \"haproxy_config\")\n    // +optional\n    Target string `json:\"target,omitempty\"`\n\n    // Pattern is the regex pattern for contains/not_contains assertions.\n    // +optional\n    Pattern string `json:\"pattern,omitempty\"`\n\n    // Expected is the expected value for equals assertions.\n    // +optional\n    Expected string `json:\"expected,omitempty\"`\n\n    // JSONPath is the JSONPath expression for jsonpath assertions.\n    // +optional\n    JSONPath string `json:\"jsonpath,omitempty\"`\n}\n\n// HAProxyTemplateConfigStatus defines the observed state of HAProxyTemplateConfig.\ntype HAProxyTemplateConfigStatus struct {\n    // ObservedGeneration reflects the generation most recently observed by the controller.\n    // +optional\n    ObservedGeneration int64 `json:\"observedGeneration,omitempty\"`\n\n    // LastValidated is the timestamp of the last successful validation.\n    // +optional\n    LastValidated *metav1.Time `json:\"lastValidated,omitempty\"`\n\n    // ValidationStatus indicates the overall validation status.\n    // +kubebuilder:validation:Enum=Valid;Invalid;Unknown\n    // +optional\n    ValidationStatus string `json:\"validationStatus,omitempty\"`\n\n    // ValidationMessage contains human-readable validation details.\n    // +optional\n    ValidationMessage string `json:\"validationMessage,omitempty\"`\n\n    // Conditions represent the latest available observations of the config's state.\n    // +optional\n    Conditions []metav1.Condition `json:\"conditions,omitempty\"`\n}\n\n// +kubebuilder:object:root=true\n\n// HAProxyTemplateConfigList contains a list of HAProxyTemplateConfig.\ntype HAProxyTemplateConfigList struct {\n    metav1.TypeMeta `json:\",inline\"`\n    metav1.ListMeta `json:\"metadata,omitempty\"`\n    Items           []HAProxyTemplateConfig `json:\"items\"`\n}\n\n// Remaining types (PodSelector, ControllerConfig, etc.) are copied from pkg/core/config/types.go\n// with appropriate kubebuilder validation markers added.\n</code></pre>"},{"location":"development/crd-validation-design/#crd-generation","title":"CRD Generation","text":"<p>Use <code>controller-gen</code> to generate CRD YAML:</p> <pre><code># Install controller-gen\ngo install sigs.k8s.io/controller-tools/cmd/controller-gen@latest\n\n# Generate CRD manifests\ncontroller-gen crd:crdVersions=v1 \\\n    paths=./pkg/apis/haproxytemplate/v1alpha1/... \\\n    output:crd:dir=./charts/haproxy-template-ic/crds/\n</code></pre>"},{"location":"development/crd-validation-design/#helm-chart-integration","title":"Helm Chart Integration","text":"<p>Place generated CRD in <code>charts/haproxy-template-ic/crds/</code>:</p> <pre><code>charts/haproxy-template-ic/\n\u251c\u2500\u2500 crds/\n\u2502   \u2514\u2500\u2500 haproxy-template-ic.github.io_haproxytemplateconfigs.yaml\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 serviceaccount.yaml\n\u2502   \u251c\u2500\u2500 rbac.yaml\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 values.yaml\n</code></pre> <p>Helm CRD Handling: - CRDs in <code>crds/</code> directory are installed before other chart resources - CRDs are not templated (no Helm variable substitution) - CRDs cannot be upgraded via <code>helm upgrade</code> (Helm limitation) - CRDs cannot be deleted via <code>helm uninstall</code> (prevents data loss) - To update CRD schema: <code>kubectl apply -f charts/haproxy-template-ic/crds/</code></p>"},{"location":"development/crd-validation-design/#rbac-requirements","title":"RBAC Requirements","text":"<p>The controller needs additional permissions to watch CRDs:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: haproxy-template-ic\nrules:\n  # CRD permissions\n  - apiGroups: [\"haproxy-template-ic.github.io\"]\n    resources: [\"haproxytemplateconfigs\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n\n  # CRD status updates\n  - apiGroups: [\"haproxy-template-ic.github.io\"]\n    resources: [\"haproxytemplateconfigs/status\"]\n    verbs: [\"update\", \"patch\"]\n\n  # Secret permissions (for credentials)\n  - apiGroups: [\"\"]\n    resources: [\"secrets\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n\n  # Existing watched resource permissions...\n  - apiGroups: [\"networking.k8s.io\"]\n    resources: [\"ingresses\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  # ...\n</code></pre>"},{"location":"development/crd-validation-design/#phase-2-credentials-management","title":"Phase 2: Credentials Management","text":""},{"location":"development/crd-validation-design/#design-decision","title":"Design Decision","text":"<p>Keep credentials in Kubernetes Secret (not in CRD) for security: - Secrets are encrypted at rest (in clusters with encryption enabled) - CRDs are typically not encrypted - Credentials should never be in version control - Follows Kubernetes security best practices</p>"},{"location":"development/crd-validation-design/#secret-structure","title":"Secret Structure","text":"<p>The Secret referenced by <code>credentialsSecretRef</code> must contain:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: haproxy-credentials\n  namespace: default\ntype: Opaque\ndata:\n  # Base64-encoded credentials\n  dataplane_username: YWRtaW4=           # admin\n  dataplane_password: cGFzc3dvcmQ=       # password\n  validation_username: dmFsaWRhdG9y=     # validator\n  validation_password: dmFscGFzcw==      # valpass\n</code></pre>"},{"location":"development/crd-validation-design/#credentials-watcher","title":"Credentials Watcher","text":"<p>The controller must watch the referenced Secret for changes:</p> <ol> <li>On startup: Load credentials from Secret referenced in config</li> <li>On config change: If <code>credentialsSecretRef</code> changes, reload credentials</li> <li>On Secret change: If Secret content changes, reload credentials</li> </ol> <p>Implementation in <code>pkg/controller/credentialsloader/</code>:</p> <pre><code>package credentialsloader\n\ntype Component struct {\n    eventBus   *events.EventBus\n    client     kubernetes.Interface\n    secretRef  *v1alpha1.SecretReference\n    namespace  string\n    logger     *slog.Logger\n}\n\nfunc (c *Component) Start(ctx context.Context) error {\n    // Subscribe to config validated events\n    eventChan := c.eventBus.Subscribe(50)\n\n    // Create Secret informer\n    informer := c.createSecretInformer()\n    go informer.Run(ctx.Done())\n\n    for {\n        select {\n        case event := &lt;-eventChan:\n            switch e := event.(type) {\n            case *events.ConfigValidatedEvent:\n                // Config changed - check if credentials ref changed\n                if c.secretRefChanged(e.Config.CredentialsSecretRef) {\n                    c.secretRef = &amp;e.Config.CredentialsSecretRef\n                    c.reloadCredentials()\n                }\n            }\n        case &lt;-ctx.Done():\n            return ctx.Err()\n        }\n    }\n}\n\nfunc (c *Component) reloadCredentials() {\n    // Load new credentials from Secret\n    secret, err := c.client.CoreV1().Secrets(c.getSecretNamespace()).\n        Get(context.TODO(), c.secretRef.Name, metav1.GetOptions{})\n    if err != nil {\n        c.eventBus.Publish(&amp;events.CredentialsLoadFailedEvent{Error: err})\n        return\n    }\n\n    // Parse credentials\n    creds, err := parseCredentials(secret.Data)\n    if err != nil {\n        c.eventBus.Publish(&amp;events.CredentialsLoadFailedEvent{Error: err})\n        return\n    }\n\n    // Publish credentials loaded event\n    c.eventBus.Publish(&amp;events.CredentialsLoadedEvent{\n        Credentials: creds,\n    })\n}\n</code></pre>"},{"location":"development/crd-validation-design/#credentials-reload-flow","title":"Credentials Reload Flow","text":"<pre><code>Secret Updated\n    \u2193\nSecret Informer Event\n    \u2193\nCredentials Reloaded\n    \u2193\nCredentialsLoadedEvent\n    \u2193\nDataplane Clients Reconnect with New Credentials\n</code></pre>"},{"location":"development/crd-validation-design/#phase-3-config-watcher-updates","title":"Phase 3: Config Watcher Updates","text":""},{"location":"development/crd-validation-design/#current-implementation","title":"Current Implementation","text":"<p>Location: <code>pkg/controller/configloader/</code></p> <p>Currently watches a ConfigMap and publishes <code>ConfigParsedEvent</code>.</p>"},{"location":"development/crd-validation-design/#required-changes","title":"Required Changes","text":"<ol> <li>Watch HAProxyTemplateConfig CRD instead of ConfigMap</li> <li>Parse CRD spec into internal config types</li> <li>Maintain same event flow (<code>ConfigParsedEvent</code> \u2192 validators \u2192 <code>ConfigValidatedEvent</code>)</li> </ol>"},{"location":"development/crd-validation-design/#implementation","title":"Implementation","text":"<pre><code>package configloader\n\nimport (\n    \"haproxy-template-ic/pkg/apis/haproxytemplate/v1alpha1\"\n    \"haproxy-template-ic/pkg/controller/events\"\n    haproxyversioned \"haproxy-template-ic/pkg/generated/clientset/versioned\"\n)\n\ntype Component struct {\n    eventBus      *events.EventBus\n    haproxyClient haproxyversioned.Interface\n    namespace     string\n    configName    string\n    logger        *slog.Logger\n}\n\nfunc (c *Component) Start(ctx context.Context) error {\n    // Create informer for HAProxyTemplateConfig\n    informerFactory := haproxyinformers.NewSharedInformerFactoryWithOptions(\n        c.haproxyClient,\n        0,\n        haproxyinformers.WithNamespace(c.namespace),\n    )\n\n    informer := informerFactory.Haproxytemplate().V1alpha1().\n        HAProxyTemplateConfigs().Informer()\n\n    informer.AddEventHandler(cache.ResourceEventHandlerFuncs{\n        AddFunc: func(obj interface{}) {\n            c.handleConfigChange(obj)\n        },\n        UpdateFunc: func(oldObj, newObj interface{}) {\n            c.handleConfigChange(newObj)\n        },\n        DeleteFunc: func(obj interface{}) {\n            c.logger.Error(\"HAProxyTemplateConfig deleted - controller will crash\",\n                \"name\", c.configName)\n            // Let controller crash - invalid state\n        },\n    })\n\n    informerFactory.Start(ctx.Done())\n    informerFactory.WaitForCacheSync(ctx.Done())\n\n    &lt;-ctx.Done()\n    return ctx.Err()\n}\n\nfunc (c *Component) handleConfigChange(obj interface{}) {\n    config, ok := obj.(*v1alpha1.HAProxyTemplateConfig)\n    if !ok {\n        c.logger.Error(\"unexpected object type\")\n        return\n    }\n\n    // Only handle our specific config\n    if config.Name != c.configName {\n        return\n    }\n\n    c.logger.Info(\"HAProxyTemplateConfig changed\",\n        \"name\", config.Name,\n        \"generation\", config.Generation)\n\n    // Convert CRD spec to internal config types\n    internalConfig := c.convertToInternalConfig(config.Spec)\n\n    // Publish config parsed event (same as before)\n    c.eventBus.Publish(&amp;events.ConfigParsedEvent{\n        Config:  internalConfig,\n        Version: fmt.Sprintf(\"gen-%d\", config.Generation),\n    })\n}\n</code></pre>"},{"location":"development/crd-validation-design/#config-resolution","title":"Config Resolution","text":"<p>The controller needs to know: 1. Which namespace to watch for HAProxyTemplateConfig 2. Which config resource to watch (by name)</p> <p>Options:</p> <p>Option A: Environment variables (current pattern) <pre><code>CONTROLLER_NAMESPACE=default\nCONFIG_NAME=haproxy-config\n</code></pre></p> <p>Option B: CLI flags <pre><code>controller --namespace=default --config-name=haproxy-config\n</code></pre></p> <p>Recommendation: Keep environment variables for consistency with existing pattern.</p>"},{"location":"development/crd-validation-design/#staged-startup-integration","title":"Staged Startup Integration","text":"<p>The existing staged startup already handles config loading:</p> <pre><code>Stage 1: Config Management\n  - ConfigWatcher (now watches CRD)\n  - ConfigValidator\n  - EventBus.Start()\n\nStage 2: Wait for Valid Config\n  - Block until ConfigValidatedEvent\n  - Publish ControllerStartedEvent\n\nStage 3-5: Rest of startup...\n</code></pre> <p>No changes needed to staged startup flow - just swap ConfigMap watcher for CRD watcher.</p>"},{"location":"development/crd-validation-design/#phase-4-validation-subcommand","title":"Phase 4: Validation Subcommand","text":""},{"location":"development/crd-validation-design/#cli-interface","title":"CLI Interface","text":"<p>Add <code>validate</code> subcommand to existing controller binary:</p> <pre><code># Validate config file before applying\ncontroller validate --config haproxytemplate-config.yaml\n\n# Validate config already in cluster\ncontroller validate --name haproxy-config --namespace default\n\n# Run specific test\ncontroller validate --config config.yaml --test test_basic_ingress\n\n# Output formats\ncontroller validate --config config.yaml --output json\ncontroller validate --config config.yaml --output yaml\ncontroller validate --config config.yaml --output summary  # default\n</code></pre>"},{"location":"development/crd-validation-design/#implementation_1","title":"Implementation","text":"<p>Location: <code>cmd/controller/validate.go</code></p> <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"os\"\n\n    \"github.com/spf13/cobra\"\n    \"haproxy-template-ic/pkg/controller/testrunner\"\n)\n\nfunc newValidateCommand() *cobra.Command {\n    var (\n        configFile string\n        configName string\n        namespace  string\n        testName   string\n        outputFormat string\n    )\n\n    cmd := &amp;cobra.Command{\n        Use:   \"validate\",\n        Short: \"Validate HAProxyTemplateConfig and run embedded tests\",\n        RunE: func(cmd *cobra.Command, args []string) error {\n            ctx := context.Background()\n\n            var config *v1alpha1.HAProxyTemplateConfig\n            var err error\n\n            if configFile != \"\" {\n                // Load from file\n                config, err = loadConfigFromFile(configFile)\n            } else if configName != \"\" {\n                // Load from cluster\n                config, err = loadConfigFromCluster(configName, namespace)\n            } else {\n                return fmt.Errorf(\"either --config or --name must be specified\")\n            }\n\n            if err != nil {\n                return fmt.Errorf(\"failed to load config: %w\", err)\n            }\n\n            // Run validation tests\n            runner := testrunner.New(config, testrunner.Options{\n                TestName: testName,\n                Logger:   logger,\n            })\n\n            results, err := runner.RunTests(ctx)\n            if err != nil {\n                return fmt.Errorf(\"failed to run tests: %w\", err)\n            }\n\n            // Output results\n            if err := outputResults(results, outputFormat); err != nil {\n                return err\n            }\n\n            // Exit code: 0 if all tests passed, 1 otherwise\n            if !results.AllPassed() {\n                os.Exit(1)\n            }\n\n            return nil\n        },\n    }\n\n    cmd.Flags().StringVar(&amp;configFile, \"config\", \"\", \"Path to HAProxyTemplateConfig YAML file\")\n    cmd.Flags().StringVar(&amp;configName, \"name\", \"\", \"Name of HAProxyTemplateConfig in cluster\")\n    cmd.Flags().StringVar(&amp;namespace, \"namespace\", \"default\", \"Namespace of HAProxyTemplateConfig\")\n    cmd.Flags().StringVar(&amp;testName, \"test\", \"\", \"Run specific test (default: all)\")\n    cmd.Flags().StringVar(&amp;outputFormat, \"output\", \"summary\", \"Output format: summary, json, yaml\")\n\n    return cmd\n}\n</code></pre>"},{"location":"development/crd-validation-design/#test-runner-implementation","title":"Test Runner Implementation","text":"<p>Location: <code>pkg/controller/testrunner/runner.go</code></p> <p>Reuses DryRunValidator pattern:</p> <pre><code>package testrunner\n\nimport (\n    \"context\"\n    \"fmt\"\n\n    \"haproxy-template-ic/pkg/apis/haproxytemplate/v1alpha1\"\n    \"haproxy-template-ic/pkg/controller/resourcestore\"\n    \"haproxy-template-ic/pkg/dataplane\"\n    \"haproxy-template-ic/pkg/templating\"\n)\n\ntype Runner struct {\n    config  *v1alpha1.HAProxyTemplateConfig\n    engine  *templating.TemplateEngine\n    validator *dataplane.Validator\n    options Options\n}\n\ntype Options struct {\n    TestName string\n    Logger   *slog.Logger\n}\n\ntype TestResults struct {\n    TotalTests   int\n    PassedTests  int\n    FailedTests  int\n    TestResults  []TestResult\n}\n\ntype TestResult struct {\n    TestName    string\n    Description string\n    Passed      bool\n    Duration    time.Duration\n    Assertions  []AssertionResult\n}\n\ntype AssertionResult struct {\n    Type        string\n    Description string\n    Passed      bool\n    Error       string\n}\n\nfunc (r *Runner) RunTests(ctx context.Context) (*TestResults, error) {\n    results := &amp;TestResults{\n        TotalTests: len(r.config.Spec.ValidationTests),\n    }\n\n    for _, test := range r.config.Spec.ValidationTests {\n        // Skip if specific test requested and this isn't it\n        if r.options.TestName != \"\" &amp;&amp; test.Name != r.options.TestName {\n            continue\n        }\n\n        result := r.runSingleTest(ctx, test)\n        results.TestResults = append(results.TestResults, result)\n\n        if result.Passed {\n            results.PassedTests++\n        } else {\n            results.FailedTests++\n        }\n    }\n\n    return results, nil\n}\n\nfunc (r *Runner) runSingleTest(ctx context.Context, test v1alpha1.ValidationTest) TestResult {\n    startTime := time.Now()\n\n    result := TestResult{\n        TestName:    test.Name,\n        Description: test.Description,\n        Passed:      true,\n    }\n\n    // 1. Create stores from fixtures\n    stores := r.createStoresFromFixtures(test.Fixtures)\n\n    // 2. Build template context\n    templateContext := r.buildTemplateContext(stores)\n\n    // 3. Render templates\n    haproxyConfig, err := r.engine.Render(\"haproxy.cfg\", templateContext)\n    if err != nil {\n        result.Passed = false\n        result.Assertions = append(result.Assertions, AssertionResult{\n            Type:        \"rendering\",\n            Description: \"Template rendering failed\",\n            Passed:      false,\n            Error:       dataplane.SimplifyRenderingError(err),\n        })\n        result.Duration = time.Since(startTime)\n        return result\n    }\n\n    // 4. Run assertions\n    for _, assertion := range test.Assertions {\n        assertionResult := r.runAssertion(ctx, assertion, haproxyConfig, templateContext)\n        result.Assertions = append(result.Assertions, assertionResult)\n\n        if !assertionResult.Passed {\n            result.Passed = false\n        }\n    }\n\n    result.Duration = time.Since(startTime)\n    return result\n}\n\nfunc (r *Runner) runAssertion(\n    ctx context.Context,\n    assertion v1alpha1.ValidationAssertion,\n    haproxyConfig string,\n    templateContext map[string]interface{},\n) AssertionResult {\n    result := AssertionResult{\n        Type:        assertion.Type,\n        Description: assertion.Description,\n        Passed:      true,\n    }\n\n    switch assertion.Type {\n    case \"haproxy_valid\":\n        // Validate HAProxy config syntax\n        _, err := r.validator.ValidateConfig(ctx, haproxyConfig, nil, 0)\n        if err != nil {\n            result.Passed = false\n            result.Error = dataplane.SimplifyValidationError(err)\n        }\n\n    case \"contains\":\n        // Check if target contains pattern\n        target := r.resolveTarget(assertion.Target, haproxyConfig, templateContext)\n        matched, err := regexp.MatchString(assertion.Pattern, target)\n        if err != nil || !matched {\n            result.Passed = false\n            result.Error = fmt.Sprintf(\"pattern '%s' not found in %s\", assertion.Pattern, assertion.Target)\n        }\n\n    case \"not_contains\":\n        // Check if target does NOT contain pattern\n        target := r.resolveTarget(assertion.Target, haproxyConfig, templateContext)\n        matched, err := regexp.MatchString(assertion.Pattern, target)\n        if err != nil {\n            result.Passed = false\n            result.Error = fmt.Sprintf(\"regex error: %v\", err)\n        } else if matched {\n            result.Passed = false\n            result.Error = fmt.Sprintf(\"pattern '%s' unexpectedly found in %s\", assertion.Pattern, assertion.Target)\n        }\n\n    case \"equals\":\n        // Check if target equals expected\n        target := r.resolveTarget(assertion.Target, haproxyConfig, templateContext)\n        if target != assertion.Expected {\n            result.Passed = false\n            result.Error = fmt.Sprintf(\"expected '%s', got '%s'\", assertion.Expected, target)\n        }\n\n    case \"jsonpath\":\n        // Evaluate JSONPath expression\n        value, err := evaluateJSONPath(assertion.JSONPath, templateContext)\n        if err != nil {\n            result.Passed = false\n            result.Error = fmt.Sprintf(\"jsonpath error: %v\", err)\n        }\n        // Additional validation based on expected value...\n    }\n\n    return result\n}\n</code></pre>"},{"location":"development/crd-validation-design/#output-formats","title":"Output Formats","text":"<p>Summary (default): <pre><code>Validating HAProxyTemplateConfig: haproxy-config\n\n\u2713 test_basic_ingress (1.2s)\n  \u2713 Generated config must pass HAProxy validation\n  \u2713 Config must include frontend for example.com\n  \u2713 Map file must include domain mapping\n\n\u2717 test_ssl_config (0.8s)\n  \u2713 Generated config must pass HAProxy validation\n  \u2717 Config must include SSL bind options\n    Error: pattern 'ssl-min-ver TLSv1.2' not found in haproxy_config\n\nTests: 1 passed, 1 failed, 2 total\nTime: 2.0s\n</code></pre></p> <p>JSON: <pre><code>{\n  \"totalTests\": 2,\n  \"passedTests\": 1,\n  \"failedTests\": 1,\n  \"testResults\": [\n    {\n      \"testName\": \"test_basic_ingress\",\n      \"description\": \"Validate that a basic ingress generates valid HAProxy config\",\n      \"passed\": true,\n      \"duration\": \"1.2s\",\n      \"assertions\": [...]\n    },\n    ...\n  ]\n}\n</code></pre></p>"},{"location":"development/crd-validation-design/#phase-5-validating-webhook","title":"Phase 5: Validating Webhook","text":""},{"location":"development/crd-validation-design/#webhook-configuration","title":"Webhook Configuration","text":"<p>Location: <code>charts/haproxy-template-ic/templates/validatingwebhook.yaml</code></p> <pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: haproxy-template-ic-webhook\nwebhooks:\n  - name: validate.haproxytemplateconfig.haproxy-template-ic.github.io\n    clientConfig:\n      service:\n        name: haproxy-template-ic-webhook\n        namespace: {{ .Release.Namespace }}\n        path: /validate-haproxytemplateconfig\n      caBundle: {{ .Values.webhook.caBundle }}\n\n    rules:\n      - apiGroups: [\"haproxy-template-ic.github.io\"]\n        apiVersions: [\"v1alpha1\"]\n        operations: [\"CREATE\", \"UPDATE\"]\n        resources: [\"haproxytemplateconfigs\"]\n        scope: \"Namespaced\"\n\n    # Fail-open: prevent deadlock when controller is down\n    failurePolicy: Ignore\n\n    # Only validate resources with specific labels (namespace isolation)\n    objectSelector:\n      matchExpressions:\n        - key: app.kubernetes.io/name\n          operator: In\n          values:\n            - haproxy-template-ic\n\n    sideEffects: None\n    admissionReviewVersions: [\"v1\"]\n    timeoutSeconds: 10\n</code></pre>"},{"location":"development/crd-validation-design/#webhook-server-implementation","title":"Webhook Server Implementation","text":"<p>Location: <code>pkg/controller/webhook/server.go</code></p> <pre><code>package webhook\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n\n    admissionv1 \"k8s.io/api/admission/v1\"\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\n    \"haproxy-template-ic/pkg/apis/haproxytemplate/v1alpha1\"\n    \"haproxy-template-ic/pkg/controller/testrunner\"\n)\n\ntype Server struct {\n    testRunner *testrunner.Runner\n    logger     *slog.Logger\n}\n\nfunc (s *Server) Start(ctx context.Context, addr string) error {\n    mux := http.NewServeMux()\n    mux.HandleFunc(\"/validate-haproxytemplateconfig\", s.handleValidation)\n    mux.HandleFunc(\"/healthz\", s.handleHealthz)\n\n    server := &amp;http.Server{\n        Addr:    addr,\n        Handler: mux,\n    }\n\n    go func() {\n        &lt;-ctx.Done()\n        server.Shutdown(context.Background())\n    }()\n\n    s.logger.Info(\"webhook server starting\", \"addr\", addr)\n    return server.ListenAndServeTLS(\"/certs/tls.crt\", \"/certs/tls.key\")\n}\n\nfunc (s *Server) handleValidation(w http.ResponseWriter, r *http.Request) {\n    var admissionReview admissionv1.AdmissionReview\n    if err := json.NewDecoder(r.Body).Decode(&amp;admissionReview); err != nil {\n        s.logger.Error(\"failed to decode admission review\", \"error\", err)\n        http.Error(w, \"invalid admission review\", http.StatusBadRequest)\n        return\n    }\n\n    response := s.validateConfig(admissionReview.Request)\n\n    admissionReview.Response = response\n    admissionReview.Response.UID = admissionReview.Request.UID\n\n    w.Header().Set(\"Content-Type\", \"application/json\")\n    json.NewEncoder(w).Encode(admissionReview)\n}\n\nfunc (s *Server) validateConfig(req *admissionv1.AdmissionRequest) *admissionv1.AdmissionResponse {\n    // Parse config from request\n    var config v1alpha1.HAProxyTemplateConfig\n    if err := json.Unmarshal(req.Object.Raw, &amp;config); err != nil {\n        return &amp;admissionv1.AdmissionResponse{\n            Allowed: false,\n            Result: &amp;metav1.Status{\n                Message: fmt.Sprintf(\"failed to parse config: %v\", err),\n            },\n        }\n    }\n\n    // Run embedded validation tests\n    ctx, cancel := context.WithTimeout(context.Background(), 8*time.Second)\n    defer cancel()\n\n    runner := testrunner.New(&amp;config, testrunner.Options{\n        Logger: s.logger,\n    })\n\n    results, err := runner.RunTests(ctx)\n    if err != nil {\n        // Fail-open: allow config if validation errors out\n        s.logger.Error(\"validation test execution failed\", \"error\", err)\n        return &amp;admissionv1.AdmissionResponse{\n            Allowed: true,\n            Warnings: []string{\n                fmt.Sprintf(\"validation tests failed to execute: %v\", err),\n            },\n        }\n    }\n\n    // Reject if any tests failed\n    if !results.AllPassed() {\n        return &amp;admissionv1.AdmissionResponse{\n            Allowed: false,\n            Result: &amp;metav1.Status{\n                Message: s.formatTestFailures(results),\n            },\n        }\n    }\n\n    // All tests passed - allow\n    return &amp;admissionv1.AdmissionResponse{\n        Allowed: true,\n    }\n}\n\nfunc (s *Server) formatTestFailures(results *testrunner.TestResults) string {\n    var msg strings.Builder\n    msg.WriteString(fmt.Sprintf(\"Validation failed: %d/%d tests passed\\n\\n\",\n        results.PassedTests, results.TotalTests))\n\n    for _, test := range results.TestResults {\n        if !test.Passed {\n            msg.WriteString(fmt.Sprintf(\"\u2717 %s\\n\", test.TestName))\n            for _, assertion := range test.Assertions {\n                if !assertion.Passed {\n                    msg.WriteString(fmt.Sprintf(\"  \u2717 %s\\n\", assertion.Description))\n                    if assertion.Error != \"\" {\n                        msg.WriteString(fmt.Sprintf(\"    Error: %s\\n\", assertion.Error))\n                    }\n                }\n            }\n            msg.WriteString(\"\\n\")\n        }\n    }\n\n    return msg.String()\n}\n</code></pre>"},{"location":"development/crd-validation-design/#certificate-management","title":"Certificate Management","text":"<p>The webhook requires TLS certificates. Options:</p> <ol> <li>cert-manager (recommended for production)</li> <li>Manual certificate generation (development)</li> <li>Self-signed with CA injection (simple deployments)</li> </ol> <p>Example with cert-manager:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: haproxy-template-ic-webhook-cert\n  namespace: default\nspec:\n  secretName: haproxy-webhook-certs\n  dnsNames:\n    - haproxy-template-ic-webhook.default.svc\n    - haproxy-template-ic-webhook.default.svc.cluster.local\n  issuerRef:\n    name: selfsigned-issuer\n    kind: ClusterIssuer\n</code></pre>"},{"location":"development/crd-validation-design/#webhook-deployment","title":"Webhook Deployment","text":"<p>The webhook runs as part of the controller (not separate pod):</p> <pre><code>// cmd/controller/main.go\nfunc main() {\n    // ... existing startup ...\n\n    // Stage 6: Webhook Server (after controller operational)\n    if webhookEnabled {\n        webhookServer := webhook.NewServer(testRunner, logger)\n        g.Go(func() error {\n            return webhookServer.Start(gCtx, \":9443\")\n        })\n    }\n\n    // ... rest of startup ...\n}\n</code></pre>"},{"location":"development/crd-validation-design/#objectselector-for-multi-controller-support","title":"ObjectSelector for Multi-Controller Support","text":"<p>The webhook only validates configs with matching labels:</p> <pre><code># Controller A's config\napiVersion: haproxy-template-ic.github.io/v1alpha1\nkind: HAProxyTemplateConfig\nmetadata:\n  name: controller-a-config\n  namespace: shared-namespace\n  labels:\n    app.kubernetes.io/name: haproxy-template-ic\n    app.kubernetes.io/instance: controller-a  # Matches controller A's webhook\n\n# Controller B's config\napiVersion: haproxy-template-ic.github.io/v1alpha1\nkind: HAProxyTemplateConfig\nmetadata:\n  name: controller-b-config\n  namespace: shared-namespace\n  labels:\n    app.kubernetes.io/name: haproxy-template-ic\n    app.kubernetes.io/instance: controller-b  # Matches controller B's webhook\n</code></pre> <p>Each controller's webhook filters by instance label to prevent cross-validation.</p>"},{"location":"development/crd-validation-design/#phase-6-documentation-updates","title":"Phase 6: Documentation Updates","text":""},{"location":"development/crd-validation-design/#files-to-update","title":"Files to Update","text":"<ol> <li>Remove ConfigMap references:</li> <li><code>docs/supported-configuration.md</code> \u2192 Update to CRD syntax</li> <li><code>README.md</code> \u2192 Update quick start examples</li> <li><code>charts/haproxy-template-ic/README.md</code> \u2192 Update installation docs</li> <li> <p>All example YAML files in <code>examples/</code></p> </li> <li> <p>Update CLAUDE.md files:</p> </li> <li><code>pkg/core/CLAUDE.md</code> \u2192 Document CRD types instead of ConfigMap parsing</li> <li><code>cmd/controller/CLAUDE.md</code> \u2192 Update startup to reference CRD watching</li> <li> <p><code>pkg/controller/configloader/CLAUDE.md</code> \u2192 Document CRD watching pattern</p> </li> <li> <p>New documentation:</p> </li> <li><code>docs/validation-tests.md</code> \u2192 Guide for writing validation tests</li> <li><code>docs/cli-reference.md</code> \u2192 Document <code>controller validate</code> subcommand</li> </ol>"},{"location":"development/crd-validation-design/#example-documentation-structure","title":"Example Documentation Structure","text":"<p>docs/validation-tests.md:</p> <p><pre><code># Writing Validation Tests\n\nValidation tests are embedded in HAProxyTemplateConfig and provide\nconfidence that template changes work correctly before deployment.\n\n## Test Structure\n\nEach test consists of:\n1. **Fixtures**: Kubernetes resources to use for rendering\n2. **Assertions**: Checks to validate the rendered output\n\n## Assertion Types\n\n### haproxy_valid\n\nValidates that the generated HAProxy configuration is syntactically valid.\n\n### contains\n\nChecks if the target contains a pattern (regex).\n\n### not_contains\n\nChecks if the target does NOT contain a pattern (regex).\n\n### equals\n\nChecks if the target exactly equals an expected value.\n\n### jsonpath\n\nEvaluates a JSONPath expression against the target.\n\n## Examples\n\n[Comprehensive examples of each assertion type...]\n\n## Best Practices\n\n1. Test critical paths (SSL, routing, backend selection)\n2. Use realistic fixtures (not toy examples)\n3. Add descriptive test names and descriptions\n4. Start simple, add complexity gradually\n5. Run `controller validate` before applying changes\n\n## CLI Usage\n\n```bash\n# Validate before applying\ncontroller validate --config myconfig.yaml\n\n# Run specific test\ncontroller validate --config myconfig.yaml --test test_ssl_config\n\n# Validate deployed config\ncontroller validate --name haproxy-config --namespace default\n</code></pre> ```</p>"},{"location":"development/crd-validation-design/#implementation-roadmap","title":"Implementation Roadmap","text":""},{"location":"development/crd-validation-design/#milestone-1-crd-infrastructure-week-1","title":"Milestone 1: CRD Infrastructure (Week 1)","text":"<ul> <li>[ ] Create <code>pkg/apis/haproxytemplate/v1alpha1/</code> package</li> <li>[ ] Define Go types with kubebuilder markers</li> <li>[ ] Generate CRD YAML with controller-gen</li> <li>[ ] Place CRD in <code>charts/haproxy-template-ic/crds/</code></li> <li>[ ] Generate clientset, informers, listers</li> <li>[ ] Update RBAC for CRD permissions</li> </ul>"},{"location":"development/crd-validation-design/#milestone-2-config-watcher-week-1-2","title":"Milestone 2: Config Watcher (Week 1-2)","text":"<ul> <li>[ ] Update <code>pkg/controller/configloader/</code> to watch CRD</li> <li>[ ] Implement CRD \u2192 internal config conversion</li> <li>[ ] Add credentials watcher for Secret</li> <li>[ ] Test config reloading with CRD changes</li> <li>[ ] Integration tests for CRD watching</li> </ul>"},{"location":"development/crd-validation-design/#milestone-3-validation-cli-week-2","title":"Milestone 3: Validation CLI (Week 2)","text":"<ul> <li>[ ] Create <code>pkg/controller/testrunner/</code> package</li> <li>[ ] Implement test fixture \u2192 store conversion</li> <li>[ ] Implement assertion evaluation</li> <li>[ ] Add <code>controller validate</code> subcommand</li> <li>[ ] Add output formatters (summary, JSON, YAML)</li> <li>[ ] Unit tests for test runner</li> </ul>"},{"location":"development/crd-validation-design/#milestone-4-webhook-week-3","title":"Milestone 4: Webhook (Week 3)","text":"<ul> <li>[ ] Create <code>pkg/controller/webhook/</code> package</li> <li>[ ] Implement webhook server</li> <li>[ ] Integrate test runner</li> <li>[ ] Add certificate management</li> <li>[ ] Deploy webhook as part of controller</li> <li>[ ] Test fail-open behavior</li> <li>[ ] Test objectSelector filtering</li> </ul>"},{"location":"development/crd-validation-design/#milestone-5-documentation-week-3-4","title":"Milestone 5: Documentation (Week 3-4)","text":"<ul> <li>[ ] Update all ConfigMap \u2192 CRD references</li> <li>[ ] Write validation test guide</li> <li>[ ] Update CLI reference</li> <li>[ ] Update Helm chart README</li> <li>[ ] Create migration examples</li> <li>[ ] Update architecture diagrams</li> </ul>"},{"location":"development/crd-validation-design/#milestone-6-testing-polish-week-4","title":"Milestone 6: Testing &amp; Polish (Week 4)","text":"<ul> <li>[ ] End-to-end tests with CRD</li> <li>[ ] Acceptance tests for validation</li> <li>[ ] Performance testing (webhook timeout)</li> <li>[ ] Error message polish</li> <li>[ ] Final documentation review</li> </ul>"},{"location":"development/crd-validation-design/#testing-strategy","title":"Testing Strategy","text":""},{"location":"development/crd-validation-design/#unit-tests","title":"Unit Tests","text":"<ul> <li>CRD conversion logic</li> <li>Assertion evaluation</li> <li>Error simplification</li> <li>Fixture store creation</li> </ul>"},{"location":"development/crd-validation-design/#integration-tests","title":"Integration Tests","text":"<ul> <li>CRD watching and config reload</li> <li>Credentials watcher</li> <li>Test runner execution</li> <li>Webhook admission</li> </ul>"},{"location":"development/crd-validation-design/#acceptance-tests","title":"Acceptance Tests","text":"<ul> <li>Full validation flow (CLI + webhook)</li> <li>Multi-controller isolation</li> <li>Fail-open webhook behavior</li> <li>Certificate rotation</li> </ul>"},{"location":"development/crd-validation-design/#rollout-plan","title":"Rollout Plan","text":"<p>Since there are no releases yet, this is a breaking change that replaces ConfigMap with CRD:</p> <ol> <li>Merge CRD implementation to main branch</li> <li>Update all documentation to reference CRD (remove ConfigMap)</li> <li>Update examples to use CRD syntax</li> <li>Announce breaking change in README and release notes</li> <li>First release (v0.1.0) with CRD-based configuration</li> </ol> <p>No migration guide needed - anyone using pre-release versions must update to CRD.</p>"},{"location":"development/crd-validation-design/#open-questions","title":"Open Questions","text":"<ol> <li>CRD versioning: Should we plan for v1alpha1 \u2192 v1beta1 \u2192 v1 graduation path?</li> <li>Status subresource: Should we surface validation results in status field?</li> <li>Webhook high availability: Multiple replicas need leader election for webhook?</li> <li>Test isolation: Should tests run in parallel or sequentially?</li> </ol>"},{"location":"development/crd-validation-design/#references","title":"References","text":"<ul> <li>Kubebuilder Book</li> <li>Kubernetes CRD Best Practices</li> <li>Helm CRD Handling</li> <li>Admission Webhook Best Practices</li> </ul>"},{"location":"development/design/","title":"Design Documentation","text":""},{"location":"development/design/#overview","title":"Overview","text":"<p>This document provides the architectural design for the HAProxy Template Ingress Controller. The controller is a Kubernetes operator that manages HAProxy load balancer configurations through template-driven configuration generation, continuously monitoring Kubernetes resources and translating them into validated HAProxy configurations.</p> <p>The design follows event-driven architecture principles with clean component separation. Components communicate through a central EventBus using pub/sub and request-response patterns, enabling observability, testability, and loose coupling.</p> <p>Why this architecture: - Event-driven design allows components to evolve independently - Template-based approach provides maximum flexibility without annotation constraints - Multi-phase validation prevents invalid configurations from reaching production - Runtime API optimization minimizes service disruption during updates</p>"},{"location":"development/design/#navigation","title":"Navigation","text":"<p>The design documentation is organized into focused documents:</p> <ul> <li> <p>Considerations - Assumptions about the operating environment, constraints imposed by HAProxy Dataplane API, and Kubernetes cluster requirements</p> </li> <li> <p>Architecture Overview - High-level system architecture with component diagrams showing the controller's internal event-driven structure and validation flow</p> </li> <li> <p>Package Structure - Go package organization including directory structure, package dependencies, and key interfaces</p> </li> <li> <p>Sequence Diagrams - Dynamic behavior including startup initialization, resource change handling, configuration validation, and zero-reload deployment</p> </li> <li> <p>Deployment - Kubernetes deployment architecture showing controller pods, HAProxy pods, container configuration, and network topology</p> </li> <li> <p>Design Decisions - Key architectural choices with rationale covering validation strategy, template engine selection, concurrency model, and observability integration</p> </li> <li> <p>Runtime Introspection - Debug HTTP endpoints for runtime state inspection, event history tracking, and integration with acceptance testing</p> </li> <li> <p>Configuration - User interface design showing how you configure the controller through HAProxyTemplateConfig CRD</p> </li> <li> <p>Appendices - Definitions, abbreviations, and external references</p> </li> </ul>"},{"location":"development/design/#core-capabilities","title":"Core Capabilities","text":"<p>The controller provides these capabilities:</p> <p>Template-Driven Configuration Generate HAProxy configurations using Jinja2-like templates with access to any Kubernetes resources you choose to watch. Templates give you complete control over the HAProxy configuration without annotation limitations.</p> <p>Dynamic Resource Watching Monitor any Kubernetes resource types (Ingress, Service, ConfigMap, custom CRDs) you specify. Resources are indexed using JSONPath expressions for fast template lookups. You define which resources to watch and how to index them.</p> <p>Validation-First Deployment All generated configurations pass through two-phase validation before deployment. The client-native parser validates syntax and structure, then the HAProxy binary performs semantic validation. This prevents invalid configurations from reaching production instances.</p> <p>Zero-Reload Optimization Configuration changes that only modify server weight, address, port, or maintenance state are applied through HAProxy's runtime API without process reloads. This maintains existing connections and minimizes service disruption. Changes requiring structural modifications trigger a reload.</p> <p>Structured Configuration Comparison The controller parses both current and desired configurations into structured representations and performs fine-grained comparison at the attribute level. This minimizes unnecessary deployments and maximizes use of runtime API operations.</p>"},{"location":"development/design/#design-principles","title":"Design Principles","text":"<p>Fail-Safe Operation Invalid configurations are rejected before reaching production. The validation phase catches syntax errors, semantic issues, and configuration conflicts. If validation fails, the current production configuration remains unchanged.</p> <p>Performance Through Indexing Resource indexing using JSONPath expressions enables O(1) lookups in templates. Debouncing prevents rapid successive template renders during bulk resource changes. Rate limiting prevents deployment conflicts.</p> <p>Observable Event Flow All component interactions flow through the EventBus. The Event Commentator subscribes to all events and produces structured logs with contextual insights. Metrics track reconciliation cycles, validation results, and deployment success rates.</p> <p>Clean Component Separation Pure business logic components (templating, k8s, dataplane) have no event dependencies and can be tested in isolation. Event adapters in the controller package coordinate these pure components through EventBus messages.</p>"},{"location":"development/design/#see-also","title":"See Also","text":""},{"location":"development/design/#user-guides","title":"User Guides","text":"<ul> <li>Templating Guide - User guide for writing templates</li> <li>CRD Reference - HAProxyTemplateConfig CRD documentation</li> <li>Supported Configuration Reference - What HAProxy features you can configure</li> </ul>"},{"location":"development/design/#operations","title":"Operations","text":"<ul> <li>High Availability - Leader election and HA deployments</li> <li>Monitoring - Prometheus metrics and alerting</li> <li>Debugging - Runtime introspection and troubleshooting</li> <li>Security - RBAC and security best practices</li> <li>Performance - Resource sizing and optimization</li> </ul>"},{"location":"development/design/#package-documentation","title":"Package Documentation","text":"<ul> <li>Controller Package - Event-driven controller implementation</li> <li>Template Engine - Template engine API reference</li> <li>Kubernetes Integration - Resource watching and indexing API</li> <li>Dataplane Integration - HAProxy configuration synchronization</li> </ul>"},{"location":"development/linting/","title":"Linting and Code Quality","text":"<p>This document describes the linting and code quality tools configured for this project.</p>"},{"location":"development/linting/#overview","title":"Overview","text":"<p>The project uses multiple linters to ensure code quality, security, and architectural consistency:</p> <ul> <li>golangci-lint: Primary Go linter with 35+ enabled checks</li> <li>govulncheck: Security vulnerability scanner from the Go team</li> <li>arch-go: Architecture linter enforcing dependency rules</li> </ul>"},{"location":"development/linting/#quick-start","title":"Quick Start","text":"<pre><code># Run all checks\nmake check-all\n\n# Run individual tools\nmake lint        # golangci-lint + arch-go\nmake audit       # govulncheck\n\n# Auto-fix issues where possible\nmake lint-fix\n</code></pre>"},{"location":"development/linting/#golangci-lint-configuration","title":"golangci-lint Configuration","text":"<p>The <code>.golangci.yml</code> configuration enables comprehensive linting tailored for Kubernetes controllers:</p>"},{"location":"development/linting/#enabled-linter-categories","title":"Enabled Linter Categories","text":"<p>Error Detection &amp; Correctness - errcheck, govet, staticcheck, ineffassign, unused - gosimple, bodyclose, errchkjson, nilerr, nilnil</p> <p>Security - gosec: Detects security vulnerabilities and hardcoded credentials</p> <p>Style &amp; Best Practices - revive, gocritic, gofmt, goimports, misspell - unconvert, unparam, nakedret, whitespace, godot - importas: Enforces Kubernetes package aliases - goprintffuncname: Checks printf-like function naming</p> <p>Code Complexity - gocyclo: Cyclomatic complexity (threshold: 20) - goconst: Repeated strings - dupl: Code duplication (threshold: 150 lines)</p> <p>Performance - prealloc: Slice preallocation opportunities - copyloopvar: Loop variable reference issues (Go 1.22+)</p> <p>Maintenance - godox: Detects TODO/FIXME/BUG comments - asciicheck: Ensures only ASCII characters - bidichk: Detects dangerous Unicode bidirectional characters - dogsled: Detects too many blank identifiers - makezero: Detects improper slice/map initialization - nolintlint: Reports ill-formed or insufficient nolint directives</p> <p>Testing - thelper: Test helper function checks</p>"},{"location":"development/linting/#exclusions","title":"Exclusions","text":"<p>The configuration excludes checks for: - Generated code in <code>codegen/**/*.gen.go</code> - Test files (<code>*_test.go</code>) have relaxed rules - Integration tests (<code>tests/integration/</code>) have additional exemptions</p>"},{"location":"development/linting/#linter-specific-settings","title":"Linter-Specific Settings","text":"<p>gocritic: Enabled tags include diagnostic, style, performance, and experimental. Some checks are disabled to reduce noise (dupImport, ifElseChain, octalLiteral, whyNoLint, wrapperFunc).</p> <p>govet: All checks enabled except fieldalignment (too noisy) and shadow (intentional shadowing sometimes used).</p> <p>gosec: Excludes G114 (HTTP server timeouts handled at infrastructure level) and G404 (weak random not used for security).</p> <p>godox: Tracks BUG, FIXME, and HACK keywords in comments.</p> <p>revive: Function length limited to 50 lines, cognitive complexity to 20. Exported and package-comments rules disabled for internal packages.</p>"},{"location":"development/linting/#import-aliases","title":"Import Aliases","text":"<p>The following import aliases are enforced:</p> <pre><code>import (\n    corev1 \"k8s.io/api/core/v1\"\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n    apierrors \"k8s.io/apimachinery/pkg/api/errors\"\n    corev1client \"k8s.io/client-go/kubernetes/typed/core/v1\"\n    haproxy \"github.com/haproxytech/client-native/v6\"\n)\n</code></pre>"},{"location":"development/linting/#govulncheck","title":"govulncheck","text":"<p>Scans for known security vulnerabilities in Go dependencies and standard library.</p> <pre><code>make audit\n</code></pre> <p>If vulnerabilities are found: 1. Review the output carefully 2. Update affected dependencies: <code>go get -u &lt;package&gt;@latest</code> 3. Run <code>go mod tidy</code> 4. Re-run <code>make audit</code></p>"},{"location":"development/linting/#arch-go","title":"arch-go","text":"<p>Validates architectural dependency rules defined in <code>arch-go.yml</code>.</p>"},{"location":"development/linting/#current-architecture-rules","title":"Current Architecture Rules","text":"<ol> <li>controller packages: Can depend on all other top-level packages (core, dataplane, events, k8s, templating, codegen)</li> <li>core packages: Must not depend on controller, dataplane, k8s, templating</li> <li>dataplane packages: Must not depend on controller, core, events, k8s, templating</li> <li>events package: Must not depend on other top-level packages</li> <li>k8s packages: Must not depend on other top-level packages</li> <li>templating package: Must not depend on other top-level packages</li> </ol>"},{"location":"development/linting/#integration-with-golangci-lint","title":"Integration with golangci-lint","text":"<p>The <code>make lint</code> target runs both golangci-lint and arch-go together. If arch-go is not installed, it will be automatically installed before running:</p> <pre><code>make lint  # Runs both golangci-lint and arch-go\n</code></pre> <p>You can also run arch-go directly:</p> <pre><code># Install if not present\ngo install github.com/arch-go/arch-go@latest\n\n# Run directly\narch-go\n</code></pre>"},{"location":"development/linting/#cicd-integration","title":"CI/CD Integration","text":"<p>To integrate these linters into your CI/CD pipeline:</p> <pre><code># Example GitHub Actions workflow\nname: Lint\non: [pull_request]\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-go@v5\n        with:\n          go-version: '1.25'\n\n      - name: Run linters\n        run: make lint  # Runs both golangci-lint and arch-go\n\n      - name: Run govulncheck\n        run: make audit\n</code></pre>"},{"location":"development/linting/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>The project supports automatic linting and auditing before each commit using pre-commit.</p>"},{"location":"development/linting/#setup","title":"Setup","text":"<p>Install the pre-commit framework (one-time):</p> <pre><code># Using pip\npip install pre-commit\n\n# Using Homebrew (macOS/Linux)\nbrew install pre-commit\n\n# Using conda\nconda install -c conda-forge pre-commit\n</code></pre> <p>Install the git hooks (one-time per repository clone):</p> <pre><code>pre-commit install\n</code></pre>"},{"location":"development/linting/#usage","title":"Usage","text":"<p>Once installed, pre-commit automatically runs before each <code>git commit</code>:</p> <pre><code># Hooks run automatically\ngit commit -m \"Add new feature\"\n\n# Output example:\n# make lint........................................Passed\n# make audit.......................................Passed\n# [main abc1234] Add new feature\n</code></pre> <p>If any hook fails, the commit is blocked until issues are fixed:</p> <pre><code># Hooks detect issues\ngit commit -m \"Add feature with linting issues\"\n\n# Output example:\n# make lint........................................Failed\n# - hook id: make-lint\n# - exit code: 1\n#\n# [golangci-lint output showing errors]\n\n# Fix issues, then commit again\nmake lint-fix  # Auto-fix where possible\ngit add .\ngit commit -m \"Add feature with linting issues\"\n</code></pre>"},{"location":"development/linting/#bypassing-hooks","title":"Bypassing Hooks","text":"<p>Sometimes you need to commit without running hooks (e.g., for WIP commits):</p> <pre><code># Skip all hooks\ngit commit --no-verify -m \"WIP: work in progress\"\n\n# Skip specific hook\nSKIP=make-audit git commit -m \"Skip audit for this commit\"\n</code></pre>"},{"location":"development/linting/#manual-execution","title":"Manual Execution","text":"<p>Run hooks manually without committing:</p> <pre><code># Run all hooks on all files\npre-commit run --all-files\n\n# Run specific hook\npre-commit run make-lint --all-files\n\n# Run on staged files only (default)\npre-commit run\n</code></pre>"},{"location":"development/linting/#configuration","title":"Configuration","text":"<p>The pre-commit configuration is defined in <code>.pre-commit-config.yaml</code>:</p> <pre><code>repos:\n  - repo: local\n    hooks:\n      - id: make-lint\n        name: make lint\n        entry: make lint\n        language: system\n        pass_filenames: false\n        files: \\.go$\n\n      - id: make-audit\n        name: make audit\n        entry: make audit\n        language: system\n        pass_filenames: false\n        always_run: true\n</code></pre> <p>The configuration uses local hooks that execute the existing <code>make lint</code> and <code>make audit</code> targets, ensuring consistency with CI and manual workflows.</p>"},{"location":"development/linting/#troubleshooting","title":"Troubleshooting","text":"<p>Hook doesn't run on commit - Verify installation: <code>pre-commit --version</code> - Reinstall hooks: <code>pre-commit install</code> - Check <code>.git/hooks/pre-commit</code> exists</p> <p>Slow hook execution - Use <code>SKIP=make-audit</code> to skip security scanning for quick commits - Run <code>make audit</code> separately or in CI - Pre-commit caches results for unchanged files</p> <p>Hook fails but manual <code>make lint</code> passes - Ensure working directory is clean: <code>git status</code> - Run <code>pre-commit run --all-files</code> to see full output - Check if pre-commit is using correct Go version</p>"},{"location":"development/linting/#common-issues","title":"Common Issues","text":""},{"location":"development/linting/#fixing-format-issues","title":"Fixing Format Issues","text":"<p>Many formatting issues can be auto-fixed:</p> <pre><code>make lint-fix\nmake fmt  # gofmt\n</code></pre>"},{"location":"development/linting/#high-complexity-functions","title":"High Complexity Functions","text":"<p>When gocyclo reports high complexity (&gt;20): 1. Consider breaking the function into smaller functions 2. Extract complex conditional logic into separate functions 3. Use table-driven approaches for complex switch/if statements</p>"},{"location":"development/linting/#hardcoded-credentials","title":"Hardcoded Credentials","text":"<p>If gosec reports G101 (potential hardcoded credentials): - Review the code to ensure it's a false positive - If it's configuration (like default names), add a comment explaining it - For real credentials, use environment variables or Kubernetes Secrets</p>"},{"location":"development/linting/#cyclomatic-complexity","title":"Cyclomatic Complexity","text":"<p>Functions with complexity &gt;20 should be refactored. Common patterns: - Extract helper functions - Use early returns to reduce nesting - Replace long if-else chains with switch statements or maps</p>"},{"location":"development/linting/#customizing-configuration","title":"Customizing Configuration","text":"<p>To adjust linter settings, edit <code>.golangci.yml</code>:</p> <pre><code>linters-settings:\n  gocyclo:\n    min-complexity: 20  # Adjust threshold\n\n  revive:\n    rules:\n      - name: function-length\n        arguments: [50, 0]  # Max 50 lines per function\n</code></pre>"},{"location":"development/linting/#tool-versions","title":"Tool Versions","text":"<p>Tools are managed via Go modules (see <code>go.mod</code> tool section):</p> <pre><code># Update tools\ngo get github.com/golangci/golangci-lint/cmd/golangci-lint@latest\ngo get golang.org/x/vuln/cmd/govulncheck@latest\ngo mod tidy\n\n# Or use the helper target\nmake install-tools\n</code></pre>"},{"location":"development/linting/#references","title":"References","text":"<ul> <li>golangci-lint documentation</li> <li>govulncheck documentation</li> <li>arch-go documentation</li> <li>Kubernetes controller-runtime linting practices</li> </ul>"},{"location":"development/design/appendices/","title":"Appendices","text":""},{"location":"development/design/appendices/#appendices-and-references","title":"Appendices and References","text":""},{"location":"development/design/appendices/#definitions-and-abbreviations","title":"Definitions and Abbreviations","text":"<ul> <li>API: Application Programming Interface</li> <li>CRD: Custom Resource Definition - Kubernetes extension mechanism</li> <li>Dataplane API: HAProxy's management interface for runtime configuration</li> <li>GVR: GroupVersionResource - Kubernetes resource identifier</li> <li>HAProxy: High Availability Proxy - Load balancer and reverse proxy</li> <li>IC: Ingress Controller</li> <li>Informer: Kubernetes client-go pattern for watching and caching resources</li> <li>O(1): Constant time complexity - performance independent of input size</li> <li>Runtime API: HAProxy's socket-based interface for zero-reload updates</li> <li>SharedInformerFactory: client-go factory for creating resource watchers with shared caches</li> </ul>"},{"location":"development/design/appendices/#references","title":"References","text":"<ol> <li>HAProxy Documentation</li> <li>Configuration Manual: https://www.haproxy.com/documentation/haproxy-configuration-manual/latest/</li> <li>Dataplane API: https://www.haproxy.com/documentation/haproxy-data-plane-api/</li> <li> <p>Runtime API: https://www.haproxy.com/documentation/haproxy-runtime-api/</p> </li> <li> <p>HAProxy Go Libraries</p> </li> <li>client-native: https://github.com/haproxytech/client-native<ul> <li>Configuration parser and validator</li> <li>Used for syntax validation without running HAProxy</li> </ul> </li> <li> <p>dataplaneapi: https://github.com/haproxytech/dataplaneapi</p> <ul> <li>Reference implementation for validation strategies</li> <li>Configuration management patterns</li> </ul> </li> <li> <p>Kubernetes Client Libraries</p> </li> <li>client-go: https://github.com/kubernetes/client-go<ul> <li>Official Kubernetes Go client</li> <li>Informer pattern documentation</li> </ul> </li> <li> <p>apimachinery: https://github.com/kubernetes/apimachinery</p> <ul> <li>Common machinery for Kubernetes API interactions</li> </ul> </li> <li> <p>Template Engines</p> </li> <li> <p>gonja v2: https://github.com/nikolalohinski/gonja</p> <ul> <li>Pure Go Jinja2 template engine, actively maintained</li> <li>Latest release: v2.4.1 (January 2025)</li> <li>Recommended for Jinja2-compatible templating</li> </ul> </li> <li> <p>Observability</p> </li> <li>Prometheus client_golang: https://github.com/prometheus/client_golang</li> <li> <p>OpenTelemetry Go: https://github.com/open-telemetry/opentelemetry-go</p> </li> <li> <p>Design Patterns</p> </li> <li>Kubernetes Operator Pattern: https://kubernetes.io/docs/concepts/extend-kubernetes/operator/</li> <li>Controller Pattern: https://kubernetes.io/docs/concepts/architecture/controller/</li> </ol>"},{"location":"development/design/architecture-overview/","title":"Architecture Overview","text":""},{"location":"development/design/architecture-overview/#overview","title":"Overview","text":"<p>The haproxy-template-ingress-controller is a Kubernetes operator that manages HAProxy load balancer configurations through template-driven configuration generation. The system continuously monitors Kubernetes resources and translates them into HAProxy configuration files using a powerful templating engine.</p> <p>Core Capabilities:</p> <ul> <li>Template-Driven Configuration: Uses a feature-rich template engine to generate HAProxy configurations from Kubernetes resources</li> <li>Dynamic Resource Watching: Monitors user-defined Kubernetes resource types (Ingress, Service, ConfigMap, custom CRDs)</li> <li>Validation-First Deployment: All configurations are parsed and validated before deployment to production instances</li> <li>Zero-Reload Optimization: Leverages HAProxy Runtime API for configuration changes that don't require process reloads</li> <li>Structured Comparison: Intelligently compares configurations to minimize deployments and maximize use of runtime operations</li> </ul> <p>Operational Model:</p> <p>The controller operates through event-driven coordination where components communicate exclusively via EventBus pub/sub:</p> <ol> <li>Resource Watchers monitor Kubernetes resources and publish change events to EventBus</li> <li>Reconciler subscribes to change events, debounces rapid changes, and publishes reconciliation trigger events</li> <li>Renderer subscribes to reconciliation trigger events, queries indexed resources from k8s stores, renders templates using pkg/templating, and publishes rendered configuration events</li> <li>HAProxyValidator subscribes to rendered configuration events, performs two-phase validation (client-native parser + haproxy binary via pkg/dataplane), and publishes validation result events</li> <li>DeploymentScheduler subscribes to validation events, enforces rate limiting, queues deployments if needed, and publishes deployment scheduled events</li> <li>Deployer subscribes to deployment scheduled events, executes parallel deployments to all HAProxy endpoints using pkg/dataplane, and publishes deployment completion events</li> <li>Executor subscribes to key lifecycle events for observability and publishes reconciliation completion events</li> <li>EventBus coordinates all component interactions - no direct component-to-component function calls</li> <li>All components publish completion/failure events for metrics, logging, and further coordination</li> </ol> <p>Key Design Principles:</p> <ul> <li>Fail-Safe: Invalid configurations are rejected before reaching production</li> <li>Performance: Debouncing prevents rapid successive renders, indexing enables fast lookups</li> <li>Observability: Comprehensive metrics (Prometheus) and distributed tracing (OpenTelemetry)</li> <li>Flexibility: Templates provide complete control over HAProxy configuration, no annotation limitations</li> </ul>"},{"location":"development/design/architecture-overview/#component-diagrams","title":"Component Diagrams","text":""},{"location":"development/design/architecture-overview/#high-level-system-components","title":"High-Level System Components","text":"<pre><code>graph TB\n    subgraph \"Kubernetes Cluster\"\n        K8S[Kubernetes API Server]\n\n        subgraph \"Controller Pod\"\n            CTRL[Controller&lt;br/&gt;- Resource Watching&lt;br/&gt;- Template Rendering&lt;br/&gt;- Config Validation&lt;br/&gt;- Deployment Orchestration]\n            VAL[Validation Module&lt;br/&gt;- client-native Parser&lt;br/&gt;- haproxy Binary Check]\n        end\n\n        subgraph \"HAProxy Pod 1\"\n            HAP1[HAProxy&lt;br/&gt;Load Balancer]\n            DP1[Dataplane API&lt;br/&gt;:5555]\n        end\n\n        subgraph \"HAProxy Pod 2\"\n            HAP2[HAProxy&lt;br/&gt;Load Balancer]\n            DP2[Dataplane API&lt;br/&gt;:5555]\n        end\n\n        CONFIG[ConfigMap&lt;br/&gt;Controller Configuration]\n        RES[Resources&lt;br/&gt;Ingress, Service, etc.]\n    end\n\n    K8S --&gt;|Watch Events| CTRL\n    CONFIG --&gt;|Read Config| CTRL\n    RES --&gt;|Watch Events| CTRL\n    CTRL --&gt;|Render &amp; Validate| VAL\n    VAL --&gt;|Deploy Config| DP1\n    VAL --&gt;|Deploy Config| DP2\n    DP1 --&gt;|Configure| HAP1\n    DP2 --&gt;|Configure| HAP2\n    HAP1 --&gt;|Stats/Health| DP1\n    HAP2 --&gt;|Stats/Health| DP2\n\n    style CTRL fill:#4CAF50\n    style VAL fill:#2196F3\n    style HAP1 fill:#FF9800\n    style HAP2 fill:#FF9800</code></pre> <p>Component Descriptions:</p> <ul> <li>Controller: Main controller process that watches Kubernetes resources, renders templates, and orchestrates configuration deployment</li> <li>Validation Module: Integrated validation using haproxytech/client-native library for parsing and haproxy binary for configuration checks</li> <li>Dataplane API: HAProxy's management interface for receiving configuration updates and performing runtime operations</li> <li>HAProxy: Production load balancer instances that serve traffic</li> </ul>"},{"location":"development/design/architecture-overview/#controller-internal-architecture","title":"Controller Internal Architecture","text":"<pre><code>graph TB\n    subgraph ext[\"External Systems\"]\n        K8S[\"Kubernetes API&lt;br/&gt;(Resource Events)\"]\n        HAP[\"HAProxy Instances&lt;br/&gt;(Dataplane API)\"]\n    end\n\n    subgraph controller[\"Controller Process - Event-Driven Architecture\"]\n        direction TB\n\n        EB[\"EventBus&lt;br/&gt;Central Pub/Sub Coordinator&lt;br/&gt;~50 Event Types\"]\n\n        subgraph watchers[\"Resource Watchers\"]\n            direction LR\n            CW[\"Config&lt;br/&gt;Watcher\"]\n            RW[\"Resource&lt;br/&gt;Watcher\"]\n        end\n\n        subgraph reconciliation[\"Reconciliation Components\"]\n            direction LR\n            RC[\"Reconciler&lt;br/&gt;(Debouncer)\"]\n            EX[\"Executor&lt;br/&gt;(Observability)\"]\n        end\n\n        subgraph pipeline[\"Event-Driven Pipeline\"]\n            direction LR\n            REND[\"Renderer\"]\n            VAL[\"HAProxy&lt;br/&gt;Validator\"]\n            SCHED[\"Deployment&lt;br/&gt;Scheduler\"]\n            DEPL[\"Deployer\"]\n        end\n\n        subgraph support[\"Support Components\"]\n            direction LR\n            DISC[\"Discovery\"]\n            METR[\"Metrics\"]\n            COMM[\"Commentator\"]\n        end\n\n        CW &amp; RW --&gt;|Publish| EB\n        EB --&gt;|Subscribe| RC\n        RC --&gt;|Publish| EB\n        EB --&gt;|Subscribe| REND\n        REND --&gt;|Publish| EB\n        EB --&gt;|Subscribe| VAL\n        VAL --&gt;|Publish| EB\n        EB --&gt;|Subscribe| SCHED\n        SCHED --&gt;|Publish| EB\n        EB --&gt;|Subscribe| DEPL\n        DEPL --&gt;|Publish| EB\n        EB --&gt;|Subscribe| EX &amp; DISC &amp; METR &amp; COMM\n        EX &amp; DISC --&gt;|Publish| EB\n    end\n\n    K8S --&gt;|Watch| RW\n    DEPL --&gt;|Deploy| HAP\n\n    style EB fill:#FFC107,stroke:#F57C00,stroke-width:4px\n    style watchers fill:#E3F2FD\n    style reconciliation fill:#F3E5F5\n    style pipeline fill:#C8E6C9\n    style support fill:#FFF9C4\n    style ext fill:#F5F5F5</code></pre> <p>Event-Driven Data Flow:</p> <ol> <li>Config/Resource Watchers receive Kubernetes changes and publish events to EventBus</li> <li>Reconciler subscribes to change events, debounces rapid changes (default 500ms), filters initial sync events, and publishes ReconciliationTriggeredEvent</li> <li>Renderer subscribes to ReconciliationTriggeredEvent, queries k8s stores for resources, renders templates via pkg/templating pure library, publishes TemplateRenderedEvent</li> <li>HAProxyValidator subscribes to TemplateRenderedEvent, validates using pkg/dataplane pure validation functions (syntax + semantics), publishes ValidationCompletedEvent or ValidationFailedEvent</li> <li>DeploymentScheduler subscribes to ValidationCompletedEvent and HAProxyPodsDiscoveredEvent, enforces rate limiting (default 2s minimum interval), implements \"latest wins\" queueing, publishes DeploymentScheduledEvent</li> <li>Deployer subscribes to DeploymentScheduledEvent, executes parallel deployments to all HAProxy endpoints using pkg/dataplane client, publishes InstanceDeployedEvent and DeploymentCompletedEvent</li> <li>Executor subscribes to key lifecycle events for observability, publishes ReconciliationStartedEvent and ReconciliationCompletedEvent with duration metrics</li> <li>Support Components (Discovery, Metrics, Commentator) subscribe to relevant events for their specific purposes</li> <li>All components publish completion/failure events that flow back through EventBus for metrics, logging, and coordination</li> </ol> <p>Key Architecture Properties:</p> <ul> <li>EventBus is the single coordination mechanism - zero direct component-to-component function calls</li> <li>Event-Driven Components (Renderer, Validator, Scheduler, Deployer) are wrappers around pure libraries (pkg/templating, pkg/dataplane, pkg/k8s)</li> <li>Pure Libraries (pkg/templating, pkg/dataplane, pkg/k8s) contain testable business logic with no event dependencies</li> <li>Event Adapters translate between EventBus pub/sub and pure library function calls</li> <li>Extensibility - new features can subscribe to existing events without modifying existing code</li> <li>Independent Testing - pure libraries can be unit tested, event adapters can be integration tested</li> </ul>"},{"location":"development/design/architecture-overview/#validation-flow","title":"Validation Flow","text":"<pre><code>graph TD\n    RENDER[Rendered Configuration]\n    PARSE[client-native Parser&lt;br/&gt;Syntax &amp; Structure Check]\n    BIN[haproxy Binary&lt;br/&gt;Semantic Validation]\n    DEPLOY[Deploy to Production]\n    ERROR[Reject &amp; Log Error]\n\n    RENDER --&gt; PARSE\n    PARSE --&gt;|Valid Syntax| BIN\n    PARSE --&gt;|Invalid| ERROR\n    BIN --&gt;|Valid Semantics| DEPLOY\n    BIN --&gt;|Invalid| ERROR\n\n    style PARSE fill:#2196F3\n    style BIN fill:#4CAF50\n    style DEPLOY fill:#FF9800\n    style ERROR fill:#F44336</code></pre> <p>Validation Strategy:</p> <p>The two-phase validation eliminates the need for a separate validation sidecar container:</p> <ol> <li>Phase 1 - Syntax Parsing: client-native library parses configuration structure and validates against HAProxy config grammar</li> <li>Phase 2 - Semantic Validation: haproxy binary (<code>haproxy -c -f config</code>) performs full semantic validation including resource availability checks. Writes auxiliary files to actual HAProxy directories (with mutex locking) to match Dataplane API validation behavior exactly.</li> </ol> <p>This approach provides the same validation guarantees as running a full HAProxy instance while being more lightweight and faster.</p>"},{"location":"development/design/configuration/","title":"Configuration","text":""},{"location":"development/design/configuration/#user-interface-design","title":"User Interface Design","text":"<p>This is a headless controller with no graphical user interface. Interaction occurs through:</p> <ol> <li>ConfigMap: Primary configuration interface</li> <li>Kubernetes Resources: Watched resources (Ingress, Service, etc.)</li> <li>Metrics Endpoint: Prometheus metrics on <code>:9090/metrics</code> (configurable)</li> <li>Health Endpoint: Liveness/readiness on <code>:8080/healthz</code> (configurable)</li> <li>Debug Endpoint: Runtime introspection on configurable port (disabled by default, typically <code>:6060/debug/vars</code> when enabled) with JSONPath support and pprof</li> <li>Logs: Structured JSON logs for operational visibility</li> </ol>"},{"location":"development/design/configuration/#configuration-example","title":"Configuration Example","text":"<p>The following example demonstrates a complete controller configuration with all major features:</p> <pre><code>pod_selector:\n  match_labels:\n    app: haproxy\n    component: loadbalancer\n\n# Grouped controller configuration (previously CLI options)\ncontroller:\n  healthz_port: 8080\n  metrics_port: 9090\n\nlogging:\n  verbose: 2  # 0=WARNING, 1=INFO, 2=DEBUG\n\nvalidation:\n  dataplane_host: localhost\n  dataplane_port: 5555\n\n# Fields to omit from indexed resources (reduces memory usage)\nwatched_resources_ignore_fields:\n  - metadata.managedFields\n\nwatched_resources:\n  ingresses:\n    api_version: networking.k8s.io/v1\n    kind: Ingress\n    # Enable validation webhook for Ingress resources to prevent faulty configs\n    enable_validation_webhook: true\n    # Default indexing by namespace and name for standard iteration\n    index_by: [\"metadata.namespace\", \"metadata.name\"]\n\n  endpoints:\n    api_version: discovery.k8s.io/v1\n    kind: EndpointSlice\n    # Leave validation disabled for critical resources like EndpointSlices\n    enable_validation_webhook: false\n    # Custom indexing by service name for O(1) service-to-endpoints matching\n    index_by: [\"metadata.labels['kubernetes.io/service-name']\"]\n\n  secrets:\n    api_version: v1\n    kind: Secret\n    # Enable validation for TLS secrets to catch certificate issues early\n    enable_validation_webhook: true\n    # Index by namespace and type for efficient TLS secret lookup\n    index_by: [\"metadata.namespace\", \"type\"]\n\n  services:\n    api_version: v1\n    kind: Service\n    enable_validation_webhook: false\n    # Index by namespace and app label for cross-resource matching\n    index_by: [\"metadata.namespace\", \"metadata.labels['app']\"]\n\ntemplate_snippets:\n  backend-name:\n    name: backend-name\n    template: |\n      ing_{{ ingress.metadata.namespace }}_{{ ingress.metadata.name }}_{{ path.backend.service.name }}_{{ path.backend.service.port.name | default(path.backend.service.port.number) }}\n\n  path-map-entry:\n    name: path-map-entry\n    template: |\n      {{ \"\" }}\n      {% for ingress in resources.ingresses.List() %}\n      {% for rule in (ingress.spec.rules | default([]) | selectattr(\"http\", \"defined\")) %}\n      {% for path in (rule.http.paths | default([]) | selectattr(\"path\", \"defined\") | selectattr(\"pathType\", \"in\", path_types)) %}\n      {{ rule.host }}{{ path.path }} {% include \"backend-name\" %}{{ suffix }}\n      {% endfor %}\n      {% endfor %}\n      {% endfor %}\n\n  validate-ingress:\n    name: validate-ingress\n    template: |\n      {#- Validation snippet for ingress resources #}\n      {%- if not ingress.spec %}\n        {% do register_error('ingresses', ingress.metadata.uid, 'Ingress missing spec') %}\n      {%- endif %}\n      {%- if ingress.spec.rules %}\n        {%- for rule in ingress.spec.rules %}\n          {%- if not rule.host %}\n            {% do register_error('ingresses', ingress.metadata.uid, 'Ingress rule missing host') %}\n          {%- endif %}\n        {%- endfor %}\n      {%- endif %}\n\n  backend-servers:\n    name: backend-servers\n    template: |\n      {#- Pre-allocated server pool with auto-expansion #}\n      {%- set initial_slots = 10 %}  {#- Single place to adjust initial slots #}\n\n      {#- Collect active endpoints #}\n      {%- set active_endpoints = [] %}\n      {%- for endpoint_slice in resources.get('endpoints', {}).get_indexed(service_name) %}\n        {%- for endpoint in endpoint_slice.get('endpoints') | default([], true) %}\n          {%- for address in endpoint.addresses %}\n            {%- set _ = active_endpoints.append({'name': endpoint.targetRef.name, 'address': address, 'port': port}) %}\n          {%- endfor %}\n        {%- endfor %}\n      {%- endfor %}\n\n      {#- Calculate required slots using mathematical approach #}\n      {%- set active_count = active_endpoints|length %}\n      # active count = {{ active_count }}\n      {%- if initial_slots &gt; 0 and active_count &gt; 0 %}\n        {%- set ratio = active_count / initial_slots %}\n        {%- set power_of_two = [0, ratio | log(2) | round(0, 'ceil')] | max %}\n      {%- else %}\n        {%- set power_of_two = 0 %}\n      {%- endif %}\n      {%- set max_servers = initial_slots * (2 ** power_of_two) | int %}\n      # max servers = {{ max_servers }}\n\n      {#- Generate all server slots with fixed names #}\n      {%- for i in range(1, max_servers + 1) %}\n        {%- if loop.index0 &lt; active_endpoints|length %}\n          {#- Active server with real endpoint #}\n          {%- set endpoint = active_endpoints[loop.index0] %}\n        server SRV_{{ i }} {{ endpoint.address }}:{{ endpoint.port }}\n        {%- else %}\n          {#- Disabled placeholder server #}\n        server SRV_{{ i }} 127.0.0.1:1 disabled\n        {%- endif %}\n      {%- endfor %}\n\n  ingress-backends:\n    name: ingress-backends\n    template: |\n      {#- Generate all backend definitions from ingress resources #}\n      {#- Usage: {% include \"ingress-backends\" %} #}\n      {%- for _, ingress in resources.get('ingresses', {}).items() %}\n      {% include \"validate-ingress\" %}\n      {%- if ingress.spec and ingress.spec.rules %}\n      {%- for rule in ingress.spec.rules %}\n      {%- if rule.http and rule.http.paths %}\n      {%- for path in rule.http.paths %}\n      {%- if path.backend and path.backend.service %}\n      {%- set service_name = path.backend.service.name %}\n      {%- set port = path.backend.service.port.number | default(80) %}\n      backend {% include \"backend-name\" %}\n        balance roundrobin\n        option httpchk GET {{ path.path | default('/') }}\n        default-server check\n        {% include \"backend-servers\" %}\n      {%- endif %}\n      {%- endfor %}\n      {%- endif %}\n      {%- endfor %}\n      {%- endif %}\n      {%- endfor %}\n\nmaps:\n  host.map:\n    template: |\n      {%- for _, ingress in resources.get('ingresses', {}).items() %}\n      {%- for rule in (ingress.spec.get('rules', []) | selectattr(\"http\", \"defined\")) %}\n      {%- set host_without_asterisk = rule.host | replace('*', '', 1) %}\n      {{ host_without_asterisk }} {{ host_without_asterisk }}\n      {%- endfor %}\n      {%- endfor %}\n\n  path-exact.map:\n    template: |\n      # This map is used to match the host header (without \":port\") concatenated with the requested path (without query params) to an HAProxy backend defined in haproxy.cfg.\n      # It should be used with the equality string matcher. Example:\n      #   http-request set-var(txn.path_match) var(txn.host_match),concat(,txn.path,),map(/etc/haproxy/maps/path-exact.map)\n      {%- set path_types = [\"Exact\"] %}\n      {%- set suffix = \"\" %}\n      {% include \"path-map-entry\" %}\n\n  path-prefix-exact.map:\n    template: |\n      # This map is used to match the host header (without \":port\") concatenated with the requested path (without query params) to an HAProxy backend defined in haproxy.cfg.\n\n      {%- for ingress in resources.ingresses.List() -%}\n      {% for rule in (ingress.spec.rules | default([]) | selectattr(\"http\", \"defined\")) %}\n      {% for path in (rule.http.paths | default([]) | selectattr(\"path\", \"defined\") | selectattr(\"pathType\", \"in\", [\"Prefix\", \"ImplementationSpecific\"])) %}\n      {{ rule.host }}{{ path.path }} ing_{{ ingress.metadata.namespace }}_{{ ingress.metadata.name }}_{{ path.backend.service.name }}_{{ path.backend.service.port.name | default(path.backend.service.port.number) }}\n      {% endfor %}\n      {% endfor %}\n      {% endfor %}\n\n  path-prefix.map:\n    template: |\n      # This map is used to match the host header (without \":port\") concatenated with the requested path (without query params) to an HAProxy backend defined in haproxy.cfg.\n      # It should be used with the prefix string matcher. Example:\n      #   http-request set-var(txn.path_match) var(txn.host_match),concat(,txn.path,),map_beg(/etc/haproxy/maps/path-prefix.map)\n      {%- set path_types = [\"Prefix\", \"ImplementationSpecific\"] %}\n      {%- set suffix = \"/\" %}\n      {% include \"path-map-entry\" %}\n\nfiles:\n  400.http:\n    template: |\n      HTTP/1.0 400 Bad Request\n      Cache-Control: no-cache\n      Connection: close\n      Content-Type: text/html\n\n      &lt;html&gt;&lt;body&gt;&lt;h1&gt;400 Bad Request&lt;/h1&gt;\n      &lt;p&gt;Your browser sent a request that this server could not understand.&lt;/p&gt;\n      &lt;/body&gt;&lt;/html&gt;\n\n  403.http:\n    template: |\n      HTTP/1.0 403 Forbidden\n      Cache-Control: no-cache\n      Connection: close\n      Content-Type: text/html\n\n      &lt;html&gt;&lt;body&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;\n      &lt;p&gt;You don't have permission to access this resource.&lt;/p&gt;\n      &lt;/body&gt;&lt;/html&gt;\n\n  408.http:\n    template: |\n      HTTP/1.0 408 Request Time-out\n      Cache-Control: no-cache\n      Connection: close\n      Content-Type: text/html\n\n      &lt;html&gt;&lt;body&gt;&lt;h1&gt;408 Request Time-out&lt;/h1&gt;\n      &lt;p&gt;Your browser didn't send a complete request in time.&lt;/p&gt;\n      &lt;/body&gt;&lt;/html&gt;\n\n  500.http:\n    template: |\n      HTTP/1.0 500 Internal Server Error\n      Cache-Control: no-cache\n      Connection: close\n      Content-Type: text/html\n\n      &lt;html&gt;&lt;body&gt;&lt;h1&gt;500 Internal Server Error&lt;/h1&gt;\n      &lt;p&gt;An internal server error occurred.&lt;/p&gt;\n      &lt;/body&gt;&lt;/html&gt;\n\n  502.http:\n    template: |\n      HTTP/1.0 502 Bad Gateway\n      Cache-Control: no-cache\n      Connection: close\n      Content-Type: text/html\n\n      &lt;html&gt;&lt;body&gt;&lt;h1&gt;502 Bad Gateway&lt;/h1&gt;\n      &lt;p&gt;The server received an invalid response from an upstream server.&lt;/p&gt;\n      &lt;/body&gt;&lt;/html&gt;\n\n  503.http:\n    template: |\n      HTTP/1.0 503 Service Unavailable\n      Cache-Control: no-cache\n      Connection: close\n      Content-Type: text/html\n\n      &lt;html&gt;&lt;body&gt;&lt;h1&gt;503 Service Unavailable&lt;/h1&gt;\n      &lt;p&gt;No server is available to handle this request.&lt;/p&gt;\n      &lt;/body&gt;&lt;/html&gt;\n\n  504.http:\n    template: |\n      HTTP/1.0 504 Gateway Time-out\n      Cache-Control: no-cache\n      Connection: close\n      Content-Type: text/html\n\n      &lt;html&gt;&lt;body&gt;&lt;h1&gt;504 Gateway Time-out&lt;/h1&gt;\n      &lt;p&gt;The server didn't respond in time.&lt;/p&gt;\n      &lt;/body&gt;&lt;/html&gt;\n\nhaproxy_config:\n  template: |\n    global\n      log stdout len 4096 local0 info\n      chroot /var/lib/haproxy\n      user haproxy\n      group haproxy\n      daemon\n      ca-base /etc/ssl/certs\n      crt-base /etc/haproxy/certs\n      tune.ssl.default-dh-param 2048\n\n    defaults\n      mode http\n      log global\n      option httplog\n      option dontlognull\n      option log-health-checks\n      option forwardfor\n      option httpchk GET /\n      timeout connect 5000\n      timeout client 50000\n      timeout server 50000\n      errorfile 400 {{ pathResolver.GetPath(\"400.http\", \"file\") }}\n      errorfile 403 {{ pathResolver.GetPath(\"403.http\", \"file\") }}\n      errorfile 408 {{ pathResolver.GetPath(\"408.http\", \"file\") }}\n      errorfile 500 {{ pathResolver.GetPath(\"500.http\", \"file\") }}\n      errorfile 502 {{ pathResolver.GetPath(\"502.http\", \"file\") }}\n      errorfile 503 {{ pathResolver.GetPath(\"503.http\", \"file\") }}\n      errorfile 504 {{ pathResolver.GetPath(\"504.http\", \"file\") }}\n\n    frontend status\n      bind *:8404\n      no log\n      http-request return status 200 content-type text/plain string \"OK\" if { path /healthz }\n      http-request return status 200 content-type text/plain string \"READY\" if { path /ready }\n\n    frontend http_frontend\n      bind *:80\n\n      # Set a few variables\n      http-request set-var(txn.base) base\n      http-request set-var(txn.path) path\n      http-request set-var(txn.host) req.hdr(Host),field(1,:),lower\n      http-request set-var(txn.host_match) var(txn.host),map(/etc/haproxy/maps/host.map)\n      http-request set-var(txn.host_match) var(txn.host),regsub(^[^.]*,,),map(/etc/haproxy/maps/host.map,'') if !{ var(txn.host_match) -m found }\n      http-request set-var(txn.path_match) var(txn.host_match),concat(,txn.path,),map(/etc/haproxy/maps/path-exact.map)\n      http-request set-var(txn.path_match) var(txn.host_match),concat(,txn.path,),map(/etc/haproxy/maps/path-prefix-exact.map) if !{ var(txn.path_match) -m found }\n      http-request set-var(txn.path_match) var(txn.host_match),concat(,txn.path,),map_beg(/etc/haproxy/maps/path-prefix.map) if !{ var(txn.path_match) -m found }\n\n      # Use path maps for routing\n      use_backend %[var(txn.path_match)]\n\n      # Default backend\n      default_backend default_backend\n\n    {% include \"ingress-backends\" %}\n\n    backend default_backend\n        http-request return status 404\n</code></pre> <p>Configuration Highlights:</p> <ol> <li> <p>Pod Selector: Identifies HAProxy pods using <code>app: haproxy</code> and <code>component: loadbalancer</code> labels</p> </li> <li> <p>Watched Resources: Four resource types with strategic indexing:</p> </li> <li>Ingresses: Indexed by namespace and name for iteration, validation webhook enabled</li> <li>EndpointSlices: Indexed by service name for O(1) endpoint lookup</li> <li>Secrets: Indexed by namespace and type for TLS certificate management</li> <li> <p>Services: Indexed by namespace and app label for cross-resource matching</p> </li> <li> <p>Template Snippets: Reusable template components:</p> </li> <li>backend-name: Generates consistent backend names from ingress metadata</li> <li>path-map-entry: Creates map entries for different path types</li> <li>validate-ingress: Validates ingress resources during rendering</li> <li>backend-servers: Dynamic server pool with auto-expansion (powers-of-two scaling)</li> <li> <p>ingress-backends: Generates complete backend definitions from ingresses</p> </li> <li> <p>Maps: Three routing maps for different match types:</p> </li> <li>host.map: Host-based routing with wildcard support</li> <li>path-exact.map: Exact path matching</li> <li> <p>path-prefix.map: Prefix-based path matching</p> </li> <li> <p>Files: HTTP error response pages (400, 403, 408, 500, 502, 503, 504)</p> </li> <li> <p>HAProxy Configuration: Complete configuration with:</p> </li> <li>Global settings and defaults</li> <li>Status frontend for health checks</li> <li>HTTP frontend with advanced routing using maps</li> <li>Dynamic backend generation via template inclusion</li> </ol> <p>This configuration demonstrates production-ready patterns including resource indexing optimization, validation webhooks for critical resources, and dynamic backend scaling.</p>"},{"location":"development/design/considerations/","title":"Considerations","text":""},{"location":"development/design/considerations/#assumptions","title":"Assumptions","text":"<p>This software acts as ingress controller for a fleet of HAProxy load-balancers. It continuously watches a list of user-defined Kubernetes resource types and uses that as input to render the HAProxy main configuration file <code>haproxy.cfg</code> via templating engine and an optional amount of auxiliary files like custom error pages (<code>500.http</code>) or map files for lookups (<code>host.map</code>). After rendering the files they are validated and pushed via HAProxy Dataplane API (https://www.haproxy.com/documentation/haproxy-data-plane-api/). By pushing only changed config parts via specialized API endpoints we can prevent unnecessary HAProxy reloads. Many specialized endpoints use the HAProxy runtime socket (https://www.haproxy.com/documentation/haproxy-runtime-api/) under the hood and perform changes at runtime. The template rendering is triggered by changed Kubernetes resources. Additionally, a drift prevention monitor periodically triggers deployments (default 60s interval) to detect and correct configuration drift caused by external changes. If any rendered file differs from the rendered files of the last run dataplane a sync is triggered. The drift prevention mechanism ensures the controller's desired configuration is eventually consistent with the actual HAProxy configuration.</p>"},{"location":"development/design/considerations/#constraints","title":"Constraints","text":"<p>The dataplane API does not support all config statements that the HAProxy config language supports (see https://www.haproxy.com/documentation/haproxy-configuration-manual/latest/). Therefore, only rendered configurations that can be parsed by the dataplane API library (https://github.com/haproxytech/client-native) are supported.</p>"},{"location":"development/design/considerations/#system-environment","title":"System Environment","text":"<p>The software is designed to be run inside a Kubernetes container. The target dataplane APIs must also run as Kubernetes container with an HAProxy sidecar. The Kubernetes service account of the ingress controller pod must be able to read all watched configurable resources cluster-wide.</p>"},{"location":"development/design/deployment/","title":"Deployment Diagrams","text":""},{"location":"development/design/deployment/#kubernetes-deployment-architecture","title":"Kubernetes Deployment Architecture","text":"<pre><code>graph TB\n    subgraph \"Kubernetes Cluster\"\n        subgraph \"kube-system Namespace\"\n            API[Kubernetes API Server]\n        end\n\n        subgraph \"haproxy-system Namespace\"\n            subgraph \"Controller Deployment\"\n                CTRL_POD[Controller Pod&lt;br/&gt;Single Container&lt;br/&gt;- Resource Watching&lt;br/&gt;- Template Rendering&lt;br/&gt;- Config Validation&lt;br/&gt;- Deployment]\n            end\n\n            CM[ConfigMap&lt;br/&gt;haproxy-config&lt;br/&gt;Templates &amp; Settings]\n\n            CTRL_SVC[Controller Service&lt;br/&gt;ClusterIP&lt;br/&gt;:8080 healthz&lt;br/&gt;:9090 metrics]\n\n            subgraph \"HAProxy StatefulSet\"\n                subgraph \"haproxy-0\"\n                    HAP1[HAProxy Container&lt;br/&gt;:80, :443, :8404]\n                    DP1[Dataplane API&lt;br/&gt;:5555]\n                end\n\n                subgraph \"haproxy-1\"\n                    HAP2[HAProxy Container&lt;br/&gt;:80, :443, :8404]\n                    DP2[Dataplane API&lt;br/&gt;:5555]\n                end\n\n                subgraph \"haproxy-N\"\n                    HAPN[HAProxy Container&lt;br/&gt;:80, :443, :8404]\n                    DPN[Dataplane API&lt;br/&gt;:5555]\n                end\n            end\n\n            HAP_SVC[HAProxy Service&lt;br/&gt;LoadBalancer/NodePort&lt;br/&gt;:80 http&lt;br/&gt;:443 https]\n        end\n\n        subgraph \"Application Namespace\"\n            ING[Ingress Resources]\n            APPSVC[Service Resources]\n            PODS[Application Pods]\n        end\n\n        subgraph \"Monitoring Namespace\"\n            PROM[Prometheus&lt;br/&gt;Metrics Collection]\n            JAEGER[Jaeger&lt;br/&gt;Trace Collection]\n        end\n    end\n\n    USERS[External Users] --&gt; HAP_SVC\n    HAP_SVC --&gt; HAP1\n    HAP_SVC --&gt; HAP2\n    HAP_SVC --&gt; HAPN\n\n    CTRL_SVC --&gt; CTRL_POD\n    API --&gt; CTRL_POD\n    CM --&gt; CTRL_POD\n    ING -.Watch.-&gt; CTRL_POD\n    APPSVC -.Watch.-&gt; CTRL_POD\n\n    CTRL_POD --&gt; DP1\n    CTRL_POD --&gt; DP2\n    CTRL_POD --&gt; DPN\n\n    DP1 --&gt; HAP1\n    DP2 --&gt; HAP2\n    DPN --&gt; HAPN\n\n    HAP1 --&gt; PODS\n    HAP2 --&gt; PODS\n    HAPN --&gt; PODS\n\n    CTRL_SVC -.Metrics.-&gt; PROM\n    CTRL_SVC -.Health.-&gt; PROM\n\n    style CTRL_POD fill:#4CAF50\n    style HAP1 fill:#FF9800\n    style HAP2 fill:#FF9800\n    style HAPN fill:#FF9800\n    style CM fill:#2196F3</code></pre> <p>Deployment Components:</p> <ol> <li>Controller Deployment: Single replica deployment running the operator</li> <li>Watches Kubernetes resources cluster-wide</li> <li>Renders templates and validates configurations</li> <li>Deploys to HAProxy instances via Dataplane API</li> <li> <p>Exposes metrics and health endpoints via Controller Service</p> </li> <li> <p>Controller Service: ClusterIP service for operational endpoints</p> </li> <li>Port 8080: Health checks (liveness/readiness probes)</li> <li>Port 9090: Prometheus metrics</li> <li> <p>Internal use only (not exposed externally)</p> </li> <li> <p>HAProxy StatefulSet: Multiple replicas for high availability</p> </li> <li>Each pod runs HAProxy + Dataplane API sidecar</li> <li>Service selector targets HAProxy pods for traffic routing</li> <li> <p>Scales horizontally based on load</p> </li> <li> <p>HAProxy Service: LoadBalancer/NodePort service for ingress traffic</p> </li> <li>Port 80: HTTP traffic routing</li> <li>Port 443: HTTPS/TLS traffic routing</li> <li> <p>Exposes HAProxy pods externally for user traffic</p> </li> <li> <p>ConfigMap: Contains controller configuration</p> </li> <li>Template definitions (haproxy.cfg, maps, certificates)</li> <li>Watched resource types and indexing configuration</li> <li>Rendering and deployment settings</li> </ol>"},{"location":"development/design/deployment/#container-architecture","title":"Container Architecture","text":"<pre><code>graph TB\n    subgraph \"Controller Pod\"\n        CTRL_MAIN[Controller Process&lt;br/&gt;Port 8080: Health&lt;br/&gt;Port 9090: Metrics&lt;br/&gt;Port 9443: Webhook]\n\n        CTRL_VOL1[ConfigMap Volume&lt;br/&gt;/config]\n    end\n\n    subgraph \"HAProxy Pod (StatefulSet Member)\"\n        HAP_PROC[HAProxy Process&lt;br/&gt;Port 80: HTTP&lt;br/&gt;Port 443: HTTPS&lt;br/&gt;Port 8404: Stats]\n\n        DP_PROC[Dataplane API&lt;br/&gt;Port 5555: API&lt;br/&gt;Port 8080: Health]\n\n        HAP_VOL1[Config Volume&lt;br/&gt;/etc/haproxy]\n        HAP_VOL2[Maps Volume&lt;br/&gt;/etc/haproxy/maps]\n        HAP_VOL3[Certs Volume&lt;br/&gt;/etc/haproxy/certs]\n    end\n\n    CM_SRC[ConfigMap&lt;br/&gt;haproxy-config] --&gt; CTRL_VOL1\n    CTRL_VOL1 --&gt; CTRL_MAIN\n\n    DP_PROC -.API.-&gt; HAP_PROC\n    HAP_VOL1 --&gt; HAP_PROC\n    HAP_VOL2 --&gt; HAP_PROC\n    HAP_VOL3 --&gt; HAP_PROC\n\n    style CTRL_MAIN fill:#4CAF50\n    style HAP_PROC fill:#FF9800\n    style DP_PROC fill:#FFB74D</code></pre> <p>Resource Requirements:</p> <p>Controller Pod: - CPU: 100m request, 500m limit - Memory: 128Mi request, 512Mi limit - Volumes: ConfigMap mount for configuration</p> <p>HAProxy Pod: - HAProxy Container: 200m CPU, 256Mi memory (per instance) - Dataplane API Container: 100m CPU, 128Mi memory - Volumes: EmptyDir for dynamic configs, maps, and certificates</p>"},{"location":"development/design/deployment/#network-topology","title":"Network Topology","text":"<pre><code>graph LR\n    subgraph \"External Network\"\n        INET[Internet]\n    end\n\n    subgraph \"Kubernetes Cluster Network\"\n        HAP_LB[HAProxy Service&lt;br/&gt;LoadBalancer&lt;br/&gt;External IP]\n        CTRL_SVC_NET[Controller Service&lt;br/&gt;ClusterIP&lt;br/&gt;10.96.0.10]\n\n        subgraph \"Pod Network\"\n            subgraph \"Controller Pod&lt;br/&gt;10.0.0.10\"\n                CTRL[Controller Process&lt;br/&gt;:8080, :9090]\n            end\n\n            subgraph \"HAProxy Instances\"\n                subgraph \"haproxy-0 Pod&lt;br/&gt;10.0.1.10\"\n                    HAP1[HAProxy Process&lt;br/&gt;:80, :443, :8404]\n                    DP1[Dataplane API&lt;br/&gt;:5555]\n                end\n\n                subgraph \"haproxy-1 Pod&lt;br/&gt;10.0.1.11\"\n                    HAP2[HAProxy Process&lt;br/&gt;:80, :443, :8404]\n                    DP2[Dataplane API&lt;br/&gt;:5555]\n                end\n            end\n\n            subgraph \"Application Pods\"\n                APP1[app-pod-1&lt;br/&gt;10.0.2.10]\n                APP2[app-pod-2&lt;br/&gt;10.0.2.11]\n            end\n        end\n\n        KUBE_API[Kubernetes API&lt;br/&gt;443]\n        PROM_NET[Prometheus]\n    end\n\n    INET --&gt; HAP_LB\n    HAP_LB --&gt; HAP1\n    HAP_LB --&gt; HAP2\n\n    CTRL_SVC_NET --&gt; CTRL\n    PROM_NET --&gt; CTRL_SVC_NET\n\n    CTRL --&gt; KUBE_API\n    CTRL --&gt; DP1\n    CTRL --&gt; DP2\n\n    HAP1 --&gt; APP1\n    HAP1 --&gt; APP2\n    HAP2 --&gt; APP1\n    HAP2 --&gt; APP2\n\n    DP1 -.API.-&gt; HAP1\n    DP2 -.API.-&gt; HAP2\n\n    style CTRL fill:#4CAF50\n    style HAP1 fill:#FF9800\n    style HAP2 fill:#FF9800\n    style HAP_LB fill:#2196F3\n    style CTRL_SVC_NET fill:#4CAF50</code></pre> <p>Network Flow:</p> <ol> <li>Ingress Traffic: Internet \u2192 HAProxy Service (LoadBalancer) \u2192 HAProxy Pods \u2192 Application Pods</li> <li>Control Plane: Controller \u2192 Kubernetes API (resource watching)</li> <li>Configuration Deployment: Controller \u2192 Dataplane API endpoints (HTTP)</li> <li>Service Discovery: Controller watches HAProxy pods via Kubernetes API</li> <li>Monitoring: Prometheus \u2192 Controller Service (ClusterIP) \u2192 Controller Pod (metrics endpoint)</li> <li>Health Checks: Kubernetes \u2192 Controller Service \u2192 Controller Pod (healthz endpoint)</li> </ol> <p>Scaling Considerations:</p> <ul> <li>Horizontal Scaling: Increase HAProxy StatefulSet replicas for more capacity</li> <li>Controller Scaling: Single active controller (leader election for HA in future)</li> <li>Resource Limits: Adjust based on number of watched resources and template complexity</li> <li>Network: Ensure LoadBalancer can distribute traffic across all HAProxy replicas</li> </ul>"},{"location":"development/design/design-decisions/","title":"Design Decisions","text":""},{"location":"development/design/design-decisions/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"development/design/design-decisions/#configuration-validation-strategy","title":"Configuration Validation Strategy","text":"<p>Decision: Use two-phase validation (client-native parser + haproxy binary) instead of running a full validation sidecar.</p> <p>Rationale: - Performance: Parser validation is fast (~10ms), binary validation completes in ~50-100ms - Resource Efficiency: No additional HAProxy container needed (saves ~256Mi memory per controller pod) - Simplicity: Single container deployment reduces complexity - Reliability: Same validation guarantees as full HAProxy instance</p> <p>Implementation: <pre><code>// Validation is implemented in pkg/dataplane/validator.go\n// Uses real HAProxy directories with mutex locking to match Dataplane API behavior\n\n// ValidationPaths holds filesystem paths for validation\ntype ValidationPaths struct {\n    MapsDir           string  // e.g., /etc/haproxy/maps\n    SSLCertsDir       string  // e.g., /etc/haproxy/certs\n    GeneralStorageDir string  // e.g., /etc/haproxy/general\n    ConfigFile        string  // e.g., /etc/haproxy/haproxy.cfg\n}\n\nfunc ValidateConfiguration(mainConfig string, auxFiles *AuxiliaryFiles, paths ValidationPaths) error {\n    // Acquire mutex to ensure only one validation at a time\n    validationMutex.Lock()\n    defer validationMutex.Unlock()\n\n    // Phase 1: Syntax validation with client-native parser\n    if err := validateSyntax(mainConfig); err != nil {\n        return &amp;ValidationError{Phase: \"syntax\", Err: err}\n    }\n\n    // Phase 2: Semantic validation with haproxy binary\n    // Writes files to real HAProxy directories, then runs haproxy -c\n    if err := validateSemantics(mainConfig, auxFiles, paths); err != nil {\n        return &amp;ValidationError{Phase: \"semantic\", Err: err}\n    }\n\n    return nil\n}\n</code></pre></p> <p>Validation Paths Configuration:</p> <p>The validation paths must match the HAProxy Dataplane API server's resource configuration. These are configured via the controller's ConfigMap:</p> <pre><code>validation:\n  maps_dir: /etc/haproxy/maps\n  ssl_certs_dir: /etc/haproxy/certs\n  general_storage_dir: /etc/haproxy/general\n  config_file: /etc/haproxy/haproxy.cfg\n</code></pre> <p>The validator uses mutex locking to ensure only one validation runs at a time, preventing concurrent writes to the HAProxy directories. This approach exactly matches how the Dataplane API performs validation.</p> <p>Parser Improvements:</p> <p>The config parser (<code>pkg/dataplane/parser</code>) has been enhanced to correctly handle all HAProxy global directives:</p> <ol> <li>Fixed Log Target Parsing: Previously, the parser incorrectly treated <code>log-send-hostname</code> as a log target</li> <li>Now correctly identifies log targets: lines starting with \"log\" followed by an address</li> <li>Properly classifies <code>log-send-hostname</code> as a general global directive (not a log target)</li> <li> <p>Example valid config now parses correctly:      <pre><code>global\n    log stdout local0\n    log-send-hostname\n</code></pre></p> </li> <li> <p>Improved Directive Classification: Enhanced logic to distinguish between:</p> </li> <li>Log targets: <code>log &lt;address&gt; &lt;facility&gt; [level]</code></li> <li>Log options: <code>log-send-hostname</code>, <code>log-tag</code>, etc.</li> <li> <p>Other global directives</p> </li> <li> <p>Better Error Messages: Parser now provides clearer error messages when encountering unsupported directives</p> </li> </ol> <p>This fix resolves issues where valid HAProxy configurations were rejected during the parsing phase of validation.</p>"},{"location":"development/design/design-decisions/#template-engine-selection","title":"Template Engine Selection","text":"<p>Decision: Use a Jinja2-like template engine for Go with rich feature set.</p> <p>Candidates Evaluated: 1. gonja v2: Pure Go Jinja2 implementation, actively maintained (v2.4.1, January 2025) 2. pongo2: Django/Jinja2 syntax, comprehensive but last release 2022 3. text/template: Go stdlib, limited features, verbose syntax 4. jet: Fast, but different syntax paradigm</p> <p>Selected: gonja v2 (github.com/nikolalohinski/gonja/v2)</p> <p>Rationale: - Active Maintenance: Latest release v2.4.1 (January 2025), ongoing development - Jinja2 Compatibility: Aims for maximum compatibility with Python's Jinja2 engine - Familiarity: Jinja2 syntax is well-known in operations community - Features: Full feature set including filters, macros, template inheritance, control flow - Extensibility: Easy to add custom filters (b64decode, etc.) and context methods (pathResolver, etc.) - Pure Go: No external dependencies, requires Go 1.21+</p>"},{"location":"development/design/design-decisions/#kubernetes-client-architecture","title":"Kubernetes Client Architecture","text":"<p>Decision: Use client-go with SharedInformerFactory for resource watching, no heavy controller framework.</p> <p>Rationale: - Control: Direct control over informer lifecycle and event handling - Flexibility: Custom indexing logic without framework constraints - Performance: Optimized cache and index management - Simplicity: No code generation, no framework-imposed structure</p> <p>Implementation Pattern: <pre><code>// Slim approach using client-go\nfactory := informers.NewSharedInformerFactory(clientset, resyncPeriod)\n\n// Add informers for each watched resource type\nfor _, resource := range config.WatchedResources {\n    gvr := schema.GroupVersionResource{...}\n    informer := factory.ForResource(gvr)\n\n    informer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{\n        AddFunc:    handleAdd,\n        UpdateFunc: handleUpdate,\n        DeleteFunc: handleDelete,\n    })\n}\n\nfactory.Start(stopCh)\n</code></pre></p>"},{"location":"development/design/design-decisions/#concurrency-model","title":"Concurrency Model","text":"<p>Decision: Use Go routines and channels for async operations with structured concurrency.</p> <p>Key Patterns:</p> <ol> <li> <p>Event Processing: Buffered channels for event debouncing    <pre><code>type Debouncer struct {\n    events chan Event\n    timer  *time.Timer\n}\n</code></pre></p> </li> <li> <p>Parallel Deployment: Worker pools for deploying to multiple HAProxy instances    <pre><code>var wg sync.WaitGroup\nfor _, endpoint := range endpoints {\n    wg.Add(1)\n    go func(ep DataplaneEndpoint) {\n        defer wg.Done()\n        deploy(ep)\n    }(endpoint)\n}\nwg.Wait()\n</code></pre></p> </li> <li> <p>Context Propagation: All operations use context.Context for cancellation    <pre><code>func (s *Synchronizer) Deploy(ctx context.Context, config Config) error {\n    ctx, cancel := context.WithTimeout(ctx, 30*time.Second)\n    defer cancel()\n    // ... deployment logic\n}\n</code></pre></p> </li> </ol>"},{"location":"development/design/design-decisions/#observability-integration","title":"Observability Integration","text":"<p>Decision: Prometheus metrics + OpenTelemetry tracing with standardized naming.</p> <p>Metrics Implementation:</p> <p>The controller implements comprehensive Prometheus metrics through the event adapter pattern:</p> <p>Architecture: - <code>pkg/metrics</code>: Generic metrics infrastructure with instance-based registry - <code>pkg/controller/metrics</code>: Event adapter subscribing to controller lifecycle events - Metrics exposed on configurable port (default 9090) at <code>/metrics</code> endpoint</p> <p>Implementation: <pre><code>// Instance-based registry (not global)\nmetricsRegistry := prometheus.NewRegistry()\n\n// Create metrics component\nmetricsComponent := metrics.NewMetricsComponent(eventBus, metricsRegistry)\n\n// Start metrics collection (after EventBus.Start())\ngo metricsComponent.Start(ctx)\n\n// Start HTTP server\nmetricsServer := pkgmetrics.NewServer(\":9090\", metricsRegistry)\ngo metricsServer.Start(ctx)\n</code></pre></p> <p>Metrics Exposed (11 total):</p> <ol> <li>Reconciliation Metrics:</li> <li><code>haproxy_ic_reconciliation_total</code>: Counter for reconciliation cycles</li> <li><code>haproxy_ic_reconciliation_errors_total</code>: Counter for reconciliation failures</li> <li> <p><code>haproxy_ic_reconciliation_duration_seconds</code>: Histogram for reconciliation duration</p> </li> <li> <p>Deployment Metrics:</p> </li> <li><code>haproxy_ic_deployment_total</code>: Counter for deployments</li> <li><code>haproxy_ic_deployment_errors_total</code>: Counter for deployment failures</li> <li> <p><code>haproxy_ic_deployment_duration_seconds</code>: Histogram for deployment duration</p> </li> <li> <p>Validation Metrics:</p> </li> <li><code>haproxy_ic_validation_total</code>: Counter for validations</li> <li> <p><code>haproxy_ic_validation_errors_total</code>: Counter for validation failures</p> </li> <li> <p>Resource Metrics:</p> </li> <li> <p><code>haproxy_ic_resource_count</code>: Gauge vector with type labels (haproxy-pods, watched-resources)</p> </li> <li> <p>Event Bus Metrics:</p> </li> <li><code>haproxy_ic_event_subscribers</code>: Gauge for active subscribers</li> <li><code>haproxy_ic_events_published_total</code>: Counter for published events</li> </ol> <p>See <code>pkg/controller/metrics/README.md</code> for complete metric definitions and Prometheus queries.</p> <p>Tracing Integration: <pre><code>import \"go.opentelemetry.io/otel\"\n\nfunc (r *Renderer) Render(ctx context.Context, tpl string) (string, error) {\n    ctx, span := otel.Tracer(\"haproxy-template-ic\").Start(ctx, \"render_template\")\n    defer span.End()\n\n    span.SetAttributes(\n        attribute.Int(\"template_size\", len(tpl)),\n    )\n\n    // ... rendering logic\n\n    return result, nil\n}\n</code></pre></p>"},{"location":"development/design/design-decisions/#error-handling-strategy","title":"Error Handling Strategy","text":"<p>Decision: Structured errors with context using standard library errors package and custom error types.</p> <p>Pattern: <pre><code>// Custom error types for different failure modes\ntype ValidationError struct {\n    ConfigSize int\n    Line       int\n    Details    string\n    Err        error\n}\n\nfunc (e *ValidationError) Error() string {\n    return fmt.Sprintf(\"validation failed at line %d: %s\", e.Line, e.Details)\n}\n\nfunc (e *ValidationError) Unwrap() error {\n    return e.Err\n}\n\n// Usage with error wrapping\nfunc validate(config string) error {\n    if err := parser.Parse(config); err != nil {\n        return &amp;ValidationError{\n            ConfigSize: len(config),\n            Details:    \"syntax error\",\n            Err:        err,\n        }\n    }\n    return nil\n}\n</code></pre></p>"},{"location":"development/design/design-decisions/#event-driven-architecture","title":"Event-Driven Architecture","text":"<p>Decision: Implement event-driven architecture for component decoupling and extensibility.</p> <p>Rationale: - Decoupling: Components communicate via events, not direct calls - Extensibility: New features can be added without modifying existing code - Observability: Complete system visibility through event stream - Testability: Pure components without event dependencies - Maintainability: Clear separation between business logic and coordination</p> <p>Architecture Pattern:</p> <p>The architecture uses a pure libraries + event-driven components pattern:</p> <pre><code>Component Architecture:\n\u251c\u2500\u2500 Pure Libraries (business logic, no event dependencies)\n\u2502   \u251c\u2500\u2500 pkg/templating - Template engine with pre-compilation and rendering\n\u2502   \u251c\u2500\u2500 pkg/dataplane - HAProxy config validation, parsing, and deployment\n\u2502   \u251c\u2500\u2500 pkg/k8s - Resource watching, indexing, and storage\n\u2502   \u2514\u2500\u2500 pkg/core - Configuration types and basic validation\n\u2502\n\u2514\u2500\u2500 Event-Driven Components (event adapters wrapping pure libraries)\n    \u251c\u2500\u2500 pkg/controller/renderer - Subscribes to ReconciliationTriggeredEvent, calls pkg/templating\n    \u251c\u2500\u2500 pkg/controller/validator - Subscribes to TemplateRenderedEvent, calls pkg/dataplane validation\n    \u251c\u2500\u2500 pkg/controller/deployer - DeploymentScheduler and Deployer components calling pkg/dataplane\n    \u251c\u2500\u2500 pkg/controller/reconciler - Debounces resource changes and triggers reconciliation\n    \u2514\u2500\u2500 pkg/controller/executor - Observability component tracking reconciliation lifecycle\n</code></pre> <p>Key Distinction: - Pure Libraries: Testable business logic with no EventBus dependencies (pkg/templating, pkg/dataplane, pkg/k8s) - Event-Driven Components: Controllers that subscribe to events, call pure libraries, and publish result events (pkg/controller/*)</p> <p>Homegrown Event Bus Implementation:</p> <pre><code>// pkg/events/bus.go\npackage events\n\nimport \"sync\"\n\n// Event interface for type safety and immutability\n//\n// All event types MUST use pointer receivers for Event interface methods.\n// This avoids copying large structs (200+ bytes) and follows Go best practices.\n//\n// All event types MUST implement both methods:\n//   - EventType() returns the unique event type string\n//   - Timestamp() returns when the event was created\n//\n// Events are immutable after creation. Constructor functions perform defensive\n// copying of slices and maps to prevent post-publication mutation.\ntype Event interface {\n    EventType() string\n    Timestamp() time.Time\n}\n\n// EventBus provides pub/sub coordination with startup coordination.\n//\n// Startup Coordination:\n// Events published before Start() is called are buffered and replayed after Start().\n// This prevents race conditions during component initialization where events might\n// be published before all subscribers have connected.\ntype EventBus struct {\n    subscribers []chan Event\n    mu          sync.RWMutex\n\n    // Startup coordination\n    started        bool\n    startMu        sync.Mutex\n    preStartBuffer []Event\n}\n\n// NewEventBus creates a new EventBus.\n//\n// The bus starts in buffering mode - events published before Start() is called\n// will be buffered and replayed when Start() is invoked.\n//\n// The capacity parameter sets the initial buffer size for pre-start events.\n// Recommended: 100 for most applications.\nfunc NewEventBus(capacity int) *EventBus {\n    return &amp;EventBus{\n        subscribers:    make([]chan Event, 0),\n        started:        false,\n        preStartBuffer: make([]Event, 0, capacity),\n    }\n}\n\n// Publish sends event to all subscribers.\n//\n// If Start() has not been called yet, the event is buffered and will be\n// replayed when Start() is invoked. After Start() is called, this is a\n// non-blocking operation that drops events to lagging subscribers.\n//\n// Returns the number of subscribers that received the event.\n// Returns 0 if event was buffered (before Start()).\nfunc (b *EventBus) Publish(event Event) int {\n    // Check if bus has started\n    b.startMu.Lock()\n    if !b.started {\n        // Buffer event for replay after Start()\n        b.preStartBuffer = append(b.preStartBuffer, event)\n        b.startMu.Unlock()\n        return 0\n    }\n    b.startMu.Unlock()\n\n    // Bus has started - publish to subscribers\n    b.mu.RLock()\n    defer b.mu.RUnlock()\n\n    sent := 0\n    for _, ch := range b.subscribers {\n        select {\n        case ch &lt;- event:\n            sent++\n        default:\n            // Channel full, subscriber lagging - drop event\n        }\n    }\n    return sent\n}\n\n// Subscribe creates new event channel\nfunc (b *EventBus) Subscribe(bufferSize int) &lt;-chan Event {\n    b.mu.Lock()\n    defer b.mu.Unlock()\n\n    ch := make(chan Event, bufferSize)\n    b.subscribers = append(b.subscribers, ch)\n    return ch\n}\n\n// Start releases all buffered events and switches the bus to normal operation mode.\n//\n// This method should be called after all components have subscribed to the bus\n// during application startup. It ensures that no events are lost during the\n// initialization phase.\n//\n// Behavior:\n//  1. Marks the bus as started\n//  2. Replays all buffered events to subscribers in order\n//  3. Clears the buffer\n//  4. All subsequent Publish() calls go directly to subscribers\n//\n// This method is idempotent - calling it multiple times has no additional effect.\nfunc (b *EventBus) Start() {\n    b.startMu.Lock()\n    defer b.startMu.Unlock()\n\n    // Idempotent - return if already started\n    if b.started {\n        return\n    }\n\n    // Mark as started (must be done before replaying to avoid recursion)\n    b.started = true\n\n    // Replay buffered events to subscribers\n    if len(b.preStartBuffer) &gt; 0 {\n        b.mu.RLock()\n        subscribers := b.subscribers\n        b.mu.RUnlock()\n\n        for _, event := range b.preStartBuffer {\n            // Publish each buffered event\n            for _, ch := range subscribers {\n                select {\n                case ch &lt;- event:\n                    // Event sent\n                default:\n                    // Channel full - drop event (same behavior as normal Publish)\n                }\n            }\n        }\n\n        // Clear buffer\n        b.preStartBuffer = nil\n    }\n}\n</code></pre> <p>Event Type Definitions:</p> <pre><code>// pkg/events/types.go\npackage events\n\n// Event categories covering complete controller lifecycle\n//\n// All events use pointer receivers and include private timestamp fields.\n// Constructor functions (New*Event) perform defensive copying of slices/maps.\n\n// Lifecycle Events\ntype ControllerStartedEvent struct {\n    ConfigVersion  string\n    SecretVersion  string\n    timestamp      time.Time\n}\n\nfunc NewControllerStartedEvent(configVersion, secretVersion string) *ControllerStartedEvent {\n    return &amp;ControllerStartedEvent{\n        ConfigVersion: configVersion,\n        SecretVersion: secretVersion,\n        timestamp:     time.Now(),\n    }\n}\n\nfunc (e *ControllerStartedEvent) EventType() string    { return \"controller.started\" }\nfunc (e *ControllerStartedEvent) Timestamp() time.Time { return e.timestamp }\n\ntype ControllerShutdownEvent struct {\n    Reason    string\n    timestamp time.Time\n}\n\nfunc NewControllerShutdownEvent(reason string) *ControllerShutdownEvent {\n    return &amp;ControllerShutdownEvent{\n        Reason:    reason,\n        timestamp: time.Now(),\n    }\n}\n\nfunc (e *ControllerShutdownEvent) EventType() string    { return \"controller.shutdown\" }\nfunc (e *ControllerShutdownEvent) Timestamp() time.Time { return e.timestamp }\n\n// Configuration Events\ntype ConfigParsedEvent struct {\n    Config        interface{}\n    Version       string\n    SecretVersion string\n    timestamp     time.Time\n}\n\nfunc NewConfigParsedEvent(config interface{}, version, secretVersion string) *ConfigParsedEvent {\n    return &amp;ConfigParsedEvent{\n        Config:        config,\n        Version:       version,\n        SecretVersion: secretVersion,\n        timestamp:     time.Now(),\n    }\n}\n\nfunc (e *ConfigParsedEvent) EventType() string    { return \"config.parsed\" }\nfunc (e *ConfigParsedEvent) Timestamp() time.Time { return e.timestamp }\n\ntype ConfigValidatedEvent struct {\n    Config        interface{}\n    Version       string\n    SecretVersion string\n    timestamp     time.Time\n}\n\nfunc NewConfigValidatedEvent(config interface{}, version, secretVersion string) *ConfigValidatedEvent {\n    return &amp;ConfigValidatedEvent{\n        Config:        config,\n        Version:       version,\n        SecretVersion: secretVersion,\n        timestamp:     time.Now(),\n    }\n}\n\nfunc (e *ConfigValidatedEvent) EventType() string    { return \"config.validated\" }\nfunc (e *ConfigValidatedEvent) Timestamp() time.Time { return e.timestamp }\n\ntype ConfigInvalidEvent struct {\n    Version          string\n    ValidationErrors map[string][]string // validator name -&gt; errors\n    timestamp        time.Time\n}\n\n// NewConfigInvalidEvent creates a new ConfigInvalidEvent with defensive copying\nfunc NewConfigInvalidEvent(version string, validationErrors map[string][]string) *ConfigInvalidEvent {\n    // Defensive copy of map with slice values\n    errorsCopy := make(map[string][]string, len(validationErrors))\n    for k, v := range validationErrors {\n        if len(v) &gt; 0 {\n            vCopy := make([]string, len(v))\n            copy(vCopy, v)\n            errorsCopy[k] = vCopy\n        }\n    }\n\n    return &amp;ConfigInvalidEvent{\n        Version:          version,\n        ValidationErrors: errorsCopy,\n        timestamp:        time.Now(),\n    }\n}\n\nfunc (e *ConfigInvalidEvent) EventType() string    { return \"config.invalid\" }\nfunc (e *ConfigInvalidEvent) Timestamp() time.Time { return e.timestamp }\n\n// Resource Events\ntype ResourceIndexUpdatedEvent struct {\n    // ResourceTypeName identifies the resource type from config (e.g., \"ingresses\", \"services\").\n    ResourceTypeName string\n\n    // ChangeStats provides detailed change statistics including Created, Modified, Deleted counts\n    // and whether this event occurred during initial sync.\n    ChangeStats types.ChangeStats\n\n    timestamp time.Time\n}\n\nfunc NewResourceIndexUpdatedEvent(resourceTypeName string, changeStats types.ChangeStats) *ResourceIndexUpdatedEvent {\n    return &amp;ResourceIndexUpdatedEvent{\n        ResourceTypeName: resourceTypeName,\n        ChangeStats:      changeStats,\n        timestamp:        time.Now(),\n    }\n}\n\nfunc (e *ResourceIndexUpdatedEvent) EventType() string    { return \"resource.index.updated\" }\nfunc (e *ResourceIndexUpdatedEvent) Timestamp() time.Time { return e.timestamp }\n\ntype ResourceSyncCompleteEvent struct {\n    // ResourceTypeName identifies the resource type from config (e.g., \"ingresses\").\n    ResourceTypeName string\n\n    // InitialCount is the number of resources loaded during initial sync.\n    InitialCount int\n\n    timestamp time.Time\n}\n\nfunc NewResourceSyncCompleteEvent(resourceTypeName string, initialCount int) *ResourceSyncCompleteEvent {\n    return &amp;ResourceSyncCompleteEvent{\n        ResourceTypeName: resourceTypeName,\n        InitialCount:     initialCount,\n        timestamp:        time.Now(),\n    }\n}\n\nfunc (e *ResourceSyncCompleteEvent) EventType() string    { return \"resource.sync.complete\" }\nfunc (e *ResourceSyncCompleteEvent) Timestamp() time.Time { return e.timestamp }\n\ntype IndexSynchronizedEvent struct {\n    // ResourceCounts maps resource types to their counts.\n    ResourceCounts map[string]int\n    timestamp      time.Time\n}\n\n// NewIndexSynchronizedEvent creates a new IndexSynchronizedEvent with defensive copying\nfunc NewIndexSynchronizedEvent(resourceCounts map[string]int) *IndexSynchronizedEvent {\n    // Defensive copy of map\n    countsCopy := make(map[string]int, len(resourceCounts))\n    for k, v := range resourceCounts {\n        countsCopy[k] = v\n    }\n\n    return &amp;IndexSynchronizedEvent{\n        ResourceCounts: countsCopy,\n        timestamp:      time.Now(),\n    }\n}\n\nfunc (e *IndexSynchronizedEvent) EventType() string    { return \"index.synchronized\" }\nfunc (e *IndexSynchronizedEvent) Timestamp() time.Time { return e.timestamp }\n\n// Reconciliation Events\ntype ReconciliationTriggeredEvent struct {\n    Reason    string\n    timestamp time.Time\n}\n\nfunc NewReconciliationTriggeredEvent(reason string) *ReconciliationTriggeredEvent {\n    return &amp;ReconciliationTriggeredEvent{\n        Reason:    reason,\n        timestamp: time.Now(),\n    }\n}\n\nfunc (e *ReconciliationTriggeredEvent) EventType() string    { return \"reconciliation.triggered\" }\nfunc (e *ReconciliationTriggeredEvent) Timestamp() time.Time { return e.timestamp }\n\ntype ReconciliationStartedEvent struct {\n    Trigger   string\n    timestamp time.Time\n}\n\nfunc NewReconciliationStartedEvent(trigger string) *ReconciliationStartedEvent {\n    return &amp;ReconciliationStartedEvent{\n        Trigger:   trigger,\n        timestamp: time.Now(),\n    }\n}\n\nfunc (e *ReconciliationStartedEvent) EventType() string    { return \"reconciliation.started\" }\nfunc (e *ReconciliationStartedEvent) Timestamp() time.Time { return e.timestamp }\n\ntype ReconciliationCompletedEvent struct {\n    DurationMs int64\n    timestamp  time.Time\n}\n\nfunc NewReconciliationCompletedEvent(durationMs int64) *ReconciliationCompletedEvent {\n    return &amp;ReconciliationCompletedEvent{\n        DurationMs: durationMs,\n        timestamp:  time.Now(),\n    }\n}\n\nfunc (e *ReconciliationCompletedEvent) EventType() string    { return \"reconciliation.completed\" }\nfunc (e *ReconciliationCompletedEvent) Timestamp() time.Time { return e.timestamp }\n\ntype ReconciliationFailedEvent struct {\n    Error     string\n    timestamp time.Time\n}\n\nfunc NewReconciliationFailedEvent(err string) *ReconciliationFailedEvent {\n    return &amp;ReconciliationFailedEvent{\n        Error:     err,\n        timestamp: time.Now(),\n    }\n}\n\nfunc (e *ReconciliationFailedEvent) EventType() string    { return \"reconciliation.failed\" }\nfunc (e *ReconciliationFailedEvent) Timestamp() time.Time { return e.timestamp }\n\n// Note: All ~50 event types follow the same pattern:\n// - Pointer receivers for EventType() and Timestamp() methods\n// - Private timestamp field set in constructor\n// - Constructor function (New*Event) that performs defensive copying\n// - Exported fields for event data\n//\n// Additional event categories (not shown for brevity):\n// - Template Events (TemplateRenderedEvent, TemplateRenderFailedEvent)\n// - Validation Events (ValidationStartedEvent, ValidationCompletedEvent, ValidationFailedEvent)\n// - Deployment Events (DeploymentStartedEvent, InstanceDeployedEvent, DeploymentCompletedEvent)\n// - Storage Events (StorageSyncStartedEvent, StorageSyncCompletedEvent)\n// - HAProxy Discovery Events (HAProxyPodsDiscoveredEvent)\n//\n// See pkg/controller/events/types.go for complete event catalog.\n</code></pre> <p>Event Immutability Contract:</p> <p>Events in the system are designed to be immutable after creation, representing historical facts about what happened. The implementation balances practical immutability with Go idioms and performance:</p> <ol> <li>Pointer Receivers: All Event interface methods use pointer receivers</li> <li>Avoids copying large structs (many events exceed 200 bytes)</li> <li>Follows Go best practices for methods on types with mutable fields</li> <li> <p>Enforced by custom <code>eventimmutability</code> linter in <code>tools/linters/</code></p> </li> <li> <p>Exported Fields: Event fields are exported for idiomatic Go access</p> </li> <li>Follows industry standards (Kubernetes, NATS)</li> <li>Enables JSON serialization without reflection tricks</li> <li> <p>Relies on team discipline rather than compiler enforcement</p> </li> <li> <p>Defensive Copying: Constructors perform defensive copies of slices and maps</p> </li> <li>Publishers cannot modify events after creation</li> <li>Example: <code>NewConfigInvalidEvent</code> deep-copies the validation errors map</li> <li> <p>Prevents accidental mutation from affecting published events</p> </li> <li> <p>Read-Only Discipline: Consumers must treat events as read-only</p> </li> <li>Enforced through code review and team practices</li> <li>This is an internal project where all consumers are controlled</li> <li> <p>Alternative (unexported fields + getters) would be less idiomatic Go</p> </li> <li> <p>Custom Linter: The <code>eventimmutability</code> analyzer enforces pointer receivers</p> </li> <li>Integrated into <code>make lint</code> and CI pipeline</li> <li>Prevents value receivers that would cause struct copying</li> <li>Located in <code>tools/linters/eventimmutability/</code></li> </ol> <p>This approach provides practical immutability while maintaining clean, idiomatic Go code without the overhead of getters or complex accessor patterns.</p> <p>Component with Event Adapter Pattern:</p> <pre><code>// pkg/dataplane/client.go - Pure component (no event knowledge)\ntype DataplaneClient struct {\n    endpoints []DataplaneEndpoint\n}\n\nfunc (c *DataplaneClient) DeployConfig(ctx context.Context, config string) error {\n    // Pure business logic\n    // No event publishing here\n    return c.deploy(ctx, config)\n}\n\n// pkg/dataplane/adapter.go - Event adapter wrapping pure component\ntype DataplaneEventAdapter struct {\n    client   *DataplaneClient\n    eventBus *EventBus\n}\n\nfunc (a *DataplaneEventAdapter) DeployConfig(ctx context.Context, config string) error {\n    // Publish start event\n    a.eventBus.Publish(DeploymentStartedEvent{\n        Endpoints: a.client.endpoints,\n    })\n\n    // Call pure component\n    err := a.client.DeployConfig(ctx, config)\n\n    // Publish result event\n    if err != nil {\n        a.eventBus.Publish(DeploymentFailedEvent{Error: err.Error()})\n        return err\n    }\n\n    a.eventBus.Publish(DeploymentCompletedEvent{\n        Total: len(a.client.endpoints),\n        Succeeded: len(a.client.endpoints),\n        Failed: 0,\n    })\n\n    return nil\n}\n</code></pre> <p>Staged Startup with Event Coordination:</p> <pre><code>// cmd/controller/main.go\nfunc main() {\n    ctx := context.Background()\n\n    // Create event bus\n    eventBus := events.NewEventBus(1000)\n\n    // Stage 1: Config Management Components\n    log.Info(\"Stage 1: Starting config management\")\n\n    configWatcher := NewConfigWatcher(client, eventBus)\n    configLoader := NewConfigLoader(eventBus)\n    configValidator := NewConfigValidator(eventBus)\n\n    go configWatcher.Run(ctx)\n    go configLoader.Run(ctx)\n    go configValidator.Run(ctx)\n\n    // Start the event bus - ensures all components have subscribed before events flow\n    // This prevents race conditions where events are published before subscribers connect\n    eventBus.Start()\n\n    // Stage 2: Wait for Valid Config\n    log.Info(\"Stage 2: Waiting for valid configuration\")\n\n    events := eventBus.Subscribe(100)\n    var config Config\n\n    for {\n        select {\n        case event := &lt;-events:\n            switch e := event.(type) {\n            case ConfigValidatedEvent:\n                config = e.Config\n                log.Info(\"Valid config received\")\n                goto ConfigReady\n            case ControllerShutdownEvent:\n                return\n            }\n        case &lt;-ctx.Done():\n            return\n        }\n    }\n\nConfigReady:\n    eventBus.Publish(ControllerStartedEvent{\n        ConfigVersion: config.Version,\n    })\n\n    // Stage 3: Resource Watchers\n    log.Info(\"Stage 3: Starting resource watchers\")\n\n    stores := make(map[string]*ResourceStore)\n    resourceWatcher := NewResourceWatcher(client, eventBus, config.WatchedResources, stores)\n\n    go resourceWatcher.Run(ctx)\n\n    indexTracker := NewIndexSynchronizationTracker(eventBus, config.WatchedResources)\n    go indexTracker.Run(ctx)\n\n    // Stage 4: Wait for Index Sync\n    log.Info(\"Stage 4: Waiting for resource sync\")\n\n    for {\n        select {\n        case event := &lt;-events:\n            if _, ok := event.(IndexSynchronizedEvent); ok {\n                goto IndexReady\n            }\n        case &lt;-time.After(30 * time.Second):\n            log.Fatal(\"Index sync timeout\")\n        }\n    }\n\nIndexReady:\n\n    // Stage 5: Reconciliation Components\n    log.Info(\"Stage 5: Starting reconciliation\")\n\n    reconciler := NewReconciliationComponent(eventBus)\n    executor := NewReconciliationExecutor(eventBus, config, stores)\n\n    go reconciler.Run(ctx)\n    go executor.Run(ctx)\n\n    log.Info(\"All components started\")\n\n    // Wait for shutdown\n    &lt;-ctx.Done()\n}\n</code></pre> <p>Event Multiplexing with <code>select</code>:</p> <pre><code>// pkg/controller/reconciler.go\ntype ReconciliationComponent struct {\n    eventBus *EventBus\n    debouncer *time.Timer\n}\n\nfunc (r *ReconciliationComponent) Run(ctx context.Context) error {\n    events := r.eventBus.Subscribe(100)\n\n    for {\n        select {\n        case event := &lt;-events:\n            switch e := event.(type) {\n            case ResourceIndexUpdatedEvent:\n                // Resource changed, trigger reconciliation after quiet period\n                r.debounce()\n\n            case ConfigValidatedEvent:\n                // Config changed, trigger immediate reconciliation\n                r.triggerImmediately()\n\n            case ReconciliationCompletedEvent:\n                log.Info(\"Reconciliation completed\", \"duration_ms\", e.DurationMs)\n            }\n\n        case &lt;-r.debouncer.C:\n            // Quiet period expired, trigger reconciliation\n            r.eventBus.Publish(ReconciliationTriggeredEvent{\n                Reason: \"debounce_timer\",\n            })\n\n        case &lt;-ctx.Done():\n            return ctx.Err()\n        }\n    }\n}\n</code></pre> <p>Graceful Shutdown with Context:</p> <pre><code>// pkg/controller/runner.go\nfunc (r *OperatorRunner) Run(ctx context.Context) error {\n    eventBus := events.NewEventBus(1000)\n\n    // Create cancellable context for components\n    compCtx, cancel := context.WithCancel(ctx)\n    defer cancel()\n\n    // Start components\n    g, gCtx := errgroup.WithContext(compCtx)\n\n    g.Go(func() error { return configWatcher.Run(gCtx) })\n    g.Go(func() error { return resourceWatcher.Run(gCtx) })\n    g.Go(func() error { return reconciler.Run(gCtx) })\n\n    // Wait for shutdown signal or component error\n    select {\n    case &lt;-ctx.Done():\n        log.Info(\"Shutdown signal received\")\n        eventBus.Publish(ControllerShutdownEvent{})\n\n    case &lt;-gCtx.Done():\n        log.Error(\"Component error\", \"err\", gCtx.Err())\n    }\n\n    // Cancel all components and wait\n    cancel()\n    return g.Wait()\n}\n</code></pre> <p>Benefits of This Approach:</p> <ol> <li>Pure Components: Business logic has no event dependencies, easily testable</li> <li>Single Event Layer: Only adapters know about events, components remain clean</li> <li>Observability: Complete system state visible through event stream</li> <li>Extensibility: New features subscribe to existing events, no code modification</li> <li>Debugging: Event log provides complete audit trail</li> <li>Idiomatic Go: Uses channels, select, context - native Go patterns</li> </ol>"},{"location":"development/design/design-decisions/#request-response-pattern-scatter-gather","title":"Request-Response Pattern (Scatter-Gather)","text":"<p>Problem: Configuration validation requires synchronous coordination across multiple validators (template syntax, JSONPath expressions, structural validation). Using async pub/sub with manual timeout management would add complexity and be error-prone.</p> <p>Solution: Implement the scatter-gather pattern alongside async pub/sub for cases requiring coordinated responses.</p> <p>Pattern: Scatter-Gather (from Enterprise Integration Patterns)</p> <p>The scatter-gather pattern broadcasts a request to multiple recipients and aggregates responses:</p> <ol> <li>Scatter Phase: Broadcast request event to all subscribers</li> <li>Gather Phase: Collect response events correlated by RequestID</li> <li>Aggregation: Wait for all (or minimum) expected responses or timeout</li> </ol> <p>When to Use: - Configuration Validation: Multiple validators must approve before config becomes active - Distributed Queries: Need responses from multiple components before proceeding - Coordinated Operations: Any scenario requiring confirmation from multiple parties</p> <p>When NOT to Use: - Fire-and-Forget Notifications: Use async pub/sub instead - Observability Events: Use async pub/sub instead - Single Response: Use direct function call instead</p> <p>Implementation:</p> <p>The EventBus provides both patterns:</p> <pre><code>// pkg/events/bus.go - Extended EventBus\n\n// Async pub/sub (existing)\nfunc (b *EventBus) Publish(event Event) int\nfunc (b *EventBus) Subscribe(bufferSize int) &lt;-chan Event\n\n// Sync request-response (new)\nfunc (b *EventBus) Request(ctx context.Context, request Request, opts RequestOptions) (*RequestResult, error)\n</code></pre> <p>Request/Response Interfaces:</p> <pre><code>// Request interface for scatter-gather\ntype Request interface {\n    Event\n    RequestID() string  // Unique ID for correlating responses\n}\n\n// Response interface\ntype Response interface {\n    Event\n    RequestID() string  // Links back to request\n    Responder() string  // Who sent this response\n}\n\n// RequestOptions configures scatter-gather behavior\ntype RequestOptions struct {\n    Timeout            time.Duration  // Max wait time\n    ExpectedResponders []string       // Who should respond\n    MinResponses       int            // Minimum required (for graceful degradation)\n}\n\n// RequestResult aggregates all responses\ntype RequestResult struct {\n    Responses []Response  // All responses received\n    Errors    []string    // Missing/timeout responders\n}\n</code></pre> <p>Config Validation Flow with Scatter-Gather:</p> <pre><code>sequenceDiagram\n    participant CW as ConfigWatcher\n    participant EB as EventBus\n    participant VC as ValidationCoordinator\n    participant BV as BasicValidator&lt;br/&gt;(Pure Function)\n    participant TV as TemplateValidator&lt;br/&gt;(Pure Function)\n    participant JV as JSONPathValidator&lt;br/&gt;(Pure Function)\n\n    CW-&gt;&gt;EB: Publish(ConfigParsedEvent)\n\n    Note over VC: Receives parsed config\n\n    VC-&gt;&gt;EB: Request(ConfigValidationRequest)\n\n    Note over EB: Scatter phase - broadcast request\n\n    EB-&gt;&gt;BV: ConfigValidationRequest\n    EB-&gt;&gt;TV: ConfigValidationRequest\n    EB-&gt;&gt;JV: ConfigValidationRequest\n\n    Note over BV,JV: Validators process in parallel\n\n    BV-&gt;&gt;EB: Publish(ConfigValidationResponse)\n    TV-&gt;&gt;EB: Publish(ConfigValidationResponse)\n    JV-&gt;&gt;EB: Publish(ConfigValidationResponse)\n\n    Note over EB: Gather phase - collect responses\n\n    EB--&gt;&gt;VC: RequestResult (all responses)\n\n    alt All Valid\n        VC-&gt;&gt;EB: Publish(ConfigValidatedEvent)\n    else Any Invalid\n        VC-&gt;&gt;EB: Publish(ConfigInvalidEvent)\n    end</code></pre> <p>Pure Packages + Event Adapters:</p> <p>All business logic packages remain event-agnostic. Only the <code>controller</code> package contains event adapters:</p> <pre><code>pkg/templating/\n  validator.go         # Pure: ValidateTemplates(templates map[string]string) []error\n\npkg/k8s/indexer/\n  validator.go         # Pure: ValidateJSONPath(expr string) error\n\npkg/core/config/\n  validator.go         # Pure: ValidateStructure(cfg Config) error  // OK - same package\n\npkg/controller/validators/    # Event adapters (glue layer)\n  template_validator.go      # Adapter: extracts primitives \u2192 templating.ValidateTemplates() \u2192 events\n  jsonpath_validator.go      # Adapter: extracts strings \u2192 indexer.ValidateJSONPath() \u2192 events\n  basic_validator.go         # Adapter: events \u2192 config.ValidateStructure() \u2192 events\n  coordinator.go             # Uses scatter-gather to coordinate validators\n</code></pre> <p>Pure Function Example:</p> <pre><code>// pkg/templating/validator.go - Zero dependencies on other packages\npackage templating\n\n// ValidateTemplates validates a map of template names to their content.\n// Accepts only primitive types - no dependency on config package.\nfunc ValidateTemplates(templates map[string]string) []error {\n    var errors []error\n    engine, err := NewTemplateEngine(EngineTypeGonja)\n    if err != nil {\n        return []error{err}\n    }\n\n    for name, content := range templates {\n        if err := engine.CompileTemplate(name, content); err != nil {\n            errors = append(errors, fmt.Errorf(\"template %s: %w\", name, err))\n        }\n    }\n\n    return errors\n}\n</code></pre> <p>Event Adapter Example:</p> <pre><code>// pkg/controller/validators/template_validator.go - Event adapter\npackage validators\n\nimport (\n    \"github.com/yourorg/haproxy-template-ic/pkg/core/config\"\n    \"github.com/yourorg/haproxy-template-ic/pkg/events\"\n    \"github.com/yourorg/haproxy-template-ic/pkg/templating\"\n)\n\ntype TemplateValidatorComponent struct {\n    eventBus *events.EventBus\n}\n\nfunc (c *TemplateValidatorComponent) Run(ctx context.Context) error {\n    eventChan := c.eventBus.Subscribe(100)\n\n    for {\n        select {\n        case event := &lt;-eventChan:\n            if req, ok := event.(events.ConfigValidationRequest); ok {\n                // Controller package knows about config structure\n                cfg := req.Config.(config.Config)\n\n                // Extract templates into map[string]string (primitive types only)\n                // This is the controller's job - converting between package types\n                templates := make(map[string]string)\n                templates[\"haproxy.cfg\"] = cfg.HAProxyConfig.Template\n                for name, snippet := range cfg.TemplateSnippets {\n                    templates[name] = snippet.Template\n                }\n                for name, mapDef := range cfg.Maps {\n                    templates[\"map:\"+name] = mapDef.Template\n                }\n                for name, file := range cfg.Files {\n                    templates[\"file:\"+name] = file.Template\n                }\n\n                // Call pure function with primitives only (no config package dependency)\n                errs := templating.ValidateTemplates(templates)\n\n                // Convert to event response\n                errStrings := make([]string, len(errs))\n                for i, e := range errs {\n                    errStrings[i] = e.Error()\n                }\n\n                // Publish response event\n                c.eventBus.Publish(events.NewConfigValidationResponse(\n                    req.RequestID(),\n                    \"template\",\n                    len(errs) == 0,\n                    errStrings,\n                ))\n            }\n\n        case &lt;-ctx.Done():\n            return ctx.Err()\n        }\n    }\n}\n</code></pre> <p>Validation Coordinator with Scatter-Gather:</p> <pre><code>// pkg/controller/validators/coordinator.go\npackage validators\n\ntype ValidationCoordinator struct {\n    eventBus *events.EventBus\n}\n\nfunc (v *ValidationCoordinator) Run(ctx context.Context) error {\n    eventChan := v.eventBus.Subscribe(100)\n\n    for {\n        select {\n        case event := &lt;-eventChan:\n            if parsed, ok := event.(events.ConfigParsedEvent); ok {\n                // Create validation request\n                req := events.NewConfigValidationRequest(parsed.Config, parsed.Version)\n\n                // Use scatter-gather to coordinate validators\n                result, err := v.eventBus.Request(ctx, req, events.RequestOptions{\n                    Timeout:            10 * time.Second,\n                    ExpectedResponders: []string{\"basic\", \"template\", \"jsonpath\"},\n                })\n\n                if err != nil || len(result.Errors) &gt; 0 {\n                    // Validation failed or timeout\n                    errorMap := make(map[string][]string)\n\n                    for _, resp := range result.Responses {\n                        if validResp, ok := resp.(events.ConfigValidationResponse); ok &amp;&amp; !validResp.Valid {\n                            errorMap[validResp.ValidatorName] = validResp.Errors\n                        }\n                    }\n\n                    for _, errMsg := range result.Errors {\n                        errorMap[\"timeout\"] = append(errorMap[\"timeout\"], errMsg)\n                    }\n\n                    v.eventBus.Publish(events.ConfigInvalidEvent{\n                        Version:          parsed.Version,\n                        ValidationErrors: errorMap,\n                    })\n                    continue\n                }\n\n                // All validators passed\n                v.eventBus.Publish(events.ConfigValidatedEvent{\n                    Config:  parsed.Config,\n                    Version: parsed.Version,\n                })\n            }\n\n        case &lt;-ctx.Done():\n            return ctx.Err()\n        }\n    }\n}\n</code></pre> <p>Validator Logging Improvements:</p> <p>The validation coordinator implements enhanced logging to provide visibility into the scatter-gather validation process:</p> <ol> <li>Structured Logging: Uses <code>log/slog</code> with structured fields for queryability</li> <li>Validator names, response counts, validation error counts</li> <li>Duration tracking for performance monitoring</li> <li> <p>Clear distinction between validation failure (expected) and system errors</p> </li> <li> <p>Appropriate Log Levels:</p> </li> <li><code>warn</code> level for validation failures (not <code>error</code>) - invalid config is an expected condition</li> <li><code>info</code> level for successful validation with validator details</li> <li> <p><code>error</code> level reserved for actual system failures (timeouts, missing validators)</p> </li> <li> <p>Detailed Error Aggregation:</p> </li> <li>Groups validation errors by validator name</li> <li>Shows which validators responded and their individual results</li> <li> <p>Provides actionable error messages for config authors</p> </li> <li> <p>Observability: Full visibility into validation workflow</p> </li> <li>Which validators participated in validation</li> <li>How long validation took</li> <li>Exactly which aspects of config failed validation</li> </ol> <p>Example log output for validation failure: <pre><code>level=warn msg=\"configuration validation failed\"\n  version=\"abc123\"\n  validators_responded=[\"basic\",\"template\",\"jsonpath\"]\n  validators_failed=[\"template\",\"jsonpath\"]\n  error_count=3\n  validation_errors={\"template\":[\"syntax error at line 5\"],\"jsonpath\":[\"invalid expression: .foo[bar\"]}\n</code></pre></p> <p>Benefits:</p> <ol> <li>No Manual Timeout Management: EventBus handles timeout and response correlation</li> <li>Pure Packages: Business logic has zero event dependencies</li> <li>Clean Separation: Only controller package contains event glue code</li> <li>Standard Pattern: Scatter-gather is well-documented in Enterprise Integration Patterns</li> <li>Flexible: Can require all responses or gracefully degrade with partial responses</li> <li>Testable: Pure functions easily tested without event infrastructure</li> </ol> <p>Comparison to Alternatives:</p> Approach Pros Cons Scatter-Gather Standard pattern, built-in timeout, clean code Slight overhead vs direct calls Manual Aggregator More control Complex timeout logic, error-prone Direct Function Calls Simplest, fastest Creates package dependencies Channels Go-native No event stream visibility <p>Selected: Scatter-gather provides the best balance of simplicity, observability, and maintainability.</p>"},{"location":"development/design/design-decisions/#event-commentator-pattern","title":"Event Commentator Pattern","text":"<p>Decision: Implement a dedicated Event Commentator component that subscribes to all EventBus events and produces domain-aware log messages with contextual insights.</p> <p>Problem: Traditional logging approaches have several limitations: - Debug log statements clutter business logic code - Logs lack cross-event context and domain knowledge - Difficult to correlate related events across the system - Business logic becomes less readable with extensive logging - Hard to produce insightful \"commentary\" without duplicating domain knowledge</p> <p>Solution: The Event Commentator Pattern - a specialized component that acts like a sports commentator for the system: - Subscribes to all events on the EventBus - Maintains a ring buffer of recent events for correlation - Produces rich, domain-aware log messages with contextual insights - Completely decoupled from business logic (pure components remain clean) - Lives in the controller package (only layer that knows about events)</p> <p>Concept Origins:</p> <p>The Event Commentator pattern combines several established patterns:</p> <ol> <li>Domain-Oriented Observability (Martin Fowler, 2024): The \"Domain Probe\" pattern where event-based monitors apply deep domain knowledge to produce insights about system behavior</li> <li>Cross-Cutting Concerns via Events: Logging as a concern separated from business logic through event-driven architecture</li> <li>Process Manager Pattern: State tracking and event correlation from saga patterns, adapted for observability</li> </ol> <p>Architecture Pattern:</p> <pre><code>Event Flow with Commentator:\n\nEventBus (50+ event types)\n    \u2502\n    \u251c\u2500&gt; Pure Components (business logic, no logging clutter)\n    \u2502   \u251c\u2500\u2500 ConfigLoader\n    \u2502   \u251c\u2500\u2500 TemplateRenderer\n    \u2502   \u251c\u2500\u2500 Validator\n    \u2502   \u2514\u2500\u2500 Deployer\n    \u2502\n    \u2514\u2500&gt; Event Commentator (observability layer)\n        \u251c\u2500\u2500 Subscribes to ALL events\n        \u251c\u2500\u2500 Ring Buffer (last N events for correlation)\n        \u251c\u2500\u2500 Domain Knowledge (understands relationships)\n        \u2514\u2500\u2500 Rich Logging (insights, not just data dumps)\n</code></pre> <p>Implementation:</p> <pre><code>// pkg/controller/commentator/commentator.go\npackage commentator\n\nimport (\n    \"context\"\n    \"log/slog\"\n    \"time\"\n\n    \"haproxy-template-ic/pkg/events\"\n)\n\n// EventCommentator subscribes to all events and produces domain-aware log messages\ntype EventCommentator struct {\n    eventBus   *events.EventBus\n    logger     *slog.Logger\n    ringBuffer *RingBuffer\n}\n\nfunc NewEventCommentator(eventBus *events.EventBus, logger *slog.Logger, bufferSize int) *EventCommentator {\n    return &amp;EventCommentator{\n        eventBus:   eventBus,\n        logger:     logger,\n        ringBuffer: NewRingBuffer(bufferSize),\n    }\n}\n\nfunc (c *EventCommentator) Run(ctx context.Context) error {\n    events := c.eventBus.Subscribe(1000)\n\n    for {\n        select {\n        case event := &lt;-events:\n            c.ringBuffer.Add(event)\n            c.commentate(ctx, event)\n        case &lt;-ctx.Done():\n            return ctx.Err()\n        }\n    }\n}\n\nfunc (c *EventCommentator) commentate(ctx context.Context, event events.Event) {\n    switch e := event.(type) {\n    case events.ReconciliationStartedEvent:\n        // Apply domain knowledge: Find what triggered this reconciliation\n        trigger := c.ringBuffer.FindMostRecent(func(ev events.Event) bool {\n            _, isConfig := ev.(events.ConfigValidatedEvent)\n            _, isResource := ev.(events.ResourceIndexUpdatedEvent)\n            return isConfig || isResource\n        })\n\n        var triggerType string\n        var debounceMs int64\n        if trigger != nil {\n            triggerType = trigger.EventType()\n            debounceMs = time.Since(trigger.Timestamp()).Milliseconds()\n        }\n\n        c.logger.InfoContext(ctx, \"Reconciliation started\",\n            \"trigger_event\", triggerType,\n            \"debounce_duration_ms\", debounceMs,\n            \"trigger_source\", e.Trigger,\n        )\n\n    case events.ValidationCompletedEvent:\n        // Count recent validation attempts for insight\n        recentValidations := c.ringBuffer.CountRecent(1*time.Minute, func(ev events.Event) bool {\n            _, ok := ev.(events.ValidationStartedEvent)\n            return ok\n        })\n\n        c.logger.InfoContext(ctx, \"Configuration validated successfully\",\n            \"endpoints\", len(e.Endpoints),\n            \"warnings\", len(e.Warnings),\n            \"recent_validations\", recentValidations,\n        )\n\n        if len(e.Warnings) &gt; 0 {\n            for _, warning := range e.Warnings {\n                c.logger.WarnContext(ctx, \"Validation warning\",\n                    \"warning\", warning,\n                )\n            }\n        }\n\n    case events.DeploymentCompletedEvent:\n        // Correlate with reconciliation start for end-to-end timing\n        reconStart := c.ringBuffer.FindMostRecent(func(ev events.Event) bool {\n            _, ok := ev.(events.ReconciliationStartedEvent)\n            return ok\n        })\n\n        var totalDurationMs int64\n        if reconStart != nil {\n            totalDurationMs = time.Since(reconStart.Timestamp()).Milliseconds()\n        }\n\n        successRate := float64(e.Succeeded) / float64(e.Total) * 100\n\n        c.logger.InfoContext(ctx, \"Deployment completed\",\n            \"total_instances\", e.Total,\n            \"succeeded\", e.Succeeded,\n            \"failed\", e.Failed,\n            \"success_rate_percent\", successRate,\n            \"total_duration_ms\", totalDurationMs,\n        )\n\n        if e.Failed &gt; 0 {\n            // Look for failed instance events\n            failures := c.ringBuffer.FindAll(func(ev events.Event) bool {\n                failEv, ok := ev.(events.InstanceDeploymentFailedEvent)\n                return ok &amp;&amp; failEv.Timestamp().After(reconStart.Timestamp())\n            })\n\n            c.logger.ErrorContext(ctx, \"Deployment had failures\",\n                \"failed_count\", e.Failed,\n                \"failure_details\", failures,\n            )\n        }\n\n    case events.ConfigInvalidEvent:\n        // Apply domain knowledge about configuration validation\n        c.logger.ErrorContext(ctx, \"Configuration validation failed\",\n            \"version\", e.Version,\n            \"error\", e.Error,\n        )\n\n        // Suggest what might be wrong based on recent activity\n        recentConfigChanges := c.ringBuffer.CountRecent(5*time.Minute, func(ev events.Event) bool {\n            _, ok := ev.(events.ConfigParsedEvent)\n            return ok\n        })\n\n        if recentConfigChanges &gt; 3 {\n            c.logger.WarnContext(ctx, \"Multiple recent config changes detected - consider reviewing all recent changes\",\n                \"recent_changes_count\", recentConfigChanges,\n            )\n        }\n\n    case events.ResourceIndexUpdatedEvent:\n        c.logger.DebugContext(ctx, \"Resource index updated\",\n            \"resource_type\", e.ResourceType,\n            \"count\", e.Count,\n        )\n\n    case events.HAProxyPodsDiscoveredEvent:\n        // Domain insight: Significant event worthy of INFO level\n        previousDiscovery := c.ringBuffer.FindMostRecent(func(ev events.Event) bool {\n            _, ok := ev.(events.HAProxyPodsDiscoveredEvent)\n            return ok\n        })\n\n        var change string\n        if previousDiscovery != nil {\n            prevEv := previousDiscovery.(events.HAProxyPodsDiscoveredEvent)\n            if len(e.Endpoints) &gt; len(prevEv.Endpoints) {\n                change = \"scaled_up\"\n            } else if len(e.Endpoints) &lt; len(prevEv.Endpoints) {\n                change = \"scaled_down\"\n            } else {\n                change = \"endpoints_changed\"\n            }\n        } else {\n            change = \"initial_discovery\"\n        }\n\n        c.logger.InfoContext(ctx, \"HAProxy pods discovered\",\n            \"endpoint_count\", len(e.Endpoints),\n            \"change_type\", change,\n        )\n    }\n}\n</code></pre> <p>Ring Buffer for Event Correlation:</p> <pre><code>// pkg/controller/commentator/ringbuffer.go\npackage commentator\n\nimport (\n    \"sync\"\n    \"time\"\n\n    \"haproxy-template-ic/pkg/events\"\n)\n\n// EventWithTimestamp wraps an event with its occurrence time\ntype EventWithTimestamp struct {\n    Event     events.Event\n    Timestamp time.Time\n}\n\n// RingBuffer maintains a circular buffer of recent events for correlation\ntype RingBuffer struct {\n    events []EventWithTimestamp\n    size   int\n    index  int\n    mu     sync.RWMutex\n}\n\nfunc NewRingBuffer(size int) *RingBuffer {\n    return &amp;RingBuffer{\n        events: make([]EventWithTimestamp, size),\n        size:   size,\n    }\n}\n\n// Add inserts an event into the ring buffer\nfunc (rb *RingBuffer) Add(event events.Event) {\n    rb.mu.Lock()\n    defer rb.mu.Unlock()\n\n    rb.events[rb.index] = EventWithTimestamp{\n        Event:     event,\n        Timestamp: time.Now(),\n    }\n    rb.index = (rb.index + 1) % rb.size\n}\n\n// FindMostRecent searches backwards from newest to oldest for an event matching the predicate\nfunc (rb *RingBuffer) FindMostRecent(predicate func(events.Event) bool) *EventWithTimestamp {\n    rb.mu.RLock()\n    defer rb.mu.RUnlock()\n\n    // Search backwards from most recent\n    for i := 0; i &lt; rb.size; i++ {\n        idx := (rb.index - 1 - i + rb.size) % rb.size\n        evt := rb.events[idx]\n        if evt.Event != nil &amp;&amp; predicate(evt.Event) {\n            return &amp;evt\n        }\n    }\n    return nil\n}\n\n// FindAll returns all events matching the predicate (newest first)\nfunc (rb *RingBuffer) FindAll(predicate func(events.Event) bool) []EventWithTimestamp {\n    rb.mu.RLock()\n    defer rb.mu.RUnlock()\n\n    var matches []EventWithTimestamp\n    for i := 0; i &lt; rb.size; i++ {\n        idx := (rb.index - 1 - i + rb.size) % rb.size\n        evt := rb.events[idx]\n        if evt.Event != nil &amp;&amp; predicate(evt.Event) {\n            matches = append(matches, evt)\n        }\n    }\n    return matches\n}\n\n// CountRecent counts events matching predicate within the time window\nfunc (rb *RingBuffer) CountRecent(duration time.Duration, predicate func(events.Event) bool) int {\n    rb.mu.RLock()\n    defer rb.mu.RUnlock()\n\n    cutoff := time.Now().Add(-duration)\n    count := 0\n\n    for i := 0; i &lt; rb.size; i++ {\n        idx := (rb.index - 1 - i + rb.size) % rb.size\n        evt := rb.events[idx]\n        if evt.Event == nil || evt.Timestamp.Before(cutoff) {\n            break\n        }\n        if predicate(evt.Event) {\n            count++\n        }\n    }\n\n    return count\n}\n</code></pre> <p>Integration:</p> <pre><code>// cmd/controller/main.go - Startup integration\nfunc main() {\n    // ... create eventBus ...\n\n    // Create logger from pkg/core/logging\n    logger := logging.NewLogger(config.Logging.Verbose)\n\n    // Start event commentator early (Stage 1)\n    commentator := commentator.NewEventCommentator(eventBus, logger, 100)\n    go commentator.Run(ctx)\n\n    // ... start other components ...\n}\n</code></pre> <p>Benefits:</p> <ol> <li>Clean Business Logic: Pure components remain focused on business logic without logging clutter</li> <li>Rich Context: Commentator applies domain knowledge to produce insightful messages</li> <li>Event Correlation: Ring buffer enables relating events (e.g., \"this deployment was triggered by that config change 234ms ago\")</li> <li>Centralized Observability: Single place to manage logging strategy and message formatting</li> <li>Extensibility: Easy to add commentary for new event types without touching business logic</li> <li>Performance: Logging is completely asynchronous, no performance impact on business logic</li> <li>Maintainability: Domain knowledge about event relationships lives in one place</li> </ol> <p>Configuration:</p> <p>The event commentator can be configured via controller configuration:</p> <pre><code>controller:\n  event_commentator:\n    enabled: true\n    buffer_size: 100  # Number of events to keep for correlation\n    log_level: info   # Minimum level for commentator (debug shows all events)\n</code></pre> <p>Example Log Output:</p> <pre><code>INFO  Reconciliation started trigger_event=resource.index.updated debounce_duration_ms=234 trigger_source=ingress\nINFO  Configuration validated successfully endpoints=3 warnings=0 recent_validations=1\nINFO  Deployment completed total_instances=3 succeeded=3 failed=0 success_rate_percent=100 total_duration_ms=456\nDEBUG Resource index updated resource_type=ingresses count=42\nINFO  HAProxy pods discovered endpoint_count=3 change_type=initial_discovery\n</code></pre> <p>Notice how the commentator provides context that would be impossible with traditional logging: - \"triggered by resource.index.updated 234ms ago\" (correlation) - \"recent_validations=1\" (trend analysis) - \"total_duration_ms=456\" (end-to-end timing across multiple events) - \"change_type=initial_discovery\" (domain knowledge about pod lifecycle)</p> <p>Comparison to Traditional Logging:</p> Aspect Traditional Logging Event Commentator Code Location Scattered throughout business logic Centralized in commentator Context Limited to current function Full event history via ring buffer Domain Knowledge Duplicated across components Centralized in one place Maintainability Hard to update log messages Single location to update Testability Logging clutters unit tests Pure components easily tested Performance Synchronous, impacts business logic Fully asynchronous via events Correlation Manual via correlation IDs Automatic via event relationships <p>Selected: The Event Commentator pattern provides superior observability while keeping business logic clean and maintaining the architectural principle of event-agnostic pure components.</p>"},{"location":"development/design/introspection/","title":"Introspection","text":""},{"location":"development/design/introspection/#runtime-introspection-and-debugging","title":"Runtime Introspection and Debugging","text":"<p>The controller provides comprehensive runtime introspection capabilities through an HTTP debug server, enabling production debugging, operational visibility, and acceptance testing without relying solely on logs.</p>"},{"location":"development/design/introspection/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    subgraph \"Controller Process\"\n        EB[EventBus]\n        SC[StateCache&lt;br/&gt;Event-Driven State Tracking]\n        EVB[EventBuffer&lt;br/&gt;Ring Buffer]\n\n        subgraph \"Debug Infrastructure\"\n            REG[Introspection Registry]\n            HTTP[HTTP Debug Server&lt;br/&gt;Configurable Port]\n\n            VARS[Debug Variables]\n            CONFIG[ConfigVar]\n            CREDS[CredentialsVar]\n            REND[RenderedVar]\n            RES[ResourcesVar]\n            EVENTS[EventsVar]\n            STATE[StateVar]\n        end\n    end\n\n    EB --&gt;|Subscribe| SC\n    EB --&gt;|Subscribe| EVB\n    SC --&gt;|Implements| SP[StateProvider]\n    SP --&gt;|Used by| VARS\n    EVB --&gt;|Events History| EVENTS\n\n    VARS --&gt; CONFIG\n    VARS --&gt; CREDS\n    VARS --&gt; REND\n    VARS --&gt; RES\n    VARS --&gt; EVENTS\n    VARS --&gt; STATE\n\n    CONFIG --&gt; REG\n    CREDS --&gt; REG\n    REND --&gt; REG\n    RES --&gt; REG\n    EVENTS --&gt; REG\n    STATE --&gt; REG\n\n    REG --&gt; HTTP\n\n    EXT[External Clients&lt;br/&gt;Tests, Debug Tools] --&gt;|HTTP| HTTP\n\n    style HTTP fill:#4CAF50\n    style SC fill:#2196F3\n    style EVB fill:#FF9800\n    style REG fill:#9C27B0</code></pre>"},{"location":"development/design/introspection/#key-components","title":"Key Components","text":"<p>pkg/introspection - Generic debug HTTP server infrastructure: - Instance-based variable registry (not global like expvar) - HTTP handlers for <code>/debug/vars</code> endpoints - JSONPath field selection support (kubectl-style syntax) - Go profiling integration (<code>/debug/pprof</code>) - Graceful shutdown with context</p> <p>pkg/events/ringbuffer - Event history storage: - Thread-safe circular buffer using Go generics - Fixed-size with automatic old-item eviction - O(1) add, O(n) retrieval performance - Used by both EventCommentator and EventBuffer</p> <p>pkg/controller/debug - Controller-specific debug variables: - Implements <code>introspection.Var</code> interface for controller data - ConfigVar, CredentialsVar (metadata only), RenderedVar, ResourcesVar - EventBuffer for independent event tracking - StateProvider interface for accessing controller state</p> <p>StateCache - Event-driven state tracking: - Subscribes to validation, rendering, and resource events - Maintains current state snapshot in memory - Thread-safe RWMutex-protected access - Implements StateProvider interface for debug endpoints - Prevents need to query EventBus for historical state</p>"},{"location":"development/design/introspection/#http-endpoints","title":"HTTP Endpoints","text":"<p>The debug server exposes controller state via HTTP (port configurable via <code>--debug-port</code> flag or <code>DEBUG_PORT</code> environment variable, disabled by default):</p> <pre><code># List all available variables (assuming port 6060 is configured)\ncurl http://localhost:6060/debug/vars\n\n# Get current configuration\ncurl http://localhost:6060/debug/vars/config\n\n# Get just the config version using JSONPath\ncurl 'http://localhost:6060/debug/vars/config?field={.version}'\n\n# Get rendered HAProxy configuration\ncurl http://localhost:6060/debug/vars/rendered\n\n# Get resource counts\ncurl http://localhost:6060/debug/vars/resources\n\n# Get recent events (last 1000)\ncurl http://localhost:6060/debug/vars/events\n\n# Get recent 100 events\ncurl 'http://localhost:6060/debug/vars/events?field={.last_100}'\n\n# Get complete state dump\ncurl http://localhost:6060/debug/vars/state\n\n# Go profiling\ncurl http://localhost:6060/debug/pprof/\ncurl http://localhost:6060/debug/pprof/heap\ncurl http://localhost:6060/debug/pprof/goroutine\n</code></pre>"},{"location":"development/design/introspection/#event-history","title":"Event History","text":"<p>Two independent event tracking mechanisms:</p> <p>EventCommentator (observability): - Subscribes to all events for domain-aware logging - Ring buffer for event correlation in log messages - Produces rich contextual log output - Lives in pkg/controller/commentator</p> <p>EventBuffer (debugging): - Subscribes to all events for debug endpoint access - Simplified event representation for HTTP API - Exposes last N events via <code>/debug/vars/events</code> - Lives in pkg/controller/debug</p> <p>This separation allows different buffer sizes, retention policies, and use cases without coupling logging to debugging infrastructure.</p>"},{"location":"development/design/introspection/#integration-with-acceptance-testing","title":"Integration with Acceptance Testing","text":"<p>The debug endpoints enable powerful acceptance testing:</p> <pre><code>// tests/acceptance/debug_client.go\ntype DebugClient struct {\n    podName   string\n    debugPort int\n}\n\n// In test\nfunc TestConfigMapReload(t *testing.T) {\n    // Create debug client with port-forward\n    debugClient := NewDebugClient(cfg.RESTConfig(), \"controller-pod\", 6060)\n    debugClient.Start(ctx)\n\n    // Update ConfigMap\n    UpdateConfigMap(ctx, \"new-template\")\n\n    // Wait for controller to process change\n    err := debugClient.WaitForConfigVersion(ctx, \"v2\", 30*time.Second)\n    require.NoError(t, err)\n\n    // Verify rendered config includes changes\n    rendered, err := debugClient.GetRenderedConfig(ctx)\n    require.NoError(t, err)\n    assert.Contains(t, rendered, \"expected-content\")\n\n    // Verify event history\n    events, err := debugClient.GetEvents(ctx)\n    require.NoError(t, err)\n    assert.Contains(t, events, \"config.validated\")\n}\n</code></pre> <p>This enables true end-to-end testing without parsing logs or relying on timing heuristics.</p>"},{"location":"development/design/introspection/#security-considerations","title":"Security Considerations","text":"<p>Debug variables implement careful filtering:</p> <pre><code>// CredentialsVar returns metadata only\nfunc (v *CredentialsVar) Get() (interface{}, error) {\n    creds, version, err := v.provider.GetCredentials()\n    if err != nil {\n        return nil, err\n    }\n\n    return map[string]interface{}{\n        \"version\":             version,\n        \"has_dataplane_creds\": creds.DataplanePassword != \"\",\n        // NEVER expose actual passwords\n    }, nil\n}\n</code></pre> <p>The debug server should be: - Bound to localhost in production (kubectl port-forward for access) - Protected by network policies - Disabled or restricted in multi-tenant environments</p>"},{"location":"development/design/introspection/#configuration","title":"Configuration","text":"<p>Debug server configuration via controller ConfigMap:</p> <pre><code>controller:\n  introspection:\n    enabled: true\n    port: 6060\n    bind_address: \"0.0.0.0\"  # For kubectl port-forward compatibility\n\n  debug:\n    enabled: true\n    event_buffer_size: 1000\n    state_snapshots: true\n</code></pre> <p>For detailed implementation and API documentation, see: - <code>pkg/introspection/README.md</code> - Generic debug HTTP server - <code>pkg/events/ringbuffer/README.md</code> - Ring buffer implementation - <code>pkg/controller/debug/README.md</code> - Controller-specific debug variables</p>"},{"location":"development/design/leader-election-implementation-status/","title":"Leader Election Implementation Status","text":""},{"location":"development/design/leader-election-implementation-status/#completed-tasks","title":"Completed Tasks \u2705","text":""},{"location":"development/design/leader-election-implementation-status/#1-design-documentation","title":"1. Design Documentation","text":"<ul> <li>\u2705 Created comprehensive design document (<code>docs/development/design/leader-election.md</code>)</li> <li>\u2705 Documented problem statement, architecture, and implementation plan</li> <li>\u2705 Included failure scenarios, testing strategy, and migration path</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#2-leader-election-infrastructure-package","title":"2. Leader Election Infrastructure Package","text":"<ul> <li>\u2705 Created <code>pkg/controller/leaderelection/</code> package</li> <li>\u2705 Implemented <code>Config</code> type with validation (<code>config.go</code>)</li> <li>\u2705 Defined error types (<code>errors.go</code>)</li> <li>\u2705 Implemented <code>LeaderElector</code> component (<code>elector.go</code>)</li> <li>Wraps client-go leaderelection</li> <li>Integrates with EventBus</li> <li>Tracks leadership state atomically</li> <li>Supports callbacks for transitions</li> <li>Handles single-replica mode (disabled election)</li> <li>\u2705 Created README.md with API documentation</li> <li>\u2705 Created CLAUDE.md with development context</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#3-leader-election-events","title":"3. Leader Election Events","text":"<ul> <li>\u2705 Added event type constants to <code>pkg/controller/events/types.go</code>:</li> <li><code>EventTypeLeaderElectionStarted</code></li> <li><code>EventTypeBecameLeader</code></li> <li><code>EventTypeLostLeadership</code></li> <li><code>EventTypeNewLeaderObserved</code></li> <li>\u2705 Implemented event structs with constructors:</li> <li><code>LeaderElectionStartedEvent</code></li> <li><code>BecameLeaderEvent</code></li> <li><code>LostLeadershipEvent</code></li> <li><code>NewLeaderObservedEvent</code></li> </ul>"},{"location":"development/design/leader-election-implementation-status/#4-configuration-schema","title":"4. Configuration Schema","text":"<ul> <li>\u2705 Added <code>LeaderElectionConfig</code> to <code>pkg/core/config/types.go</code></li> <li>\u2705 Added default constants to <code>pkg/core/config/defaults.go</code></li> <li>\u2705 Implemented helper methods (GetLeaseDuration, etc.)</li> <li>\u2705 Integrated into <code>ControllerConfig</code> struct</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#5-controller-startup-integration","title":"5. Controller Startup Integration \u2705","text":"<ul> <li>\u2705 Modified <code>pkg/controller/controller.go</code>:</li> <li>Read POD_NAME and POD_NAMESPACE from environment</li> <li>Create LeaderElector early in startup (Stage 0)</li> <li>Define callbacks for leadership transitions</li> <li>Start leader election loop in background goroutine</li> <li>Conditionally start deployment components based on leadership</li> <li>Handle leadership changes during runtime</li> <li>\u2705 Created <code>leaderOnlyComponents</code> struct for lifecycle management</li> <li>\u2705 Refactored component startup to separate all-replica vs leader-only</li> <li>\u2705 Implemented mutex-protected callback handlers for thread-safe transitions</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#6-metrics","title":"6. Metrics \u2705","text":"<ul> <li>\u2705 Added metrics to <code>pkg/controller/metrics/metrics.go</code>:</li> <li><code>haproxy_ic_leader_election_is_leader</code> (gauge)</li> <li><code>haproxy_ic_leader_election_transitions_total</code> (counter)</li> <li><code>haproxy_ic_leader_election_time_as_leader_seconds_total</code> (counter)</li> <li>\u2705 Updated <code>pkg/controller/metrics/component.go</code> to collect leader election events</li> <li>\u2705 Added automatic time tracking (starts on BecameLeader, records on LostLeadership)</li> <li>\u2705 Subscribed to leader election events in metrics component</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#7-commentator","title":"7. Commentator \u2705","text":"<ul> <li>\u2705 Updated <code>pkg/controller/commentator/commentator.go</code>:</li> <li>Added case for <code>LeaderElectionStartedEvent</code></li> <li>Added case for <code>BecameLeaderEvent</code> (with \ud83c\udf96\ufe0f emoji)</li> <li>Added case for <code>LostLeadershipEvent</code> (with \u26a0\ufe0f emoji)</li> <li>Added case for <code>NewLeaderObservedEvent</code></li> <li>\u2705 Added rich contextual logging for each event</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#8-rbac-manifests","title":"8. RBAC Manifests \u2705","text":"<ul> <li>\u2705 Updated <code>charts/haproxy-template-ic/templates/clusterrole.yaml</code>:</li> <li>Added coordination.k8s.io/v1 Lease permissions</li> <li>Verbs: get, create, update</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#9-deployment-manifests","title":"9. Deployment Manifests \u2705","text":"<ul> <li>\u2705 Updated <code>charts/haproxy-template-ic/templates/deployment.yaml</code>:</li> <li>Added POD_NAME environment variable (downward API)</li> <li>Added POD_NAMESPACE environment variable (downward API)</li> <li>Changed replicas from 1 to 2 (for HA by default)</li> <li>\u2705 Updated <code>charts/haproxy-template-ic/values.yaml</code>:</li> <li>Added leader election configuration section with defaults</li> <li>Documented leader election parameters (lease duration, renew deadline, retry period)</li> <li>Set replicaCount default to 2</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#10-unit-tests","title":"10. Unit Tests \u2705","text":"<ul> <li>\u2705 Created <code>pkg/controller/leaderelection/config_test.go</code></li> <li>Tested configuration validation (13 scenarios)</li> <li>Tested default config generation</li> <li>Tested all validation error types</li> <li>\u2705 Created <code>pkg/controller/leaderelection/elector_test.go</code></li> <li>Tested IsLeader() accuracy</li> <li>Tested event publishing in disabled mode</li> <li>Tested callback invocation</li> <li>Tested transition counting</li> <li>Tested time-as-leader tracking</li> <li>All 9 tests passing</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#remaining-tasks","title":"Remaining Tasks \ud83d\udea7","text":""},{"location":"development/design/leader-election-implementation-status/#11-integration-tests","title":"11. Integration Tests","text":"<ul> <li>\u23f3 Create <code>tests/integration/leader_election_test.go</code>:</li> <li>TestLeaderElection_OnlyLeaderDeploys</li> <li>TestLeaderElection_Failover</li> <li>TestLeaderElection_BothReplicasWatchResources</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#12-documentation-updates","title":"12. Documentation Updates","text":"<ul> <li>\u23f3 Update <code>docs/deployment/README.md</code> with HA setup instructions</li> <li>\u23f3 Create <code>docs/operations/high-availability.md</code></li> <li>\u23f3 Update <code>docs/operations/troubleshooting.md</code> with leader election debugging</li> <li>\u23f3 Update <code>README.md</code> to mention HA support</li> <li>\u23f3 Update <code>CHANGELOG.md</code> with new feature</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#dependencies","title":"Dependencies","text":"<p>The implementation requires: - <code>k8s.io/client-go/tools/leaderelection</code> (already in go.mod) - <code>k8s.io/api/coordination/v1</code> (for Lease resources, already in go.mod)</p> <p>No new external dependencies needed.</p>"},{"location":"development/design/leader-election-implementation-status/#testing-plan","title":"Testing Plan","text":""},{"location":"development/design/leader-election-implementation-status/#manual-testing-steps","title":"Manual Testing Steps","text":"<ol> <li> <p>Deploy with single replica (leader election disabled):    <pre><code>controller:\n  leader_election:\n    enabled: false\nreplicas: 1\n</code></pre>    Verify: Controller works as before</p> </li> <li> <p>Deploy with multiple replicas (leader election enabled):    <pre><code>controller:\n  leader_election:\n    enabled: true\nreplicas: 3\n</code></pre>    Verify: Only one replica deploys configs</p> </li> <li> <p>Kill leader pod:    <pre><code>kubectl delete pod &lt;leader-pod&gt;\n</code></pre>    Verify: Follower becomes leader within 20 seconds</p> </li> <li> <p>Check lease status:    <pre><code>kubectl get lease -n &lt;namespace&gt; haproxy-template-ic-leader -o yaml\n</code></pre>    Verify: Holder identity matches current leader</p> </li> <li> <p>Check metrics:    <pre><code>kubectl port-forward deployment/haproxy-template-ic 9090:9090\ncurl http://localhost:9090/metrics | grep controller_is_leader\n</code></pre>    Verify: Only one pod reports is_leader=1</p> </li> </ol>"},{"location":"development/design/leader-election-implementation-status/#integration-test-plan","title":"Integration Test Plan","text":"<p>Run integration tests with kind cluster: <pre><code># Start test cluster with 3 controller replicas\nmake test-integration-leader-election\n\n# Verify tests pass:\n# - Only leader deploys\n# - Failover works\n# - All replicas watch resources\n</code></pre></p>"},{"location":"development/design/leader-election-implementation-status/#rollout-strategy","title":"Rollout Strategy","text":""},{"location":"development/design/leader-election-implementation-status/#phase-1-opt-in-v020","title":"Phase 1: Opt-in (v0.2.0)","text":"<ul> <li>Release with <code>enabled: false</code> default</li> <li>Document how to enable for HA</li> <li>Collect feedback from early adopters</li> <li>Monitor for issues</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#phase-2-enabled-by-default-v030","title":"Phase 2: Enabled by Default (v0.3.0)","text":"<ul> <li>Change default to <code>enabled: true</code></li> <li>Update documentation</li> <li>Provide migration guide for single-replica users</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#phase-3-deprecate-single-replica-v100","title":"Phase 3: Deprecate Single Replica (v1.0.0)","text":"<ul> <li>Remove <code>enabled</code> flag</li> <li>Always use leader election</li> <li>Multi-replica is standard deployment</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#known-limitations","title":"Known Limitations","text":"<ol> <li>Lease expiry time: ~15-20 second downtime during failover</li> <li>Clock skew sensitivity: Requires synchronized node clocks</li> <li>Split-brain prevention: Relies on Kubernetes API availability</li> <li>No priority-based selection: Random selection among followers</li> </ol>"},{"location":"development/design/leader-election-implementation-status/#next-steps","title":"Next Steps","text":"<ol> <li>Complete controller startup integration (Task #5)</li> <li>Add metrics (Task #6)</li> <li>Update commentator (Task #7)</li> <li>Update RBAC and deployment manifests (Tasks #8-9)</li> <li>Write tests (Tasks #10-11)</li> <li>Update documentation (Task #12)</li> <li>Manual testing with kind cluster</li> <li>Code review and refinement</li> <li>Release v0.2.0 with opt-in HA support</li> </ol>"},{"location":"development/design/leader-election-implementation-status/#implementation-summary","title":"Implementation Summary","text":""},{"location":"development/design/leader-election-implementation-status/#what-was-completed","title":"What Was Completed","text":"<p>Core Infrastructure (100% complete): - Leader election package with Config, LeaderElector, comprehensive validation - Event types for all leadership transitions - Configuration schema with YAML support and defaults - Full controller integration with conditional component startup - Mutex-protected lifecycle management for thread-safe transitions</p> <p>Observability (100% complete): - 3 new Prometheus metrics (leader status, transitions, time as leader) - Automatic time tracking with event-driven updates - Rich contextual logging with emojis for visibility - All leader election events integrated into commentator</p> <p>Deployment (100% complete): - RBAC permissions for Lease resources - POD_NAME/POD_NAMESPACE via downward API - Helm chart configured with sensible defaults - Default replica count set to 2 for HA</p> <p>Testing (Unit tests complete, integration tests pending): - 9 comprehensive unit tests covering all scenarios - Config validation, disabled mode, state tracking, events - All tests passing</p>"},{"location":"development/design/leader-election-implementation-status/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Component Separation: Leader-only components (Deployer, DeploymentScheduler, DriftMonitor) vs all-replica components (Reconciler, Renderer, Validator, Executor, Discovery)</li> <li>Disabled Mode: Backward compatible single-replica mode with consistent code paths</li> <li>Graceful Failover: ~15-20 second downtime during leadership transitions</li> <li>Lease-based: Using coordination.k8s.io/v1 Lease resources for lower overhead</li> </ul>"},{"location":"development/design/leader-election-implementation-status/#next-steps_1","title":"Next Steps","text":"<ol> <li>Write integration tests (Task #11)</li> <li>Update documentation (Task #12)</li> <li>Manual testing with kind cluster</li> <li>Code review and refinement</li> <li>Release with HA support</li> </ol>"},{"location":"development/design/leader-election-implementation-status/#questions-for-review","title":"Questions for Review","text":"<ul> <li>\u2705 Should leader election be enabled by default in first release? YES - Now enabled by default with 2 replicas</li> <li>Should we add health check endpoint showing leadership status? Consider for future</li> <li>Do we need metrics for lease renewal failures? client-go handles this internally, current metrics sufficient</li> <li>Should we add alerts for frequent leadership transitions? Yes - document in operations guide</li> </ul>"},{"location":"development/design/leader-election/","title":"Leader Election for High Availability","text":""},{"location":"development/design/leader-election/#overview","title":"Overview","text":"<p>This document describes the leader election system for the HAProxy Template Ingress Controller, which enables running multiple controller replicas for high availability while preventing conflicting updates to HAProxy instances.</p>"},{"location":"development/design/leader-election/#problem-statement","title":"Problem Statement","text":"<p>The controller currently runs as a single instance. Running multiple replicas without coordination would cause:</p> <ol> <li>Resource waste: Multiple replicas performing identical dataplane API calls</li> <li>Potential conflicts: Race conditions when multiple controllers push updates simultaneously</li> <li>Unnecessary HAProxy reloads: Multiple deployments of the same configuration</li> </ol> <p>However, all replicas should: - Watch Kubernetes resources (to maintain hot cache for failover) - Render templates (to have configurations ready) - Validate configurations (to share the workload) - Handle webhook requests (for high availability)</p> <p>Only deployment operations (pushing configurations to HAProxy Dataplane API) need exclusivity.</p>"},{"location":"development/design/leader-election/#state-of-the-art-solution","title":"State-of-the-Art Solution","text":"<p>Use <code>k8s.io/client-go/tools/leaderelection</code> with Lease-based resource locks, the industry standard for Kubernetes operator high availability.</p>"},{"location":"development/design/leader-election/#why-lease-based-locks","title":"Why Lease-based Locks?","text":"<ul> <li>Lower overhead: Leases create less watch traffic than ConfigMaps or Endpoints</li> <li>Purpose-built: Designed specifically for leader election</li> <li>Reliable: Used by core Kubernetes components (kube-controller-manager, kube-scheduler)</li> <li>Clock skew tolerant: Configurable tolerance for node clock differences</li> </ul>"},{"location":"development/design/leader-election/#recommended-configuration","title":"Recommended Configuration","text":"<pre><code>LeaderElectionConfig{\n    LeaseDuration: 60 * time.Second,  // How long leader holds lock\n    RenewDeadline: 15 * time.Second,  // Renewal deadline before losing leadership\n    RetryPeriod:   5 * time.Second,   // Interval between renewal attempts\n    ReleaseOnCancel: true,            // Cleanup on graceful shutdown\n}\n</code></pre> <p>Tolerance formula: <code>LeaseDuration / RenewDeadline = clock skew tolerance ratio</code></p> <p>With 60s/15s settings, the system tolerates nodes progressing 4x faster than others.</p>"},{"location":"development/design/leader-election/#architecture-changes","title":"Architecture Changes","text":""},{"location":"development/design/leader-election/#component-classification","title":"Component Classification","text":"<p>All replicas run (read-only or validation operations): - ConfigWatcher - Monitors ConfigMap changes - CredentialsLoader - Monitors Secret changes - ResourceWatcher - Watches Kubernetes resources (Ingress, Service, etc.) - Reconciler - Debounces changes and triggers reconciliation - Renderer - Generates HAProxy configurations from templates - HAProxyValidator - Validates generated configurations - Executor - Orchestrates reconciliation workflow - Discovery - Discovers HAProxy pod endpoints - ConfigValidators - Validates controller configuration - WebhookValidators - Validates admission webhook requests - Commentator - Logs events for observability - Metrics - Records Prometheus metrics - StateCache - Maintains debug state</p> <p>Leader-only components (write operations to dataplane API): - Deployer - Deploys configurations to HAProxy instances - DeploymentScheduler - Rate-limits and queues deployments - DriftMonitor - Monitors and corrects configuration drift</p>"},{"location":"development/design/leader-election/#new-component-leaderelector","title":"New Component: LeaderElector","text":"<p>Package: <code>pkg/controller/leaderelection/</code></p> <p>Responsibilities: - Create and manage Lease lock in controller namespace - Use pod name as unique identity (via POD_NAME env var) - Publish leader election events to EventBus - Provide <code>IsLeader()</code> method for status queries - Handle graceful leadership release on shutdown</p> <p>Event integration: <pre><code>type LeaderElector struct {\n    eventBus *events.EventBus\n    elector  *leaderelection.LeaderElector\n    isLeader atomic.Bool\n}\n\n// Callbacks publish events\nOnStartedLeading: func(ctx context.Context) {\n    e.isLeader.Store(true)\n    e.eventBus.Publish(events.NewBecameLeaderEvent())\n}\n\nOnStoppedLeading: func() {\n    e.isLeader.Store(false)\n    e.eventBus.Publish(events.NewLostLeadershipEvent())\n}\n\nOnNewLeader: func(identity string) {\n    e.eventBus.Publish(events.NewNewLeaderObservedEvent(identity))\n}\n</code></pre></p>"},{"location":"development/design/leader-election/#new-events","title":"New Events","text":"<p>Leader election events (<code>pkg/controller/events/types.go</code>):</p> <pre><code>// LeaderElectionStartedEvent is published when leader election begins\ntype LeaderElectionStartedEvent struct {\n    Identity      string\n    LeaseName     string\n    LeaseNamespace string\n}\n\n// BecameLeaderEvent is published when this replica becomes leader\ntype BecameLeaderEvent struct {\n    Identity   string\n    Timestamp  time.Time\n}\n\n// LostLeadershipEvent is published when this replica loses leadership\ntype LostLeadershipEvent struct {\n    Identity   string\n    Timestamp  time.Time\n    Reason     string  // graceful_shutdown, lease_expired, etc.\n}\n\n// NewLeaderObservedEvent is published when a new leader is observed\ntype NewLeaderObservedEvent struct {\n    NewLeaderIdentity string\n    PreviousLeader    string\n    Timestamp         time.Time\n}\n</code></pre> <p>These events enable: - Observability: Commentator logs all transitions - Metrics: Track leadership duration, transition count - Debugging: Understand which replica is active</p>"},{"location":"development/design/leader-election/#controller-startup-changes","title":"Controller Startup Changes","text":"<p>Modified startup sequence (<code>pkg/controller/controller.go</code>):</p> <pre><code>Stage 0: Leader Election Initialization (NEW)\n  - Read POD_NAME from environment\n  - Create LeaderElector with Lease lock\n  - Start leader election loop in background goroutine\n  - Continue startup (don't block on becoming leader)\n\nStage 1: Config Management Components\n  - ConfigWatcher (all replicas)\n  - ConfigValidator (all replicas)\n  - EventBus.Start()\n\nStage 2: Wait for Valid Config\n  - All replicas block here\n\nStage 3: Resource Watchers\n  - Create ResourceWatcher (all replicas)\n  - Start IndexSynchronizationTracker (all replicas)\n\nStage 4: Wait for Index Sync\n  - All replicas block here\n\nStage 5: Reconciliation Components\n  - Reconciler (all replicas)\n  - Renderer (all replicas)\n  - HAProxyValidator (all replicas)\n  - Executor (all replicas)\n  - Discovery (all replicas)\n  - Deployer (LEADER ONLY - NEW)\n  - DeploymentScheduler (LEADER ONLY - NEW)\n  - DriftMonitor (LEADER ONLY - NEW)\n\nStage 6: Webhook Validation\n  - Webhook component (all replicas)\n  - DryRunValidator (all replicas)\n\nStage 7: Debug Infrastructure\n  - Debug server (all replicas)\n  - Metrics server (all replicas)\n</code></pre>"},{"location":"development/design/leader-election/#conditional-component-startup","title":"Conditional Component Startup","text":"<p>Implementation pattern:</p> <pre><code>// Create separate context for leader-only components\nleaderCtx, leaderCancel := context.WithCancel(iterCtx)\n\n// Track leader-only components\nvar leaderComponents struct {\n    sync.Mutex\n    deployer            *deployer.Component\n    deploymentScheduler *deployer.DeploymentScheduler\n    driftMonitor        *deployer.DriftPreventionMonitor\n    cancel              context.CancelFunc\n}\n\n// Leadership callbacks\nOnStartedLeading: func(ctx context.Context) {\n    logger.Info(\"Became leader, starting deployment components\")\n\n    leaderComponents.Lock()\n    defer leaderComponents.Unlock()\n\n    // Create fresh context for leader components\n    leaderComponents.cancel = leaderCancel\n\n    // Create and start leader-only components\n    leaderComponents.deployer = deployer.New(bus, logger)\n    leaderComponents.deploymentScheduler = deployer.NewDeploymentScheduler(bus, logger, minInterval)\n    leaderComponents.driftMonitor = deployer.NewDriftPreventionMonitor(bus, logger, driftInterval)\n\n    go leaderComponents.deployer.Start(leaderCtx)\n    go leaderComponents.deploymentScheduler.Start(leaderCtx)\n    go leaderComponents.driftMonitor.Start(leaderCtx)\n}\n\nOnStoppedLeading: func() {\n    logger.Warn(\"Lost leadership, stopping deployment components\")\n\n    leaderComponents.Lock()\n    defer leaderComponents.Unlock()\n\n    if leaderComponents.cancel != nil {\n        leaderComponents.cancel()\n        leaderComponents.cancel = nil\n    }\n}\n</code></pre> <p>Graceful transition: 1. Old leader loses lease \u2192 stops deployment components 2. Brief pause (lease expiry time) 3. New leader acquires lease \u2192 starts deployment components 4. New leader has hot cache and rendered config \u2192 immediate reconciliation</p>"},{"location":"development/design/leader-election/#configuration","title":"Configuration","text":"<p>New configuration section (<code>pkg/core/config/config.go</code>):</p> <pre><code>controller:\n  # ... existing fields ...\n\n  leaderElection:\n    enabled: true  # Enable leader election (default: true)\n    leaseName: \"haproxy-template-ic-leader\"\n    leaseDuration: 60s\n    renewDeadline: 15s\n    retryPeriod: 5s\n</code></pre> <p>Backwards compatibility: - <code>enabled: false</code> \u2192 Run without leader election (single replica mode) - Existing single-replica deployments work unchanged</p>"},{"location":"development/design/leader-election/#rbac-requirements","title":"RBAC Requirements","text":"<p>New permissions (<code>charts/haproxy-template-ic/templates/rbac.yaml</code>):</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: haproxy-template-ic\nrules:\n  # ... existing rules ...\n\n  # Leader election\n  - apiGroups: [\"coordination.k8s.io\"]\n    resources: [\"leases\"]\n    verbs: [\"get\", \"create\", \"update\"]\n</code></pre> <p>The controller creates a Lease in its own namespace (not cluster-wide).</p>"},{"location":"development/design/leader-election/#deployment-changes","title":"Deployment Changes","text":"<p>Environment variables (<code>charts/haproxy-template-ic/templates/deployment.yaml</code>):</p> <pre><code>spec:\n  template:\n    spec:\n      containers:\n      - name: controller\n        env:\n        # ... existing env vars ...\n\n        # Pod identity for leader election\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n</code></pre> <p>Multiple replicas:</p> <pre><code>spec:\n  replicas: 2  # Change from 1 to 2+ for HA\n</code></pre> <p>Resource adjustments:</p> <p>No changes needed - non-leader replicas consume similar resources since they perform all read-only work.</p>"},{"location":"development/design/leader-election/#observability","title":"Observability","text":""},{"location":"development/design/leader-election/#metrics","title":"Metrics","text":"<p>New Prometheus metrics (<code>pkg/controller/metrics/metrics.go</code>):</p> <pre><code>// controller_leader_transitions_total\n// Counter of leadership changes (acquire + lose)\ncontroller_leader_transitions_total counter\n\n// controller_is_leader\n// Gauge indicating current leadership status (1=leader, 0=follower)\ncontroller_is_leader{pod=\"&lt;pod-name&gt;\"} gauge\n\n// controller_leader_election_duration_seconds\n// Histogram of time to acquire leadership after startup\ncontroller_leader_election_duration_seconds histogram\n\n// controller_time_as_leader_seconds\n// Counter of cumulative seconds spent as leader\ncontroller_time_as_leader_seconds counter\n</code></pre> <p>Usage: - Alert on frequent transitions (indicates instability) - Dashboard showing current leader identity - Track leadership duration distribution</p>"},{"location":"development/design/leader-election/#logging","title":"Logging","text":"<p>Commentator enhancements (<code>pkg/controller/commentator/commentator.go</code>):</p> <pre><code>case LeaderElectionStartedEvent:\n    c.logger.Info(\"leader election started\",\n        \"identity\", e.Identity,\n        \"lease\", e.LeaseName,\n        \"namespace\", e.LeaseNamespace)\n\ncase BecameLeaderEvent:\n    c.logger.Info(\"became leader\",\n        \"identity\", e.Identity)\n\ncase LostLeadershipEvent:\n    c.logger.Warn(\"lost leadership\",\n        \"identity\", e.Identity,\n        \"reason\", e.Reason)\n\ncase NewLeaderObservedEvent:\n    c.logger.Info(\"new leader observed\",\n        \"new_leader\", e.NewLeaderIdentity,\n        \"previous_leader\", e.PreviousLeader)\n</code></pre>"},{"location":"development/design/leader-election/#debug-endpoints","title":"Debug Endpoints","text":"<p>Lease status (via debug server):</p> <pre><code>GET /debug/vars\n\n{\n  \"leader_election\": {\n    \"enabled\": true,\n    \"is_leader\": true,\n    \"identity\": \"haproxy-template-ic-7f8d9c5b-abc123\",\n    \"lease_name\": \"haproxy-template-ic-leader\",\n    \"lease_holder\": \"haproxy-template-ic-7f8d9c5b-abc123\",\n    \"time_as_leader\": \"45m32s\",\n    \"transitions\": 2\n  }\n}\n</code></pre>"},{"location":"development/design/leader-election/#testing-strategy","title":"Testing Strategy","text":""},{"location":"development/design/leader-election/#unit-tests","title":"Unit Tests","text":"<p>LeaderElector tests (<code>pkg/controller/leaderelection/elector_test.go</code>):</p> <pre><code>// Test leader election configuration\nfunc TestLeaderElector_Config(t *testing.T)\n\n// Test event publishing on leadership changes\nfunc TestLeaderElector_EventPublishing(t *testing.T)\n\n// Test IsLeader() method accuracy\nfunc TestLeaderElector_IsLeaderStatus(t *testing.T)\n\n// Test graceful shutdown\nfunc TestLeaderElector_GracefulShutdown(t *testing.T)\n</code></pre>"},{"location":"development/design/leader-election/#integration-tests","title":"Integration Tests","text":"<p>Multi-replica tests (<code>tests/integration/leader_election_test.go</code>):</p> <pre><code>// Deploy 2 replicas, verify only one deploys configs\nfunc TestLeaderElection_OnlyLeaderDeploys(t *testing.T)\n\n// Kill leader pod, verify follower takes over\nfunc TestLeaderElection_Failover(t *testing.T)\n\n// Verify both replicas watch resources\nfunc TestLeaderElection_BothReplicasWatchResources(t *testing.T)\n\n// Verify both replicas render configs\nfunc TestLeaderElection_BothReplicasRenderConfigs(t *testing.T)\n</code></pre> <p>Test setup: - Use kind cluster with multi-node setup - Deploy controller with 3 replicas - Create test Ingress resources - Verify deployment behavior - Simulate pod failures</p>"},{"location":"development/design/leader-election/#manual-testing","title":"Manual Testing","text":"<p>Verification steps:</p> <pre><code># Deploy with 3 replicas\nkubectl scale deployment haproxy-template-ic --replicas=3\n\n# Check lease status\nkubectl get lease -n haproxy-system haproxy-template-ic-leader -o yaml\n\n# Verify leader via metrics\nkubectl port-forward deployment/haproxy-template-ic 9090:9090\ncurl http://localhost:9090/metrics | grep controller_is_leader\n\n# Check logs for leadership events\nkubectl logs -l app=haproxy-template-ic --tail=100 | grep -i leader\n\n# Simulate failover\nkubectl delete pod &lt;leader-pod&gt;\n\n# Verify new leader takes over\nwatch kubectl get lease -n haproxy-system haproxy-template-ic-leader\n\n# Check HAProxy configs only deployed once per change\nkubectl logs -l app=haproxy-template-ic | grep \"deployment completed\"\n</code></pre>"},{"location":"development/design/leader-election/#failure-scenarios","title":"Failure Scenarios","text":""},{"location":"development/design/leader-election/#leader-pod-crashes","title":"Leader Pod Crashes","text":"<p>Behavior: 1. Leader lease expires (15s after last renewal) 2. Followers detect expired lease 3. First follower to update lease becomes new leader 4. New leader starts deployment components 5. Reconciliation continues from hot cache</p> <p>Downtime: ~15-20 seconds (RenewDeadline + startup time)</p>"},{"location":"development/design/leader-election/#network-partition","title":"Network Partition","text":"<p>Scenario: Leader pod loses connectivity to Kubernetes API</p> <p>Behavior: 1. Leader cannot renew lease 2. After RenewDeadline (15s), leader voluntarily releases leadership 3. Leader stops deployment components 4. Connected replica acquires lease 5. System continues with new leader</p> <p>Protection: Split-brain prevented by Kubernetes API acting as coordination point</p>"},{"location":"development/design/leader-election/#clock-skew","title":"Clock Skew","text":"<p>Scenario: Nodes have different clock speeds</p> <p>Tolerance: Configured ratio of LeaseDuration/RenewDeadline - With 60s/15s: Tolerates 4x clock speed difference - If exceeded: May experience frequent leadership changes</p> <p>Mitigation: Run NTP on cluster nodes (Kubernetes best practice)</p>"},{"location":"development/design/leader-election/#all-replicas-down","title":"All Replicas Down","text":"<p>Behavior: 1. Lease expires 2. No deployments occur (expected behavior) 3. HAProxy continues serving with last known configuration 4. When replica starts, acquires lease and reconciles</p> <p>Impact: No new configuration updates until controller recovers</p>"},{"location":"development/design/leader-election/#migration-path","title":"Migration Path","text":""},{"location":"development/design/leader-election/#phase-1-code-implementation","title":"Phase 1: Code Implementation","text":"<ol> <li>Implement LeaderElector package</li> <li>Add leader election events</li> <li>Modify controller startup for conditional components</li> <li>Add configuration options</li> <li>Update RBAC manifests</li> </ol>"},{"location":"development/design/leader-election/#phase-2-testing","title":"Phase 2: Testing","text":"<ol> <li>Unit tests for LeaderElector</li> <li>Integration tests with multi-replica setup</li> <li>Chaos testing (kill leaders, network partitions)</li> <li>Performance testing (ensure no regression)</li> </ol>"},{"location":"development/design/leader-election/#phase-3-documentation","title":"Phase 3: Documentation","text":"<ol> <li>Update deployment guide for HA setup</li> <li>Document troubleshooting procedures</li> <li>Update architecture diagrams</li> <li>Create runbooks for common scenarios</li> </ol>"},{"location":"development/design/leader-election/#phase-4-rollout","title":"Phase 4: Rollout","text":"<ol> <li>Release with <code>enabled: false</code> default</li> <li>Document opt-in HA setup</li> <li>Collect feedback from early adopters</li> <li>After validation, change default to <code>enabled: true</code></li> </ol>"},{"location":"development/design/leader-election/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"development/design/leader-election/#single-active-replica-with-pod-disruption-budget","title":"Single Active Replica with Pod Disruption Budget","text":"<p>Rejected: Doesn't provide HA, just prevents voluntary disruptions</p>"},{"location":"development/design/leader-election/#active-active-with-distributed-locking-per-haproxy-instance","title":"Active-Active with Distributed Locking per HAProxy Instance","text":"<p>Rejected: More complex, potential deadlocks, not idiomatic for Kubernetes</p>"},{"location":"development/design/leader-election/#external-coordination-etcd-consul","title":"External Coordination (etcd, Consul)","text":"<p>Rejected: Adds operational complexity, Kubernetes API sufficient</p>"},{"location":"development/design/leader-election/#config-generation-only-no-deployment","title":"Config Generation Only (No Deployment)","text":"<p>Rejected: Requires external system to deploy, doesn't solve core problem</p>"},{"location":"development/design/leader-election/#references","title":"References","text":"<ul> <li>Kubernetes client-go Leader Election</li> <li>Kubernetes Coordinated Leader Election (beta)</li> <li>Official client-go example</li> <li>Leader Election in Kubernetes Controllers (blog post)</li> </ul>"},{"location":"development/design/package-structure/","title":"Package Structure","text":"<p>The application is organized into focused Go packages following clean architecture principles:</p> <pre><code>haproxy-template-ic/\n\u251c\u2500\u2500 cmd/\n\u2502   \u2514\u2500\u2500 controller/          # Main entry point\n\u2502       \u2514\u2500\u2500 main.go\n\u251c\u2500\u2500 pkg/\n\u2502   \u251c\u2500\u2500 core/                # Core functionality\n\u2502   \u2502   \u251c\u2500\u2500 config/          # Configuration loading and validation\n\u2502   \u2502   \u2514\u2500\u2500 logging/         # Structured logging setup\n\u2502   \u251c\u2500\u2500 dataplane/           # HAProxy Dataplane API integration\n\u2502   \u2502   \u251c\u2500\u2500 auxiliaryfiles/  # Auxiliary file management (general, SSL, maps)\n\u2502   \u2502   \u251c\u2500\u2500 client/          # API client with transactions and retries\n\u2502   \u2502   \u251c\u2500\u2500 comparator/      # Fine-grained config comparison\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 sections/    # Section-specific comparators (30+ files)\n\u2502   \u2502   \u251c\u2500\u2500 discovery/       # HAProxy endpoint discovery\n\u2502   \u2502   \u251c\u2500\u2500 parser/          # Config parser using client-native\n\u2502   \u2502   \u251c\u2500\u2500 synchronizer/    # Operation execution logic\n\u2502   \u2502   \u251c\u2500\u2500 transform/       # Model transformation (client-native \u2194 Dataplane API)\n\u2502   \u2502   \u251c\u2500\u2500 types/           # Public types (Endpoint, SyncOptions, etc.)\n\u2502   \u2502   \u251c\u2500\u2500 config.go        # Public types (Endpoint, SyncOptions)\n\u2502   \u2502   \u251c\u2500\u2500 dataplane.go     # Public API (Client, Sync, DryRun, Diff)\n\u2502   \u2502   \u251c\u2500\u2500 errors.go        # Structured error types\n\u2502   \u2502   \u251c\u2500\u2500 orchestrator.go  # Sync workflow orchestration\n\u2502   \u2502   \u2514\u2500\u2500 result.go        # Result types\n\u2502   \u251c\u2500\u2500 events/              # Event bus infrastructure\n\u2502   \u2502   \u251c\u2500\u2500 bus.go           # EventBus with startup coordination\n\u2502   \u2502   \u251c\u2500\u2500 request.go       # Scatter-gather pattern\n\u2502   \u2502   \u2514\u2500\u2500 ringbuffer/      # Generic ring buffer for event history\n\u2502   \u2502       \u251c\u2500\u2500 ringbuffer.go         # Thread-safe circular buffer with generics\n\u2502   \u2502       \u251c\u2500\u2500 ringbuffer_test.go    # Unit tests\n\u2502   \u2502       \u251c\u2500\u2500 README.md             # API documentation\n\u2502   \u2502       \u2514\u2500\u2500 CLAUDE.md             # Development context\n\u2502   \u251c\u2500\u2500 introspection/       # Generic debug HTTP server infrastructure\n\u2502   \u2502   \u251c\u2500\u2500 handlers.go      # HTTP handlers for /debug/vars endpoints\n\u2502   \u2502   \u251c\u2500\u2500 http.go          # HTTP client utilities\n\u2502   \u2502   \u251c\u2500\u2500 jsonpath.go      # JSONPath field selection\n\u2502   \u2502   \u251c\u2500\u2500 registry.go      # Instance-based variable registry\n\u2502   \u2502   \u251c\u2500\u2500 registry_test.go # Registry tests\n\u2502   \u2502   \u251c\u2500\u2500 server.go        # HTTP server with graceful shutdown\n\u2502   \u2502   \u251c\u2500\u2500 types.go         # Var interface and types\n\u2502   \u2502   \u251c\u2500\u2500 var.go           # Built-in Var implementations\n\u2502   \u2502   \u251c\u2500\u2500 README.md        # API documentation\n\u2502   \u2502   \u2514\u2500\u2500 CLAUDE.md        # Development context\n\u2502   \u251c\u2500\u2500 metrics/             # Prometheus metrics infrastructure\n\u2502   \u2502   \u251c\u2500\u2500 server.go        # HTTP server for /metrics endpoint\n\u2502   \u2502   \u251c\u2500\u2500 helpers.go       # Metric creation helpers\n\u2502   \u2502   \u251c\u2500\u2500 server_test.go   # Server tests\n\u2502   \u2502   \u251c\u2500\u2500 helpers_test.go  # Helper tests\n\u2502   \u2502   \u251c\u2500\u2500 README.md        # API documentation\n\u2502   \u2502   \u2514\u2500\u2500 CLAUDE.md        # Development context\n\u2502   \u251c\u2500\u2500 k8s/                 # Kubernetes integration\n\u2502   \u2502   \u251c\u2500\u2500 types/           # Core interfaces and types\n\u2502   \u2502   \u251c\u2500\u2500 client/          # Kubernetes client wrapper with dynamic client\n\u2502   \u2502   \u251c\u2500\u2500 indexer/         # JSONPath key extraction and field filtering\n\u2502   \u2502   \u251c\u2500\u2500 store/           # Memory and cached store implementations\n\u2502   \u2502   \u2514\u2500\u2500 watcher/         # Resource watching with debouncing and sync tracking\n\u2502   \u251c\u2500\u2500 controller/          # Controller lifecycle\n\u2502   \u2502   \u251c\u2500\u2500 commentator/     # Event commentator for domain-aware logging\n\u2502   \u2502   \u251c\u2500\u2500 configchange/    # Configuration change handler\n\u2502   \u2502   \u251c\u2500\u2500 configloader/    # Config parsing and loading\n\u2502   \u2502   \u251c\u2500\u2500 credentialsloader/ # Credentials parsing and loading\n\u2502   \u2502   \u251c\u2500\u2500 debug/           # Controller-specific debug variables\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 events.go    # Event buffer implementation\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 setup.go     # Variable registration\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 state.go     # State snapshot variables\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 vars.go      # Debug Var implementations (Config, Credentials, etc.)\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 README.md    # API documentation\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 CLAUDE.md    # Development context\n\u2502   \u2502   \u251c\u2500\u2500 deployer/        # Deployment orchestration (scheduler, executor, drift monitor)\n\u2502   \u2502   \u251c\u2500\u2500 discovery/       # HAProxy pod discovery and endpoint management\n\u2502   \u2502   \u251c\u2500\u2500 events/          # Domain-specific event types\n\u2502   \u2502   \u251c\u2500\u2500 executor/        # Reconciliation orchestrator\n\u2502   \u2502   \u251c\u2500\u2500 indextracker/    # Index synchronization tracking\n\u2502   \u2502   \u251c\u2500\u2500 metrics/         # Prometheus metrics (event adapter)\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 metrics.go   # Domain metric definitions\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 component.go # Event adapter for metrics\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 metrics_test.go\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 component_test.go\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 README.md    # API documentation\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 CLAUDE.md    # Development context\n\u2502   \u2502   \u251c\u2500\u2500 reconciler/      # Reconciliation debouncer and trigger\n\u2502   \u2502   \u251c\u2500\u2500 renderer/        # Template rendering component\n\u2502   \u2502   \u251c\u2500\u2500 resourcewatcher/ # Resource watcher lifecycle management\n\u2502   \u2502   \u251c\u2500\u2500 validator/       # Config validation (basic, template, jsonpath)\n\u2502   \u2502   \u251c\u2500\u2500 controller.go    # Event coordination and startup orchestration\n\u2502   \u2502   \u2514\u2500\u2500 statecache.go    # Event-driven state tracking for debug endpoints\n\u2502   \u2514\u2500\u2500 templating/          # Template engine library\n\u2502       \u251c\u2500\u2500 engine.go        # TemplateEngine with pre-compilation and rendering\n\u2502       \u251c\u2500\u2500 types.go         # Engine type definitions (EngineTypeGonja)\n\u2502       \u251c\u2500\u2500 errors.go        # Custom error types\n\u2502       \u251c\u2500\u2500 loader.go        # Template loading utilities\n\u2502       \u251c\u2500\u2500 loader_test.go   # Loader tests\n\u2502       \u251c\u2500\u2500 engine_test.go   # Unit tests\n\u2502       \u2514\u2500\u2500 README.md        # Usage documentation\n\u251c\u2500\u2500 tests/                   # End-to-end tests\n\u2502   \u251c\u2500\u2500 acceptance/          # Acceptance tests with debug endpoint validation\n\u2502   \u2502   \u251c\u2500\u2500 configmap_reload_test.go  # ConfigMap reload validation\n\u2502   \u2502   \u251c\u2500\u2500 metrics_test.go  # Metrics endpoint validation\n\u2502   \u2502   \u251c\u2500\u2500 debug_client.go  # Debug endpoint client for tests\n\u2502   \u2502   \u251c\u2500\u2500 env.go           # Test environment setup\n\u2502   \u2502   \u251c\u2500\u2500 fixtures.go      # Test resource management\n\u2502   \u2502   \u251c\u2500\u2500 main_test.go     # Test suite entry point\n\u2502   \u2502   \u251c\u2500\u2500 README.md        # Testing documentation\n\u2502   \u2502   \u2514\u2500\u2500 CLAUDE.md        # Development context\n\u2502   \u2514\u2500\u2500 integration/         # Integration tests\n\u2502       \u251c\u2500\u2500 README.md        # Integration test documentation\n\u2502       \u2514\u2500\u2500 CLAUDE.md        # Development context\n\u2514\u2500\u2500 tools/                   # Development tools\n    \u2514\u2500\u2500 linters/             # Custom linters\n        \u2514\u2500\u2500 eventimmutability/  # Event pointer receiver linter\n            \u251c\u2500\u2500 analyzer.go  # Custom golangci-lint analyzer\n            \u251c\u2500\u2500 analyzer_test.go\n            \u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"development/design/package-structure/#package-responsibilities","title":"Package Responsibilities","text":"<p>Core Packages:</p> <ul> <li><code>pkg/core/config</code>: Provides pure functions for parsing configuration from YAML strings and loading credentials from Kubernetes Secret data. Includes basic structural validation (port ranges, required fields). Does NOT implement watchers - the controller package instantiates watchers from pkg/k8s and coordinates validation using scatter-gather pattern.</li> <li><code>pkg/core/logging</code>: Structured logging setup using Go's standard library <code>log/slog</code> package with configurable levels and formats</li> </ul> <p>Event Bus Infrastructure:</p> <ul> <li><code>pkg/events</code>: Generic event bus providing pub/sub and request-response (scatter-gather) patterns for component coordination. Domain-agnostic infrastructure that could be extracted as standalone library.</li> <li><code>pkg/events/ringbuffer</code>: Generic thread-safe ring buffer implementation using Go generics. Fixed-size circular buffer with automatic old-item eviction. O(1) add, O(n) retrieval. Used by EventCommentator and EventBuffer for event history tracking. Domain-agnostic, can be used with any type.</li> </ul> <p>Runtime Introspection Infrastructure:</p> <ul> <li><code>pkg/introspection</code>: Generic HTTP debug server infrastructure for exposing internal application state. Provides instance-based variable registry (not global like expvar), HTTP handlers for <code>/debug/vars</code> endpoints, JSONPath field selection support, Go profiling integration (<code>/debug/pprof</code>), and graceful shutdown. Domain-agnostic package that can be reused in any Go application.</li> </ul> <p>Observability - Metrics Infrastructure:</p> <ul> <li><code>pkg/metrics</code>: Generic Prometheus metrics infrastructure providing HTTP server for <code>/metrics</code> endpoint and metric creation helpers. Uses instance-based <code>prometheus.Registry</code> (not global DefaultRegisterer) for clean lifecycle management. Provides <code>NewServer()</code> for metrics HTTP server and helper functions for creating counters, histograms, gauges, and gauge vectors. Domain-agnostic package that can be reused in any Go application.</li> <li><code>pkg/controller/metrics</code>: Domain-specific Prometheus metrics for controller operations. Implements event adapter pattern to track reconciliation, deployment, validation, resource counts, and event activity. Exposes 11 metrics including operation counters, error counters, duration histograms, and resource gauges. Started via explicit <code>Start()</code> method after EventBus initialization to prevent race conditions during startup.</li> </ul> <p>Dataplane Integration:</p> <ul> <li><code>pkg/dataplane</code>: Public API providing Client interface and convenience functions (Sync, DryRun, Diff)</li> <li><code>pkg/dataplane/orchestrator</code>: Coordinates complete sync workflow (parse \u2192 compare \u2192 sync \u2192 auxiliary files)</li> <li><code>pkg/dataplane/parser</code>: Wraps haproxytech/client-native for syntax validation and structured config parsing</li> <li><code>pkg/dataplane</code> (validator.go): Pure validation functions implementing two-phase HAProxy configuration validation: Phase 1 (syntax validation using client-native parser) and Phase 2 (semantic validation using haproxy binary with -c flag). Writes auxiliary files to actual HAProxy directories (with mutex locking to prevent concurrent writes) to validate file references exactly as the Dataplane API does. Requires ValidationPaths parameter matching Dataplane API resource configuration. Provides ValidateConfiguration(mainConfig, auxFiles, paths) as pure function with no event dependencies.</li> <li><code>pkg/dataplane/comparator</code>: Performs fine-grained section-by-section comparison to generate minimal change operations</li> <li><code>pkg/dataplane/comparator/sections</code>: Section-specific comparison logic for all HAProxy config sections (global, defaults, frontends, backends, servers, ACLs, rules, binds, filters, checks, etc.)</li> <li><code>pkg/dataplane/transform</code>: Converts client-native parser models to Dataplane API models using JSON marshaling. Provides 35+ type-specific transformation functions (ToAPIACL, ToAPIBackend, ToAPIFrontend, etc.) that eliminate ~77 duplicate inline conversions previously scattered across comparator sections. Uses generic <code>transform[T]</code> function for nil-safe JSON-based type conversion. Performance: ~10\u00b5s per transformation, acceptable for reconciliation workflow.</li> <li><code>pkg/dataplane/discovery</code>: HAProxy endpoint discovery utilities for identifying dataplane API endpoints</li> <li><code>pkg/dataplane/synchronizer</code>: Executes operations with transaction management and retry logic</li> <li><code>pkg/dataplane/auxiliaryfiles</code>: Manages auxiliary files (general files, SSL certificates, map files) with 3-phase sync: pre-config (create/update), config sync, post-config (delete)</li> <li><code>pkg/dataplane/client</code>: HTTP client wrapper for Dataplane API with version conflict handling, transaction lifecycle, and storage API integration</li> <li><code>pkg/dataplane/types</code>: Public types used across dataplane package (Endpoint, SyncOptions, Result types)</li> </ul> <p>Kubernetes Integration:</p> <ul> <li><code>pkg/k8s/types</code>: Core interfaces and types for the k8s package</li> <li><code>Store</code> interface for resource indexing (Get, List, Add, Update, Delete, Clear)</li> <li><code>WatcherConfig</code> for configuring bulk resource watching with filters, indexing, and callbacks</li> <li><code>SingleWatcherConfig</code> for configuring single named resource watching (ConfigMap, Secret, etc.)</li> <li><code>OnChangeCallback</code> and <code>OnSyncCompleteCallback</code> for bulk watcher change notifications</li> <li><code>OnResourceChangeCallback</code> for single resource watcher immediate callbacks</li> <li><code>ChangeStats</code> for tracking resource changes with initial sync context</li> <li><code>StoreType</code> enum for memory vs cached store selection</li> <li><code>pkg/k8s/client</code>: Kubernetes client wrapper with dynamic client support</li> <li>Wraps kubernetes.Interface and dynamic.Interface</li> <li>Auto-detects in-cluster vs out-of-cluster configuration</li> <li>Provides namespace detection from service account token</li> <li><code>pkg/k8s/indexer</code>: JSONPath evaluation and field filtering</li> <li>Extracts index keys from resources using JSONPath expressions (e.g., <code>metadata.namespace</code>, <code>metadata.labels['key']</code>)</li> <li>Removes unnecessary fields to reduce memory usage (e.g., <code>metadata.managedFields</code>)</li> <li>Fail-fast validation of JSONPath expressions at startup</li> <li><code>pkg/k8s/store</code>: Store implementations for indexed resource storage</li> <li><code>MemoryStore</code>: Fast in-memory storage with complete resources (default)</li> <li><code>CachedStore</code>: Memory-efficient storage with API-backed fetches and TTL caching (for large resources like Secrets)</li> <li>Thread-safe with RWMutex for concurrent access</li> <li>O(1) lookups using composite keys from multiple index expressions</li> <li><code>pkg/k8s/watcher</code>: High-level resource watching with two watcher types</li> <li>Watcher (bulk resource watching): For watching collections of resources (Ingress, Service, EndpointSlice)<ul> <li>Uses SharedInformerFactory for efficient resource watching</li> <li>Supports namespace and label selector filtering</li> <li>Debounces rapid changes with configurable interval (default 500ms)</li> <li>Indexed storage with O(1) lookups using JSONPath expressions</li> <li>Tracks initial sync state with <code>OnSyncComplete</code> callback and <code>IsInitialSync</code> flag</li> <li>Provides <code>WaitForSync()</code> and <code>IsSynced()</code> for manual synchronization control</li> <li>Supports incremental or bulk processing during initial sync via <code>CallOnChangeDuringSync</code></li> </ul> </li> <li>SingleWatcher (single resource watching): For watching one specific named resource (ConfigMap, Secret)<ul> <li>Lightweight implementation with no indexing or store overhead</li> <li>Watches single resource by namespace + name using field selector</li> <li>Immediate callbacks (no debouncing) with <code>OnResourceChangeCallback</code></li> <li>Ideal for controller configuration (ConfigMap) and credentials (Secret)</li> <li>Provides <code>WaitForSync()</code> and <code>IsSynced()</code> for startup coordination</li> </ul> </li> </ul> <p>Initial Synchronization Handling:</p> <p>The k8s package provides comprehensive support for distinguishing between initial bulk loading of pre-existing resources and real-time changes:</p> <ul> <li>Sync Tracking: Watcher tracks initial sync state internally and provides sync status via <code>IsSynced()</code> method</li> <li>OnSyncComplete Callback: Called once after initial sync completes with the fully populated store and resource count</li> <li>IsInitialSync Flag: ChangeStats includes <code>IsInitialSync</code> field to distinguish bulk load events from real-time changes</li> <li>Callback Control: <code>CallOnChangeDuringSync</code> flag allows choosing between:</li> <li>Incremental processing: Receive callbacks during sync with <code>IsInitialSync=true</code> for progressive resource handling</li> <li>Bulk processing: Suppress callbacks during sync, receive only <code>OnSyncComplete</code> with final state</li> <li>Manual Synchronization: <code>WaitForSync(ctx)</code> blocks until initial sync completes, useful for staged startup</li> <li>Non-Blocking Status: <code>IsSynced()</code> provides non-blocking sync status check</li> </ul> <p>This prevents common pitfalls like rendering HAProxy configuration before all ingresses are loaded, ensuring the system always has complete data before taking action.</p> <p>Configuration and Credentials Management:</p> <p>The controller monitors two critical single resources using <code>SingleWatcher</code>:</p> <ol> <li>Controller Configuration (ConfigMap):</li> <li>Contains templates, watched resource definitions, and controller settings</li> <li>Watched using <code>SingleWatcher</code> for immediate re-parsing and validation on changes</li> <li>Name and namespace configured via environment variables or auto-detected</li> <li>Parsed using <code>ParseConfig(configMapData map[string][]byte)</code> from pkg/core/config</li> <li> <p>Changes trigger scatter-gather validation before becoming active</p> </li> <li> <p>Controller Credentials (Secret):</p> </li> <li>Secret name: Configurable via environment variable (default: haproxy-template-ic-credentials)</li> <li>Contains 4 required keys:<ul> <li><code>dataplane_username</code>: Username for HAProxy Dataplane API</li> <li><code>dataplane_password</code>: Password for HAProxy Dataplane API</li> <li><code>validation_username</code>: Username for validation endpoint</li> <li><code>validation_password</code>: Password for validation endpoint</li> </ul> </li> <li>Watched using <code>SingleWatcher</code> for immediate credential rotation on changes</li> <li>Loaded using <code>LoadCredentials(secretData map[string][]byte)</code> from pkg/core/config</li> <li>All fields are required and validated to be non-empty</li> </ol> <p>Both watchers use the event-driven architecture: changes publish events to EventBus, triggering validation (ConfigMap) or credential updates (Secret).</p> <p>Controller Logic:</p> <ul> <li><code>pkg/controller</code>: Main controller package implementing reinitialization loop pattern. Coordinates startup orchestration and component lifecycle via EventBus, responds to configuration changes by cleanly restarting iterations with new settings</li> <li><code>pkg/controller/commentator</code>: Event commentator that subscribes to all events and produces domain-aware log messages with contextual insights using ring buffer for event correlation</li> <li><code>pkg/controller/configloader</code>: Loads and parses controller configuration from ConfigMap resources, publishes ConfigParsedEvent</li> <li><code>pkg/controller/credentialsloader</code>: Loads and validates credentials from Secret resources, publishes CredentialsUpdatedEvent</li> <li><code>pkg/controller/configchange</code>: Handles configuration change events and coordinates reloading of resources</li> <li><code>pkg/controller/deployer</code>: Deployment orchestration package (Stage 5) implementing three-component architecture:</li> <li>DeploymentScheduler: Coordinates WHEN deployments happen. Maintains state (last validated config, current endpoints), enforces minimum deployment interval (default 2s) for rate limiting, implements \"latest wins\" queueing for concurrent changes. Subscribes to TemplateRenderedEvent, ValidationCompletedEvent, HAProxyPodsDiscoveredEvent, DriftPreventionTriggeredEvent, DeploymentCompletedEvent. Publishes DeploymentScheduledEvent.</li> <li>Deployer: Stateless executor that performs deployments. Subscribes to DeploymentScheduledEvent, executes parallel deployments to multiple HAProxy endpoints. Publishes DeploymentStartedEvent, InstanceDeployedEvent, InstanceDeploymentFailedEvent, DeploymentCompletedEvent.</li> <li>DriftPreventionMonitor: Prevents configuration drift from external changes. Monitors deployment activity and triggers periodic deployments (default 60s) when system is idle. Subscribes to DeploymentCompletedEvent. Publishes DriftPreventionTriggeredEvent.</li> <li><code>pkg/controller/discovery</code>: HAProxy pod discovery component (Stage 5). Discovers HAProxy pods in the cluster and provides endpoint information to the Deployer. Publishes HAProxyPodsDiscoveredEvent with discovered endpoints.</li> <li><code>pkg/controller/executor</code>: Orchestrates reconciliation cycles by handling events from pure components. Subscribes to ReconciliationTriggeredEvent, TemplateRenderedEvent, TemplateRenderFailedEvent, ValidationCompletedEvent, and ValidationFailedEvent. Publishes ReconciliationStartedEvent, ReconciliationCompletedEvent, and ReconciliationFailedEvent. Coordinates the event-driven flow: Renderer \u2192 Validator \u2192 Deployer. Measures reconciliation duration for observability and handles validation failures by publishing ReconciliationFailedEvent.</li> <li><code>pkg/controller/indextracker</code>: Tracks synchronization state across multiple resource types, publishes IndexSynchronizedEvent when all resources complete initial sync, enabling staged controller startup with clear initialization checkpoints</li> <li><code>pkg/controller/metrics</code>: Prometheus metrics event adapter. Subscribes to controller lifecycle events (reconciliation, deployment, validation, discovery) and exports 11 metrics including operation counters (reconciliation_total, deployment_total, validation_total), error counters, duration histograms (reconciliation_duration_seconds, deployment_duration_seconds), resource gauges (resource_count with type labels), and event bus metrics (event_subscribers, events_published_total). Uses instance-based prometheus.Registry and explicit Start() method to prevent race conditions during startup. Metrics exposed on configurable port (default 9090) via pkg/metrics HTTP server.</li> <li><code>pkg/controller/reconciler</code>: Debounces resource change events and triggers reconciliation cycles. Subscribes to ResourceIndexUpdatedEvent (applies debouncing with configurable interval, default 500ms) and ConfigValidatedEvent (triggers immediately without debouncing). Publishes ReconciliationTriggeredEvent when conditions are met. Filters initial sync events to prevent premature reconciliation. First Stage 5 component enabling controlled reconciliation trigger logic.</li> <li><code>pkg/controller/resourcewatcher</code>: Manages lifecycle of all Kubernetes resource watchers defined in configuration, provides centralized WaitForAllSync() method for coordinated initialization, publishes ResourceIndexUpdatedEvent with detailed change statistics</li> <li><code>pkg/controller/validator</code>: Contains validation components for controller configuration validation (basic structural validation, template syntax validation, JSONPath expression validation) that respond to ConfigValidationRequest events using scatter-gather pattern</li> <li><code>pkg/controller/validator/haproxy_validator.go</code>: HAProxy configuration validator component (Stage 5). Subscribes to TemplateRenderedEvent and validates rendered HAProxy configurations using two-phase validation: syntax validation with client-native parser and semantic validation with haproxy binary. Publishes ValidationCompletedEvent on success or ValidationFailedEvent with detailed error messages on failure. Integrates pkg/dataplane validation logic into the event-driven architecture.</li> <li><code>pkg/controller/renderer</code>: Template rendering component (Stage 5). Subscribes to ReconciliationTriggeredEvent and renders HAProxy configuration and auxiliary files from templates using the templating engine. Publishes TemplateRenderedEvent with rendered configuration and auxiliary files, or TemplateRenderFailedEvent on rendering errors.</li> <li><code>pkg/controller/events</code>: Domain-specific event type definitions (~50 event types covering complete controller lifecycle including validation events)</li> <li><code>pkg/controller/debug</code>: Controller-specific debug variable implementations for introspection HTTP server. Implements <code>introspection.Var</code> interface for controller data including ConfigVar, CredentialsVar (metadata only, not actual passwords), RenderedVar, ResourcesVar, and EventsVar. Provides EventBuffer for independent event tracking separate from EventCommentator. Exposes StateProvider interface for accessing controller state in a thread-safe manner.</li> <li><code>pkg/controller/statecache.go</code>: Event-driven state cache implementing StateProvider interface. Subscribes to validation, rendering, and resource events to maintain current state snapshot in memory with thread-safe RWMutex-protected access. Provides debug endpoints with access to current configuration, credentials (metadata), rendered output, and resource counts without querying EventBus for historical state.</li> </ul> <p>Testing Infrastructure:</p> <ul> <li><code>tests/acceptance</code>: End-to-end acceptance testing framework with debug endpoint and metrics validation. Provides fixture management for test resources (ConfigMaps, Services, Endpoints), debug client for querying introspection endpoints during tests via kubectl port-forward, environment helpers for test cluster setup and teardown, ConfigMap reload tests, and metrics endpoint validation tests. Tests verify metrics endpoint accessibility, presence of all 11 expected metrics, non-zero operational values, and histogram structure. Enables true end-to-end testing without parsing logs or relying on timing heuristics.</li> <li><code>tests/integration</code>: Integration testing infrastructure for component interaction tests with documentation of testing strategies and environment setup.</li> </ul> <p>Template Engine:</p> <ul> <li><code>pkg/templating</code>: Low-level template engine library providing template compilation and rendering</li> <li>Pre-compiles templates at initialization for optimal runtime performance</li> <li>Wraps Gonja v2 for Jinja2-compatible template syntax</li> <li>Provides TemplateEngine with Render(templateName, context) API</li> <li>Custom error types for compilation, rendering, and template-not-found scenarios</li> <li>Custom filters (b64decode) and context methods (pathResolver) are integrated in controller rendering components</li> </ul> <p>Development Tools:</p> <ul> <li><code>tools/linters/eventimmutability</code>: Custom golangci-lint analyzer that enforces event immutability contract</li> <li>Checks that all Event interface method implementations use pointer receivers</li> <li>Prevents accidental struct copying (events often exceed 200 bytes)</li> <li>Integrated into <code>make lint</code> and CI pipeline via golangci-lint</li> <li>Provides clear error messages with file locations when violations detected</li> <li>See <code>tools/linters/eventimmutability/README.md</code> for details</li> </ul>"},{"location":"development/design/package-structure/#key-interfaces","title":"Key Interfaces","text":"<pre><code>// Resource storage (pkg/k8s/types)\ntype Store interface {\n    // Get retrieves all resources matching the provided index keys\n    Get(keys ...string) ([]interface{}, error)\n\n    // List returns all resources in the store\n    List() ([]interface{}, error)\n\n    // Add inserts a new resource with the provided index keys\n    Add(resource interface{}, keys []string) error\n\n    // Update modifies an existing resource\n    Update(resource interface{}, keys []string) error\n\n    // Delete removes a resource using its index keys\n    Delete(keys ...string) error\n\n    // Clear removes all resources from the store\n    Clear() error\n}\n\n// Change notification callbacks (pkg/k8s/types)\ntype OnChangeCallback func(store Store, stats ChangeStats)\n\ntype OnSyncCompleteCallback func(store Store, initialCount int)\n\ntype ChangeStats struct {\n    Created  int  // Number of resources added\n    Modified int  // Number of resources updated\n    Deleted  int  // Number of resources removed\n    IsInitialSync bool  // True during initial synchronization\n}\n\n// Template rendering\ntype TemplateRenderer interface {\n    Render(template string, context interface{}) (string, error)\n    RenderAll(config models.Config, resources ResourceCollection) (RenderedContext, error)\n}\n\n// Configuration validation\ntype ConfigValidator interface {\n    ValidateSyntax(config string) error\n    ValidateSemantics(config string) error\n}\n\n// Dataplane operations\ntype DataplaneClient interface {\n    GetVersion() (VersionInfo, error)\n    DeployConfiguration(config string) (DeploymentResult, error)\n    FetchStructuredConfig() (StructuredConfig, error)\n    SyncMaps(maps map[string]string) error\n    SyncCertificates(certs map[string]string) error\n}\n\n// Configuration synchronization\ntype ConfigSynchronizer interface {\n    SyncConfiguration(ctx context.Context, config RenderedContext) (SyncResult, error)\n    UpdateEndpoints(endpoints []DataplaneEndpoint) error\n}\n</code></pre>"},{"location":"development/design/sequence-diagrams/","title":"Sequence Diagrams","text":""},{"location":"development/design/sequence-diagrams/#startup-and-initialization","title":"Startup and Initialization","text":"<p>The controller uses a reinitialization loop pattern where it responds to configuration changes by restarting with the new configuration. Each iteration follows these initialization steps:</p> <pre><code>sequenceDiagram\n    participant Main\n    participant Iteration as runIteration()\n    participant EventBus\n    participant Components\n    participant ResourceWatcher as Resource&lt;br/&gt;Watcher\n    participant ConfigWatcher as Config&lt;br/&gt;Watcher\n    participant Reconciler\n\n    Main-&gt;&gt;Main: Reinitialization Loop\n\n    loop Until Context Cancelled\n        Main-&gt;&gt;Iteration: Run iteration\n\n        Note over Iteration: 1. Fetch &amp; Validate Initial Config\n        Iteration-&gt;&gt;Iteration: Fetch ConfigMap &amp; Secret\n        Iteration-&gt;&gt;Iteration: Parse &amp; Validate\n\n        Note over Iteration,EventBus: 2. Setup Components\n        Iteration-&gt;&gt;EventBus: Create EventBus(100)\n        Iteration-&gt;&gt;Components: Start validators, loaders, commentator\n\n        Note over Iteration,ResourceWatcher: 3. Setup Resource Watchers\n        Iteration-&gt;&gt;ResourceWatcher: Create &amp; Start\n        Iteration-&gt;&gt;ResourceWatcher: WaitForAllSync()\n\n        Note over Iteration,ConfigWatcher: 4. Setup Config/Secret Watchers\n        Iteration-&gt;&gt;ConfigWatcher: Create &amp; Start\n        Iteration-&gt;&gt;ConfigWatcher: WaitForSync()\n\n        Note over Iteration,EventBus: 5. Start EventBus\n        Iteration-&gt;&gt;EventBus: Start() (replay buffered events)\n\n        Note over Iteration,Reconciler: Stage 5: Reconciliation &amp; Observability Components\n        Iteration-&gt;&gt;Reconciler: Start Reconciler, Renderer, Validator, Executor, Deployer, Discovery, Metrics\n        Iteration-&gt;&gt;EventBus: Publish initial ReconciliationTriggeredEvent\n\n        Note over Iteration: 6. Event Loop\n        Iteration-&gt;&gt;Iteration: Wait for config change or cancellation\n\n        alt Config Change Detected\n            ConfigWatcher-&gt;&gt;EventBus: ConfigValidatedEvent (new config)\n            Iteration-&gt;&gt;Iteration: Cancel iteration context\n            Iteration--&gt;&gt;Main: Return nil (reinitialize)\n        else Context Cancelled\n            Iteration--&gt;&gt;Main: Return nil (shutdown)\n        end\n    end</code></pre> <p>Reinitialization Loop Pattern:</p> <p>The controller runs iterations that respond to configuration changes:</p> <ol> <li>Initial Config Fetch: Fetch and validate ConfigMap and Secret synchronously before starting components</li> <li>Component Setup: Create EventBus and start config management components (validators, loaders, commentator)</li> <li>Resource Watchers: Create watchers for configured resources and wait for initial sync</li> <li>Config Watchers: Create watchers for ConfigMap and Secret, wait for sync</li> <li>EventBus Start: Call EventBus.Start() to replay buffered events and begin normal operation</li> <li>Stage 5 - Reconciliation &amp; Observability: Start reconciliation components (Reconciler, Renderer, Validator, Executor, Deployer, Discovery) and observability components (Metrics, Debug HTTP servers)</li> <li>Event Loop: Wait for configuration changes or context cancellation</li> <li>Reinitialization: When config changes, cancel iteration context to stop all components, then restart with new config</li> </ol> <p>This pattern ensures the controller always operates with validated configuration and handles configuration updates by cleanly restarting with the new settings. The Stage 5 label is explicitly used in code for reconciliation components; earlier stages are implicit in the initialization sequence. Metrics collection starts in Stage 5 after EventBus.Start() to ensure all event subscriptions are properly registered before metrics begin tracking events.</p>"},{"location":"development/design/sequence-diagrams/#resource-change-handling","title":"Resource Change Handling","text":"<pre><code>sequenceDiagram\n    participant K8S as Kubernetes API\n    participant ResourceWatcher as Resource&lt;br/&gt;Watcher\n    participant EventBus\n    participant Reconciler as Reconciler&lt;br/&gt;(Debouncer)\n    participant Executor\n    participant Renderer as Renderer\n    participant Validator as HAProxy&lt;br/&gt;Validator\n    participant Scheduler as Deployment&lt;br/&gt;Scheduler\n    participant Deployer as Deployer\n    participant HAProxy1 as HAProxy&lt;br/&gt;Instance 1\n    participant HAProxy2 as HAProxy&lt;br/&gt;Instance 2\n\n    K8S-&gt;&gt;ResourceWatcher: Resource update event\n    ResourceWatcher-&gt;&gt;ResourceWatcher: Update local index\n    ResourceWatcher-&gt;&gt;EventBus: Publish(ResourceIndexUpdatedEvent)\n\n    EventBus-&gt;&gt;Reconciler: ResourceIndexUpdatedEvent\n    Note over Reconciler: Start debounce timer\n\n    Note over Reconciler: Wait for quiet period\n\n    Reconciler-&gt;&gt;EventBus: Publish(ReconciliationTriggeredEvent)\n\n    EventBus-&gt;&gt;Executor: ReconciliationTriggeredEvent\n    Executor-&gt;&gt;EventBus: Publish(ReconciliationStartedEvent)\n\n    EventBus-&gt;&gt;Renderer: ReconciliationTriggeredEvent\n    Note over Renderer: Query indexed resources&lt;br/&gt;Render templates\n    Renderer-&gt;&gt;EventBus: Publish(TemplateRenderedEvent)\n\n    EventBus-&gt;&gt;Validator: TemplateRenderedEvent\n    Note over Validator: Phase 1: Syntax (parser)&lt;br/&gt;Phase 2: Semantics (haproxy -c)\n    Validator-&gt;&gt;EventBus: Publish(ValidationCompletedEvent)\n\n    EventBus-&gt;&gt;Scheduler: ValidationCompletedEvent\n    Note over Scheduler: Check rate limit&lt;br/&gt;Queue if deployment in progress\n    Scheduler-&gt;&gt;EventBus: Publish(DeploymentScheduledEvent)\n\n    EventBus-&gt;&gt;Deployer: DeploymentScheduledEvent\n    Deployer-&gt;&gt;EventBus: Publish(DeploymentStartedEvent)\n\n    par Parallel Deployment\n        Deployer-&gt;&gt;HAProxy1: Deploy via Dataplane API\n        HAProxy1--&gt;&gt;Deployer: Success\n        Deployer-&gt;&gt;EventBus: Publish(InstanceDeployedEvent)\n    and\n        Deployer-&gt;&gt;HAProxy2: Deploy via Dataplane API\n        HAProxy2--&gt;&gt;Deployer: Success\n        Deployer-&gt;&gt;EventBus: Publish(InstanceDeployedEvent)\n    end\n\n    Deployer-&gt;&gt;EventBus: Publish(DeploymentCompletedEvent)\n    EventBus-&gt;&gt;Executor: DeploymentCompletedEvent\n    Executor-&gt;&gt;EventBus: Publish(ReconciliationCompletedEvent)</code></pre> <p>Event-Driven Flow:</p> <ol> <li>Resource Change: ResourceWatcher receives Kubernetes event, updates local index, publishes ResourceIndexUpdatedEvent</li> <li>Debouncing: Reconciler subscribes to index events, starts debounce timer to batch rapid changes</li> <li>Reconciliation Trigger: After quiet period, Reconciler publishes ReconciliationTriggeredEvent</li> <li>Orchestration Start: Executor subscribes to ReconciliationTriggeredEvent and publishes ReconciliationStartedEvent for observability</li> <li>Template Rendering: Renderer component subscribes to ReconciliationTriggeredEvent, queries indexed resources, renders templates using pkg/templating, and publishes TemplateRenderedEvent with rendered configuration and auxiliary files</li> <li>Validation: HAProxyValidator component subscribes to TemplateRenderedEvent, performs two-phase validation (syntax with client-native parser, semantics with haproxy binary via pkg/dataplane), and publishes ValidationCompletedEvent or ValidationFailedEvent</li> <li>Deployment Scheduling: DeploymentScheduler subscribes to ValidationCompletedEvent, enforces minimum deployment interval (default 2s) for rate limiting, implements \"latest wins\" queueing if deployment is in progress, and publishes DeploymentScheduledEvent when ready</li> <li>Deployment Execution: Deployer component subscribes to DeploymentScheduledEvent, executes parallel deployments to all discovered HAProxy endpoints using pkg/dataplane, publishes InstanceDeployedEvent for each instance and DeploymentCompletedEvent when all complete</li> <li>Completion: Executor subscribes to DeploymentCompletedEvent and publishes ReconciliationCompletedEvent with duration metrics</li> </ol> <p>All coordination happens via EventBus pub/sub. Components are fully event-driven with no direct function calls between them, enabling clean separation of concerns and independent testability.</p>"},{"location":"development/design/sequence-diagrams/#configuration-validation-process","title":"Configuration Validation Process","text":"<pre><code>sequenceDiagram\n    participant EventBus\n    participant Validator as HAProxy&lt;br/&gt;Validator\n    participant Parser as client-native Parser\n    participant Binary as haproxy Binary\n    participant EventBus2 as EventBus\n\n    EventBus-&gt;&gt;Validator: TemplateRenderedEvent\n    Note over Validator: Extract config and&lt;br/&gt;auxiliary files from event\n\n    Validator-&gt;&gt;Validator: Acquire validation mutex\n    Note over Validator: Single-threaded validation\n\n    Validator-&gt;&gt;Parser: ParseConfiguration(config)\n\n    alt Syntax Error\n        Parser--&gt;&gt;Validator: Parse error\n        Validator-&gt;&gt;EventBus2: Publish(ValidationFailedEvent)\n    else Valid Syntax\n        Parser--&gt;&gt;Validator: Parsed structure\n\n        Validator-&gt;&gt;Binary: Execute haproxy -c -f config\n        Note over Binary: Write aux files to directories&lt;br/&gt;Validate with -c flag\n\n        alt Semantic Error\n            Binary--&gt;&gt;Validator: Exit code 1 + error msg\n            Validator-&gt;&gt;EventBus2: Publish(ValidationFailedEvent)\n        else Valid Config\n            Binary--&gt;&gt;Validator: Exit code 0\n            Validator-&gt;&gt;EventBus2: Publish(ValidationCompletedEvent)\n        end\n    end</code></pre> <p>Validation Steps:</p> <ol> <li>Event Subscription: HAProxyValidator component subscribes to TemplateRenderedEvent and receives rendered configuration and auxiliary files</li> <li>Event-driven trigger - no direct function calls from Renderer</li> <li> <p>Decouples rendering from validation</p> </li> <li> <p>Mutex Acquisition: Acquire validation mutex to ensure single-threaded validation</p> </li> <li>Prevents concurrent writes to HAProxy directories</li> <li> <p>Ensures consistent validation state</p> </li> <li> <p>Syntax Validation: client-native library (pkg/dataplane) parses config structure</p> </li> <li>Checks grammar and syntax rules</li> <li>Validates section structure</li> <li> <p>Returns parsing errors if invalid</p> </li> <li> <p>Semantic Validation: haproxy binary performs full validation</p> </li> <li>Writes auxiliary files to configured HAProxy directories (maps, certs, general files)</li> <li>Writes main configuration to configured path</li> <li>Executes <code>haproxy -c -f /etc/haproxy/haproxy.cfg</code></li> <li>Checks resource availability (files referenced in config must exist)</li> <li>Validates directive combinations</li> <li>Verifies configuration coherence</li> <li> <p>Returns detailed error messages if invalid</p> </li> <li> <p>Event Publishing: Validator publishes ValidationCompletedEvent or ValidationFailedEvent</p> </li> <li>Other components (Executor, DeploymentScheduler) subscribe to these events</li> <li>Event-driven coordination continues the reconciliation workflow</li> </ol>"},{"location":"development/design/sequence-diagrams/#zero-reload-deployment-strategy","title":"Zero-Reload Deployment Strategy","text":"<pre><code>sequenceDiagram\n    participant Sync as Synchronizer\n    participant Client as Dataplane Client\n    participant DP as Dataplane API\n    participant HAProxy\n\n    Sync-&gt;&gt;Client: DeployConfiguration(new_config)\n    Client-&gt;&gt;DP: GET /configuration (current)\n    DP--&gt;&gt;Client: Current config\n\n    Client-&gt;&gt;Client: Compare structures\n\n    alt Only runtime changes\n        Note over Client: Servers, maps, ACLs only\n        Client-&gt;&gt;DP: POST /runtime/servers\n        DP-&gt;&gt;HAProxy: Runtime API command\n        HAProxy--&gt;&gt;DP: Updated\n        DP--&gt;&gt;Client: Success (no reload)\n    else Mixed changes\n        Note over Client: Runtime + config changes\n        Client-&gt;&gt;DP: POST /runtime/servers\n        Client-&gt;&gt;DP: POST /configuration\n        DP-&gt;&gt;HAProxy: Apply config\n        HAProxy--&gt;&gt;DP: Reload triggered\n        DP--&gt;&gt;Client: Success (reload)\n    else Structural changes\n        Note over Client: Backends, frontends, etc.\n        Client-&gt;&gt;DP: POST /configuration\n        DP-&gt;&gt;HAProxy: Replace config + reload\n        HAProxy--&gt;&gt;DP: Reload complete\n        DP--&gt;&gt;Client: Success (reload)\n    end\n\n    Client--&gt;&gt;Sync: DeploymentResult</code></pre> <p>Deployment Optimization:</p> <p>The synchronizer analyzes configuration changes to determine the optimal deployment strategy:</p> <ol> <li>Runtime-Only Updates: Server additions/removals, map updates, ACL changes \u2192 No reload</li> <li>Mixed Updates: Apply runtime changes first, then config changes \u2192 Single reload</li> <li>Structural Updates: Backend/frontend changes \u2192 Full reload required</li> </ol> <p>This minimizes service disruption by avoiding unnecessary HAProxy process reloads.</p>"},{"location":"operations/debugging/","title":"Debugging Guide","text":"<p>This guide explains how to debug and troubleshoot the HAProxy Template Ingress Controller using built-in introspection and debugging tools.</p>"},{"location":"operations/debugging/#overview","title":"Overview","text":"<p>The controller provides a debug HTTP server that exposes internal state, event history, and Go profiling endpoints. These tools help you understand controller behavior without requiring log analysis.</p> <p>Key features: - Real-time controller state inspection - Event history for tracking controller activity - JSONPath field selection for targeted queries - Go profiling for performance analysis</p>"},{"location":"operations/debugging/#enabling-debug-endpoints","title":"Enabling Debug Endpoints","text":"<p>The debug HTTP server is configured via Helm values:</p> <pre><code># values.yaml\ncontroller:\n  debugPort: 6060  # Set to 0 to disable\n</code></pre> <p>By default, the debug port is 6060. The server binds to all interfaces (<code>0.0.0.0</code>) for kubectl port-forward compatibility.</p>"},{"location":"operations/debugging/#accessing-debug-endpoints","title":"Accessing Debug Endpoints","text":""},{"location":"operations/debugging/#from-outside-the-cluster","title":"From Outside the Cluster","text":"<p>Use kubectl port-forward to access the debug server:</p> <pre><code># Forward debug port from controller pod\nkubectl port-forward -n &lt;namespace&gt; deployment/haproxy-template-ic 6060:6060\n\n# Access endpoints\ncurl http://localhost:6060/debug/vars\n</code></pre>"},{"location":"operations/debugging/#from-inside-the-cluster","title":"From Inside the Cluster","text":"<p>Access directly via pod IP or service:</p> <pre><code># Get pod IP\nPOD_IP=$(kubectl get pod -n &lt;namespace&gt; &lt;pod-name&gt; -o jsonpath='{.status.podIP}')\ncurl http://${POD_IP}:6060/debug/vars\n</code></pre>"},{"location":"operations/debugging/#debug-variables","title":"Debug Variables","text":""},{"location":"operations/debugging/#list-all-variables","title":"List All Variables","text":"<pre><code>curl http://localhost:6060/debug/vars\n</code></pre> <p>Returns a list of all available debug variable paths:</p> <pre><code>{\n  \"vars\": [\n    \"config\",\n    \"credentials\",\n    \"rendered\",\n    \"auxfiles\",\n    \"resources\",\n    \"events\",\n    \"state\",\n    \"uptime\"\n  ]\n}\n</code></pre>"},{"location":"operations/debugging/#configuration-state","title":"Configuration State","text":"<p>Get the current controller configuration:</p> <pre><code># Full configuration\ncurl http://localhost:6060/debug/vars/config\n\n# Just the version\ncurl 'http://localhost:6060/debug/vars/config?field={.version}'\n\n# Template names\ncurl 'http://localhost:6060/debug/vars/config?field={.config.templates}'\n</code></pre> <p>Response: <pre><code>{\n  \"config\": {\n    \"templates\": {\n      \"main\": \"global\\n  maxconn {{ maxconn }}\\n...\"\n    },\n    \"watched_resources\": [...]\n  },\n  \"version\": \"12345\",\n  \"updated\": \"2025-01-15T10:30:45Z\"\n}\n</code></pre></p>"},{"location":"operations/debugging/#rendered-haproxy-config","title":"Rendered HAProxy Config","text":"<p>Get the most recently rendered HAProxy configuration:</p> <pre><code># Full rendered config\ncurl http://localhost:6060/debug/vars/rendered\n\n# Just the config text (useful for saving to file)\ncurl 'http://localhost:6060/debug/vars/rendered?field={.config}' | jq -r '.'\n\n# Config size and timestamp\ncurl 'http://localhost:6060/debug/vars/rendered?field={.size}'\ncurl 'http://localhost:6060/debug/vars/rendered?field={.timestamp}'\n</code></pre> <p>Response: <pre><code>{\n  \"config\": \"global\\n  maxconn 2000\\n  log stdout local0\\n\\ndefaults\\n...\",\n  \"timestamp\": \"2025-01-15T10:30:45Z\",\n  \"size\": 4567\n}\n</code></pre></p>"},{"location":"operations/debugging/#resource-counts","title":"Resource Counts","text":"<p>Get counts of watched Kubernetes resources:</p> <pre><code># All resource counts\ncurl http://localhost:6060/debug/vars/resources\n\n# Specific resource type\ncurl 'http://localhost:6060/debug/vars/resources?field={.ingresses}'\n</code></pre> <p>Response: <pre><code>{\n  \"ingresses\": 5,\n  \"services\": 12,\n  \"haproxy-pods\": 2\n}\n</code></pre></p>"},{"location":"operations/debugging/#auxiliary-files","title":"Auxiliary Files","text":"<p>Get SSL certificates, map files, and general files used in the last deployment:</p> <pre><code>curl http://localhost:6060/debug/vars/auxfiles\n</code></pre> <p>Response: <pre><code>{\n  \"files\": {\n    \"ssl_certificates\": [\n      {\n        \"name\": \"tls-cert\",\n        \"path\": \"/etc/haproxy/ssl/tls-cert.pem\"\n      }\n    ],\n    \"map_files\": [...],\n    \"general_files\": [...]\n  },\n  \"timestamp\": \"2025-01-15T10:30:45Z\",\n  \"summary\": {\n    \"ssl_count\": 2,\n    \"map_count\": 1,\n    \"general_count\": 3\n  }\n}\n</code></pre></p>"},{"location":"operations/debugging/#event-history","title":"Event History","text":"<p>Get recent controller events:</p> <pre><code># Last 100 events (default)\ncurl http://localhost:6060/debug/vars/events\n</code></pre> <p>Response: <pre><code>[\n  {\n    \"timestamp\": \"2025-01-15T10:30:45Z\",\n    \"type\": \"config.validated\",\n    \"summary\": \"config.validated\"\n  },\n  {\n    \"timestamp\": \"2025-01-15T10:30:46Z\",\n    \"type\": \"reconciliation.triggered\",\n    \"summary\": \"reconciliation.triggered\"\n  }\n]\n</code></pre></p>"},{"location":"operations/debugging/#full-state-dump","title":"Full State Dump","text":"<p>Get all controller state in a single response:</p> <pre><code>curl http://localhost:6060/debug/vars/state\n</code></pre> <p>Warning</p> <p>The full state dump can return very large responses. Prefer specific variables or JSONPath field selection for production debugging.</p>"},{"location":"operations/debugging/#controller-uptime","title":"Controller Uptime","text":"<pre><code>curl http://localhost:6060/debug/vars/uptime\n</code></pre>"},{"location":"operations/debugging/#jsonpath-field-selection","title":"JSONPath Field Selection","text":"<p>All debug endpoints support JSONPath field selection using kubectl-style syntax:</p> <pre><code># Basic field access\ncurl 'http://localhost:6060/debug/vars/config?field={.version}'\n\n# Nested field access\ncurl 'http://localhost:6060/debug/vars/config?field={.config.templates.main}'\n\n# Array indexing\ncurl 'http://localhost:6060/debug/vars/events?field={[0]}'\n</code></pre> <p>See Kubernetes JSONPath documentation for full syntax.</p>"},{"location":"operations/debugging/#go-profiling","title":"Go Profiling","text":"<p>The debug server includes Go pprof endpoints for performance analysis:</p> <pre><code># CPU profile (30 second sample)\ncurl http://localhost:6060/debug/pprof/profile?seconds=30 &gt; cpu.pprof\n\n# Heap profile\ncurl http://localhost:6060/debug/pprof/heap &gt; heap.pprof\n\n# Goroutine dump\ncurl http://localhost:6060/debug/pprof/goroutine?debug=1\n\n# Block profile\ncurl http://localhost:6060/debug/pprof/block &gt; block.pprof\n\n# Mutex profile\ncurl http://localhost:6060/debug/pprof/mutex &gt; mutex.pprof\n\n# All profiles summary\ncurl http://localhost:6060/debug/pprof/\n</code></pre> <p>Analyze profiles with <code>go tool pprof</code>:</p> <pre><code>go tool pprof -http=:8080 cpu.pprof\n</code></pre>"},{"location":"operations/debugging/#common-debugging-workflows","title":"Common Debugging Workflows","text":""},{"location":"operations/debugging/#configuration-not-loading","title":"Configuration Not Loading","text":"<ol> <li> <p>Check if config is loaded:    <pre><code>curl http://localhost:6060/debug/vars/config\n</code></pre></p> </li> <li> <p>If error \"config not loaded yet\", check controller logs for parsing errors:    <pre><code>kubectl logs -n &lt;namespace&gt; deployment/haproxy-template-ic | grep -i error\n</code></pre></p> </li> <li> <p>Verify HAProxyTemplateConfig exists:    <pre><code>kubectl get haproxytemplateconfig -n &lt;namespace&gt;\n</code></pre></p> </li> </ol>"},{"location":"operations/debugging/#haproxy-config-not-updating","title":"HAProxy Config Not Updating","text":"<ol> <li> <p>Check rendered config timestamp:    <pre><code>curl 'http://localhost:6060/debug/vars/rendered?field={.timestamp}'\n</code></pre></p> </li> <li> <p>Check recent events for reconciliation activity:    <pre><code>curl http://localhost:6060/debug/vars/events | jq '.[] | select(.type | contains(\"reconciliation\"))'\n</code></pre></p> </li> <li> <p>Verify resource counts match expected:    <pre><code>curl http://localhost:6060/debug/vars/resources\n</code></pre></p> </li> </ol>"},{"location":"operations/debugging/#template-rendering-errors","title":"Template Rendering Errors","text":"<ol> <li> <p>Check if config is valid:    <pre><code>curl 'http://localhost:6060/debug/vars/config?field={.version}'\n</code></pre></p> </li> <li> <p>Look for template-related events:    <pre><code>curl http://localhost:6060/debug/vars/events | jq '.[] | select(.type | contains(\"template\"))'\n</code></pre></p> </li> <li> <p>Try rendering with debug output (see Templating Guide)</p> </li> </ol>"},{"location":"operations/debugging/#memory-issues","title":"Memory Issues","text":"<ol> <li> <p>Get current heap usage:    <pre><code>curl http://localhost:6060/debug/pprof/heap?debug=1 | head -20\n</code></pre></p> </li> <li> <p>Generate heap profile for analysis:    <pre><code>curl http://localhost:6060/debug/pprof/heap &gt; heap.pprof\ngo tool pprof -top heap.pprof\n</code></pre></p> </li> <li> <p>Check resource counts (large counts may indicate memory pressure):    <pre><code>curl http://localhost:6060/debug/vars/resources\n</code></pre></p> </li> </ol>"},{"location":"operations/debugging/#high-cpu-usage","title":"High CPU Usage","text":"<ol> <li> <p>Generate CPU profile:    <pre><code>curl http://localhost:6060/debug/pprof/profile?seconds=30 &gt; cpu.pprof\n</code></pre></p> </li> <li> <p>Analyze hot spots:    <pre><code>go tool pprof -top cpu.pprof\n</code></pre></p> </li> <li> <p>Check reconciliation frequency in events:    <pre><code>curl http://localhost:6060/debug/vars/events | jq '[.[] | select(.type == \"reconciliation.triggered\")] | length'\n</code></pre></p> </li> </ol>"},{"location":"operations/debugging/#deployment-failures","title":"Deployment Failures","text":"<ol> <li> <p>Check recent events for deployment status:    <pre><code>curl http://localhost:6060/debug/vars/events | jq '.[] | select(.type | contains(\"deployment\"))'\n</code></pre></p> </li> <li> <p>Verify HAProxy pods are discovered:    <pre><code>curl 'http://localhost:6060/debug/vars/resources?field={.\"haproxy-pods\"}'\n</code></pre></p> </li> <li> <p>Check rendered config for syntax errors:    <pre><code>curl 'http://localhost:6060/debug/vars/rendered?field={.config}' &gt; haproxy.cfg\nhaproxy -c -f haproxy.cfg\n</code></pre></p> </li> </ol>"},{"location":"operations/debugging/#security-considerations","title":"Security Considerations","text":"<ol> <li> <p>Credentials: The <code>/debug/vars/credentials</code> endpoint exposes only metadata, NOT actual passwords</p> </li> <li> <p>Access Control: Debug endpoints have no built-in authentication - use kubectl port-forward or network policies to restrict access</p> </li> <li> <p>Large Responses: Full state dump can expose sensitive configuration - use specific variables instead</p> </li> <li> <p>Production Usage: Consider disabling debug port in high-security environments:    <pre><code>controller:\n  debugPort: 0  # Disabled\n</code></pre></p> </li> </ol>"},{"location":"operations/debugging/#scripting-examples","title":"Scripting Examples","text":""},{"location":"operations/debugging/#monitor-reconciliation-activity","title":"Monitor Reconciliation Activity","text":"<pre><code>#!/bin/bash\n# Watch reconciliation events\nwhile true; do\n    count=$(curl -s http://localhost:6060/debug/vars/events | \\\n            jq '[.[] | select(.type == \"reconciliation.completed\")] | length')\n    echo \"$(date): $count reconciliations completed\"\n    sleep 10\ndone\n</code></pre>"},{"location":"operations/debugging/#export-rendered-config","title":"Export Rendered Config","text":"<pre><code>#!/bin/bash\n# Export current HAProxy config with timestamp\nTIMESTAMP=$(curl -s 'http://localhost:6060/debug/vars/rendered?field={.timestamp}' | jq -r '.')\ncurl -s 'http://localhost:6060/debug/vars/rendered?field={.config}' | jq -r '.' &gt; \"haproxy-${TIMESTAMP}.cfg\"\necho \"Exported to haproxy-${TIMESTAMP}.cfg\"\n</code></pre>"},{"location":"operations/debugging/#health-check-script","title":"Health Check Script","text":"<pre><code>#!/bin/bash\n# Check controller health via debug endpoints\nset -e\n\nCONFIG_VERSION=$(curl -s 'http://localhost:6060/debug/vars/config?field={.version}' 2&gt;/dev/null || echo \"error\")\nUPTIME=$(curl -s http://localhost:6060/debug/vars/uptime 2&gt;/dev/null || echo \"error\")\nRESOURCES=$(curl -s http://localhost:6060/debug/vars/resources 2&gt;/dev/null || echo \"{}\")\n\necho \"Config Version: $CONFIG_VERSION\"\necho \"Uptime: $UPTIME\"\necho \"Resources: $RESOURCES\"\n\nif [ \"$CONFIG_VERSION\" = \"error\" ]; then\n    echo \"ERROR: Cannot access debug endpoints\"\n    exit 1\nfi\n</code></pre>"},{"location":"operations/debugging/#see-also","title":"See Also","text":"<ul> <li>Monitoring Guide - Prometheus metrics and alerting</li> <li>High Availability - Leader election and failover</li> <li>Troubleshooting Guide - General troubleshooting</li> <li>Templating Guide - Template debugging</li> </ul>"},{"location":"operations/high-availability/","title":"High Availability with Leader Election","text":"<p>This guide explains how to deploy and operate the HAProxy Template Ingress Controller in high availability (HA) mode with multiple replicas.</p>"},{"location":"operations/high-availability/#overview","title":"Overview","text":"<p>The controller supports running multiple replicas for high availability using leader election based on Kubernetes Leases. Only the elected leader performs write operations (deploying configurations to HAProxy), while all replicas continue watching resources, rendering templates, and validating configurations to maintain \"hot standby\" status.</p> <p>Benefits of HA deployment: - Zero-downtime during controller upgrades (rolling updates) - Automatic failover if leader pod crashes (~15-20 seconds) - All replicas ready to take over immediately (hot standby) - Balanced leader distribution across nodes</p> <p>How it works: 1. All replicas watch Kubernetes resources and render HAProxy configurations 2. Leader election determines which replica can deploy configs to HAProxy 3. When leader fails, followers automatically elect a new leader 4. Leadership transitions are logged and tracked via Prometheus metrics</p>"},{"location":"operations/high-availability/#configuration","title":"Configuration","text":""},{"location":"operations/high-availability/#enable-leader-election","title":"Enable Leader Election","text":"<p>Leader election is enabled by default when deploying with 2+ replicas via Helm:</p> <pre><code># values.yaml (defaults)\nreplicaCount: 2  # Run 2 replicas for HA\n\ncontroller:\n  config:\n    controller:\n      leader_election:\n        enabled: true\n        lease_name: haproxy-template-ic-leader\n        lease_duration: 60s    # Failover happens within this time\n        renew_deadline: 15s    # Leader tries to renew for this long\n        retry_period: 5s       # Interval between renewal attempts\n</code></pre>"},{"location":"operations/high-availability/#disable-leader-election","title":"Disable Leader Election","text":"<p>For development or single-replica deployments:</p> <pre><code># values.yaml\nreplicaCount: 1\n\ncontroller:\n  config:\n    controller:\n      leader_election:\n        enabled: false  # Disabled in single-replica mode\n</code></pre>"},{"location":"operations/high-availability/#timing-parameters","title":"Timing Parameters","text":"<p>The timing parameters control failover speed and tolerance:</p> Parameter Default Purpose Recommendations <code>lease_duration</code> 60s Max time followers wait before taking over Increase for flaky networks (120s) <code>renew_deadline</code> 15s How long leader retries before giving up Should be &lt; <code>lease_duration</code> (1/4 ratio) <code>retry_period</code> 5s Interval between leader renewal attempts Should be &lt; <code>renew_deadline</code> (1/3 ratio) <p>Failover time calculation: <pre><code>Worst-case failover = lease_duration + renew_deadline\nDefault failover    = 60s + 15s = 75s (but typically 15-20s)\n</code></pre></p> <p>Clock skew tolerance: <pre><code>Skew tolerance = lease_duration - renew_deadline\nDefault        = 60s - 15s = 45s (handles up to 4x clock differences)\n</code></pre></p>"},{"location":"operations/high-availability/#deployment","title":"Deployment","text":""},{"location":"operations/high-availability/#standard-ha-deployment","title":"Standard HA Deployment","text":"<p>Deploy with 2-3 replicas (default Helm configuration):</p> <pre><code>helm install haproxy-ic charts/haproxy-template-ic \\\n  --set replicaCount=2\n</code></pre>"},{"location":"operations/high-availability/#scaling","title":"Scaling","text":"<p>Scale the deployment dynamically:</p> <pre><code># Scale to 3 replicas\nkubectl scale deployment haproxy-template-ic --replicas=3\n\n# Scale back to 2\nkubectl scale deployment haproxy-template-ic --replicas=2\n</code></pre>"},{"location":"operations/high-availability/#rbac-requirements","title":"RBAC Requirements","text":"<p>The controller requires these additional permissions for leader election:</p> <pre><code>apiGroups: [\"coordination.k8s.io\"]\nresources: [\"leases\"]\nverbs: [\"get\", \"create\", \"update\"]\n</code></pre> <p>These are automatically configured in the Helm chart's ClusterRole.</p>"},{"location":"operations/high-availability/#monitoring-leadership","title":"Monitoring Leadership","text":""},{"location":"operations/high-availability/#check-current-leader","title":"Check Current Leader","text":"<pre><code># View Lease resource\nkubectl get lease -n &lt;namespace&gt; haproxy-template-ic-leader -o yaml\n\n# Output shows current leader:\n# spec:\n#   holderIdentity: haproxy-template-ic-7d9f8b4c6d-abc12\n</code></pre>"},{"location":"operations/high-availability/#view-leadership-status-in-logs","title":"View Leadership Status in Logs","text":"<pre><code># Leader logs show:\nkubectl logs -n &lt;namespace&gt; deployment/haproxy-template-ic | grep -E \"leader|election\"\n\n# Example output:\n# level=INFO msg=\"Leader election started\" identity=pod-abc12 lease=haproxy-template-ic-leader\n# level=INFO msg=\"\ud83c\udf96\ufe0f  Became leader\" identity=pod-abc12 transition_count=1\n</code></pre>"},{"location":"operations/high-availability/#prometheus-metrics","title":"Prometheus Metrics","text":"<p>Monitor leader election via metrics endpoint:</p> <pre><code>kubectl port-forward -n &lt;namespace&gt; deployment/haproxy-template-ic 9090:9090\ncurl http://localhost:9090/metrics | grep leader_election\n</code></pre> <p>Key metrics:</p> <pre><code># Current leader (should be 1 across all replicas)\nsum(haproxy_ic_leader_election_is_leader)\n\n# Identify which pod is leader\nhaproxy_ic_leader_election_is_leader{pod=~\".*\"} == 1\n\n# Leadership transition rate (should be low)\nrate(haproxy_ic_leader_election_transitions_total[1h])\n</code></pre>"},{"location":"operations/high-availability/#troubleshooting","title":"Troubleshooting","text":""},{"location":"operations/high-availability/#no-leader-elected","title":"No Leader Elected","text":"<p>Symptoms: - No deployments happening - All replicas show <code>is_leader=0</code> - Logs show constant election failures</p> <p>Common causes:</p> <ol> <li> <p>Missing RBAC permissions: <pre><code>kubectl auth can-i get leases --as=system:serviceaccount:&lt;namespace&gt;:haproxy-template-ic\nkubectl auth can-i create leases --as=system:serviceaccount:&lt;namespace&gt;:haproxy-template-ic\nkubectl auth can-i update leases --as=system:serviceaccount:&lt;namespace&gt;:haproxy-template-ic\n</code></pre></p> </li> <li> <p>Missing environment variables: <pre><code>kubectl get pod &lt;pod-name&gt; -o yaml | grep -A2 \"POD_NAME\\|POD_NAMESPACE\"\n\n# Should show:\n# - name: POD_NAME\n#   valueFrom:\n#     fieldRef:\n#       fieldPath: metadata.name\n</code></pre></p> </li> <li> <p>API server connectivity: <pre><code>kubectl logs &lt;pod-name&gt; | grep \"connection refused\\|timeout\"\n</code></pre></p> </li> </ol>"},{"location":"operations/high-availability/#multiple-leaders-split-brain","title":"Multiple Leaders (Split-Brain)","text":"<p>Symptoms: - <code>sum(haproxy_ic_leader_election_is_leader) &gt; 1</code> - Multiple pods deploying configs simultaneously - Conflicting deployments in HAProxy</p> <p>This should never happen with proper Kubernetes Lease implementation. If it does:</p> <ol> <li> <p>Check for severe clock skew between nodes:    <pre><code># On each node\ntimedatectl status\n</code></pre></p> </li> <li> <p>Verify Kubernetes API server health:    <pre><code>kubectl get --raw /healthz\n</code></pre></p> </li> <li> <p>Restart all controller pods:    <pre><code>kubectl rollout restart deployment haproxy-template-ic\n</code></pre></p> </li> </ol>"},{"location":"operations/high-availability/#frequent-leadership-changes","title":"Frequent Leadership Changes","text":"<p>Symptoms: - <code>rate(haproxy_ic_leader_election_transitions_total[1h]) &gt; 5</code> - Logs show frequent \"Lost leadership\" / \"Became leader\" messages - Deployments failing intermittently</p> <p>Common causes:</p> <ol> <li>Resource contention - Leader pod can't renew lease in time:    <pre><code>kubectl top pods -n &lt;namespace&gt;\nkubectl describe pod &lt;leader-pod&gt; | grep -A10 \"Limits\\|Requests\"\n</code></pre></li> </ol> <p>Solution: Increase CPU/memory limits</p> <ol> <li>Network issues - API server communication delays:    <pre><code>kubectl logs &lt;pod-name&gt; | grep \"lease renew\\|deadline\"\n</code></pre></li> </ol> <p>Solution: Increase <code>lease_duration</code> and <code>renew_deadline</code></p> <ol> <li>Node issues - Leader pod node experiencing problems:    <pre><code>kubectl describe node &lt;node-name&gt;\n</code></pre></li> </ol> <p>Solution: Drain and investigate node</p>"},{"location":"operations/high-availability/#leader-not-deploying","title":"Leader Not Deploying","text":"<p>Symptoms: - One replica shows <code>is_leader=1</code> - No deployment errors in logs - HAProxy configs not updating</p> <p>Diagnosis:</p> <pre><code># Check leader logs for deployment activity\nkubectl logs &lt;leader-pod&gt; | grep -i \"deploy\"\n\n# Verify leader-only components started\nkubectl logs &lt;leader-pod&gt; | grep \"Started.*Deployer\\|DeploymentScheduler\"\n</code></pre> <p>Common causes: - Deployment components failed to start (check logs for errors) - Rate limiting preventing deployment (check drift prevention interval) - HAProxy instances unreachable (check network connectivity)</p>"},{"location":"operations/high-availability/#best-practices","title":"Best Practices","text":""},{"location":"operations/high-availability/#replica-count","title":"Replica Count","text":"<p>Development: - 1 replica with <code>leader_election.enabled: false</code></p> <p>Staging: - 2 replicas with leader election enabled</p> <p>Production: - 2-3 replicas across multiple availability zones - Enable PodDisruptionBudget:   <pre><code>podDisruptionBudget:\n  enabled: true\n  minAvailable: 1\n</code></pre></p>"},{"location":"operations/high-availability/#resource-allocation","title":"Resource Allocation","text":"<p>Allocate sufficient resources for hot standby:</p> <pre><code>resources:\n  requests:\n    cpu: 100m\n    memory: 128Mi\n  limits:\n    cpu: 500m      # Allow bursts during leader work\n    memory: 512Mi\n</code></pre> <p>All replicas perform the same work (watching, rendering, validating), so resource usage is similar.</p>"},{"location":"operations/high-availability/#anti-affinity","title":"Anti-Affinity","text":"<p>Distribute replicas across nodes for better availability:</p> <pre><code>affinity:\n  podAntiAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchLabels:\n              app.kubernetes.io/name: haproxy-template-ic\n          topologyKey: kubernetes.io/hostname\n</code></pre>"},{"location":"operations/high-availability/#monitoring-and-alerts","title":"Monitoring and Alerts","text":"<p>Set up Prometheus alerts for leader election health:</p> <pre><code>groups:\n  - name: haproxy-ic-leader-election\n    rules:\n      # No leader\n      - alert: NoLeaderElected\n        expr: sum(haproxy_ic_leader_election_is_leader) &lt; 1\n        for: 1m\n        annotations:\n          summary: \"No HAProxy controller leader elected\"\n\n      # Multiple leaders (split-brain)\n      - alert: MultipleLeaders\n        expr: sum(haproxy_ic_leader_election_is_leader) &gt; 1\n        annotations:\n          summary: \"Multiple HAProxy controller leaders detected (split-brain)\"\n\n      # Frequent transitions\n      - alert: FrequentLeadershipChanges\n        expr: rate(haproxy_ic_leader_election_transitions_total[1h]) &gt; 5\n        for: 15m\n        annotations:\n          summary: \"HAProxy controller experiencing frequent leadership changes\"\n</code></pre>"},{"location":"operations/high-availability/#migration-from-single-replica","title":"Migration from Single-Replica","text":"<p>To migrate an existing single-replica deployment to HA:</p> <ol> <li> <p>Verify RBAC permissions (Helm chart updates this automatically)</p> </li> <li> <p>Update values.yaml: <pre><code>replicaCount: 2\ncontroller:\n  config:\n    controller:\n      leader_election:\n        enabled: true\n</code></pre></p> </li> <li> <p>Upgrade with Helm: <pre><code>helm upgrade haproxy-ic charts/haproxy-template-ic \\\n  --reuse-values \\\n  -f new-values.yaml\n</code></pre></p> </li> <li> <p>Verify leadership: <pre><code>kubectl logs -f deployment/haproxy-template-ic | grep leader\n</code></pre></p> </li> <li> <p>Confirm one leader: <pre><code>kubectl get pods -l app.kubernetes.io/name=haproxy-template-ic \\\n  -o custom-columns=NAME:.metadata.name,LEADER:.status.podIP\n\n# Check metrics to identify leader\nfor pod in $(kubectl get pods -l app.kubernetes.io/name=haproxy-template-ic -o name); do\n  echo \"$pod:\"\n  kubectl exec $pod -- wget -qO- localhost:9090/metrics | grep is_leader\ndone\n</code></pre></p> </li> </ol>"},{"location":"operations/high-availability/#see-also","title":"See Also","text":"<ul> <li>Leader Election Design - Architecture and implementation details</li> <li>Monitoring Guide - Prometheus metrics and alerting</li> <li>Debugging Guide - Runtime introspection and troubleshooting</li> <li>Security Guide - RBAC and security best practices</li> <li>Performance Guide - Resource sizing and optimization</li> <li>Troubleshooting Guide - General troubleshooting</li> </ul>"},{"location":"operations/monitoring/","title":"Monitoring Guide","text":"<p>This guide explains how to monitor the HAProxy Template Ingress Controller using Prometheus metrics, including setup, key metrics, alerting, and dashboards.</p>"},{"location":"operations/monitoring/#overview","title":"Overview","text":"<p>The controller exposes Prometheus metrics via an HTTP endpoint, providing visibility into reconciliation performance, deployment status, resource counts, and leader election state.</p> <p>Key monitoring areas: - Reconciliation cycle performance and errors - HAProxy deployment latency and success rates - Configuration validation status - Kubernetes resource counts - Leader election for HA deployments</p>"},{"location":"operations/monitoring/#enabling-metrics","title":"Enabling Metrics","text":"<p>Metrics are enabled by default. Configure the metrics port in Helm values:</p> <pre><code># values.yaml\ncontroller:\n  config:\n    controller:\n      metrics_port: 9090  # Default\n</code></pre> <p>Set to <code>0</code> to disable metrics.</p>"},{"location":"operations/monitoring/#accessing-metrics","title":"Accessing Metrics","text":""},{"location":"operations/monitoring/#prometheus-scrape-configuration","title":"Prometheus Scrape Configuration","text":"<p>Add a scrape config for the controller:</p> <pre><code>scrape_configs:\n  - job_name: 'haproxy-template-ic'\n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]\n        regex: haproxy-template-ic\n        action: keep\n      - source_labels: [__meta_kubernetes_pod_container_port_number]\n        regex: \"9090\"\n        action: keep\n</code></pre>"},{"location":"operations/monitoring/#servicemonitor-prometheus-operator","title":"ServiceMonitor (Prometheus Operator)","text":"<p>If using Prometheus Operator, enable the ServiceMonitor in Helm:</p> <pre><code># values.yaml\nserviceMonitor:\n  enabled: true\n  interval: 30s\n  labels:\n    release: prometheus  # Match your Prometheus selector\n</code></pre>"},{"location":"operations/monitoring/#manual-access","title":"Manual Access","text":"<pre><code># Port-forward to metrics endpoint\nkubectl port-forward -n &lt;namespace&gt; deployment/haproxy-template-ic 9090:9090\n\n# Fetch metrics\ncurl http://localhost:9090/metrics\n</code></pre>"},{"location":"operations/monitoring/#metrics-reference","title":"Metrics Reference","text":""},{"location":"operations/monitoring/#reconciliation-metrics","title":"Reconciliation Metrics","text":"Metric Type Description <code>haproxy_ic_reconciliation_total</code> Counter Total reconciliation cycles triggered <code>haproxy_ic_reconciliation_duration_seconds</code> Histogram Time spent in reconciliation cycles <code>haproxy_ic_reconciliation_errors_total</code> Counter Failed reconciliation cycles <p>Key queries: <pre><code># Reconciliation rate per second\nrate(haproxy_ic_reconciliation_total[5m])\n\n# Average reconciliation duration\nrate(haproxy_ic_reconciliation_duration_seconds_sum[5m]) /\nrate(haproxy_ic_reconciliation_duration_seconds_count[5m])\n\n# Success rate percentage\n100 * (1 - (\n  rate(haproxy_ic_reconciliation_errors_total[5m]) /\n  rate(haproxy_ic_reconciliation_total[5m])\n))\n</code></pre></p>"},{"location":"operations/monitoring/#deployment-metrics","title":"Deployment Metrics","text":"Metric Type Description <code>haproxy_ic_deployment_total</code> Counter Total deployment attempts <code>haproxy_ic_deployment_duration_seconds</code> Histogram Time spent deploying to HAProxy <code>haproxy_ic_deployment_errors_total</code> Counter Failed deployments <p>Key queries: <pre><code># Deployment rate\nrate(haproxy_ic_deployment_total[5m])\n\n# 95th percentile deployment latency\nhistogram_quantile(0.95, rate(haproxy_ic_deployment_duration_seconds_bucket[5m]))\n\n# Deployment success rate\n100 * (1 - (\n  rate(haproxy_ic_deployment_errors_total[5m]) /\n  rate(haproxy_ic_deployment_total[5m])\n))\n</code></pre></p>"},{"location":"operations/monitoring/#validation-metrics","title":"Validation Metrics","text":"Metric Type Description <code>haproxy_ic_validation_total</code> Counter Total validation attempts <code>haproxy_ic_validation_errors_total</code> Counter Failed validations <p>Key queries: <pre><code># Validation rate\nrate(haproxy_ic_validation_total[5m])\n\n# Validation success rate\n100 * (1 - (\n  rate(haproxy_ic_validation_errors_total[5m]) /\n  rate(haproxy_ic_validation_total[5m])\n))\n</code></pre></p>"},{"location":"operations/monitoring/#resource-metrics","title":"Resource Metrics","text":"Metric Type Labels Description <code>haproxy_ic_resource_count</code> Gauge <code>type</code> Current count of watched resources <p>Key queries: <pre><code># All resource counts\nhaproxy_ic_resource_count\n\n# Specific resource types\nhaproxy_ic_resource_count{type=\"ingresses\"}\nhaproxy_ic_resource_count{type=\"services\"}\nhaproxy_ic_resource_count{type=\"haproxy-pods\"}\n\n# Resource count changes\ndelta(haproxy_ic_resource_count[1h])\n</code></pre></p>"},{"location":"operations/monitoring/#event-metrics","title":"Event Metrics","text":"Metric Type Description <code>haproxy_ic_event_subscribers</code> Gauge Active event subscribers <code>haproxy_ic_events_published_total</code> Counter Total events published <p>Key queries: <pre><code># Event publishing rate\nrate(haproxy_ic_events_published_total[5m])\n\n# Subscriber count (should be constant)\nhaproxy_ic_event_subscribers\n\n# Subscriber changes (indicates component restarts)\ndelta(haproxy_ic_event_subscribers[5m])\n</code></pre></p>"},{"location":"operations/monitoring/#leader-election-metrics","title":"Leader Election Metrics","text":"Metric Type Description <code>haproxy_ic_leader_election_is_leader</code> Gauge 1 if this replica is leader, 0 otherwise <code>haproxy_ic_leader_election_transitions_total</code> Counter Leadership transitions (gain/loss) <code>haproxy_ic_leader_election_time_as_leader_seconds_total</code> Counter Cumulative time as leader <p>Key queries: <pre><code># Current leader count (should be exactly 1)\nsum(haproxy_ic_leader_election_is_leader)\n\n# Identify leader pod\nhaproxy_ic_leader_election_is_leader == 1\n\n# Leadership transition rate\nrate(haproxy_ic_leader_election_transitions_total[1h])\n\n# Average time as leader per transition\nhaproxy_ic_leader_election_time_as_leader_seconds_total /\nhaproxy_ic_leader_election_transitions_total\n</code></pre></p>"},{"location":"operations/monitoring/#alerting-rules","title":"Alerting Rules","text":""},{"location":"operations/monitoring/#recommended-alerts","title":"Recommended Alerts","text":"<pre><code>groups:\n  - name: haproxy-template-ic\n    rules:\n      # Reconciliation failures\n      - alert: HAProxyICHighReconciliationErrorRate\n        expr: |\n          rate(haproxy_ic_reconciliation_errors_total[5m]) /\n          rate(haproxy_ic_reconciliation_total[5m]) &gt; 0.1\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"High reconciliation error rate (&gt;10%)\"\n          description: \"Controller is failing to reconcile configurations\"\n\n      # Deployment latency\n      - alert: HAProxyICHighDeploymentLatency\n        expr: |\n          histogram_quantile(0.95,\n            rate(haproxy_ic_deployment_duration_seconds_bucket[5m])\n          ) &gt; 5\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"95th percentile deployment latency &gt;5s\"\n          description: \"Deploying configs to HAProxy is taking too long\"\n\n      # Validation failures\n      - alert: HAProxyICValidationFailures\n        expr: |\n          rate(haproxy_ic_validation_errors_total[5m]) &gt; 0\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Configuration validation failing\"\n          description: \"HAProxy configuration has syntax or validation errors\"\n\n      # Component crash\n      - alert: HAProxyICComponentStopped\n        expr: |\n          delta(haproxy_ic_event_subscribers[5m]) &lt; 0\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Event subscriber count decreased\"\n          description: \"A controller component may have crashed\"\n\n      # No leader elected (HA)\n      - alert: HAProxyICNoLeader\n        expr: sum(haproxy_ic_leader_election_is_leader) &lt; 1\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"No HAProxy controller leader elected\"\n          description: \"No controller replica is elected as leader\"\n\n      # Multiple leaders (split-brain)\n      - alert: HAProxyICMultipleLeaders\n        expr: sum(haproxy_ic_leader_election_is_leader) &gt; 1\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Multiple HAProxy controller leaders detected\"\n          description: \"Split-brain condition - multiple replicas think they are leader\"\n\n      # Frequent leadership changes\n      - alert: HAProxyICFrequentLeadershipChanges\n        expr: rate(haproxy_ic_leader_election_transitions_total[1h]) &gt; 5\n        for: 15m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Frequent leadership transitions\"\n          description: \"Controller leadership changing too often, may indicate cluster instability\"\n\n      # No HAProxy pods discovered\n      - alert: HAProxyICNoHAProxyPods\n        expr: haproxy_ic_resource_count{type=\"haproxy-pods\"} &lt; 1\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"No HAProxy pods discovered\"\n          description: \"Controller cannot find any HAProxy pods to manage\"\n</code></pre>"},{"location":"operations/monitoring/#dashboard-examples","title":"Dashboard Examples","text":""},{"location":"operations/monitoring/#grafana-dashboard-queries","title":"Grafana Dashboard Queries","text":"<p>Reconciliation Overview Panel: <pre><code># Success rate (stat panel)\n100 * (1 - (\n  rate(haproxy_ic_reconciliation_errors_total[5m]) /\n  rate(haproxy_ic_reconciliation_total[5m])\n))\n\n# Rate over time (graph)\nrate(haproxy_ic_reconciliation_total[5m])\nrate(haproxy_ic_reconciliation_errors_total[5m])\n</code></pre></p> <p>Deployment Latency Panel: <pre><code># P50, P95, P99 latencies\nhistogram_quantile(0.50, rate(haproxy_ic_deployment_duration_seconds_bucket[5m]))\nhistogram_quantile(0.95, rate(haproxy_ic_deployment_duration_seconds_bucket[5m]))\nhistogram_quantile(0.99, rate(haproxy_ic_deployment_duration_seconds_bucket[5m]))\n</code></pre></p> <p>Resource Count Panel: <pre><code># All resource types\nhaproxy_ic_resource_count\n\n# Stacked area chart by type\nhaproxy_ic_resource_count{type=~\"ingresses|services|endpoints\"}\n</code></pre></p> <p>Leader Election Panel: <pre><code># Current leader indicator\nhaproxy_ic_leader_election_is_leader == 1\n\n# Transition count over time\nincrease(haproxy_ic_leader_election_transitions_total[1h])\n</code></pre></p>"},{"location":"operations/monitoring/#dashboard-json-template","title":"Dashboard JSON Template","text":"<p>A basic Grafana dashboard template can be imported from:</p> <pre><code>{\n  \"title\": \"HAProxy Template IC\",\n  \"panels\": [\n    {\n      \"title\": \"Reconciliation Rate\",\n      \"targets\": [\n        {\"expr\": \"rate(haproxy_ic_reconciliation_total[5m])\"}\n      ]\n    },\n    {\n      \"title\": \"Reconciliation Success Rate\",\n      \"targets\": [\n        {\"expr\": \"100 * (1 - rate(haproxy_ic_reconciliation_errors_total[5m]) / rate(haproxy_ic_reconciliation_total[5m]))\"}\n      ]\n    },\n    {\n      \"title\": \"Deployment Latency\",\n      \"targets\": [\n        {\"expr\": \"histogram_quantile(0.95, rate(haproxy_ic_deployment_duration_seconds_bucket[5m]))\"}\n      ]\n    },\n    {\n      \"title\": \"Resource Counts\",\n      \"targets\": [\n        {\"expr\": \"haproxy_ic_resource_count\"}\n      ]\n    },\n    {\n      \"title\": \"Leader Status\",\n      \"targets\": [\n        {\"expr\": \"haproxy_ic_leader_election_is_leader\"}\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"operations/monitoring/#operational-insights","title":"Operational Insights","text":""},{"location":"operations/monitoring/#key-health-indicators","title":"Key Health Indicators","text":"Indicator Healthy Range Action if Unhealthy Reconciliation success rate &gt;99% Check logs for template/validation errors Deployment success rate &gt;99% Check HAProxy pod connectivity P95 deployment latency &lt;2s Check HAProxy DataPlane API performance Leader count =1 Check HA configuration and network Event subscribers Constant Restart controller if dropping"},{"location":"operations/monitoring/#capacity-planning","title":"Capacity Planning","text":"<p>Monitor these metrics for capacity planning:</p> <pre><code># Reconciliation frequency (how often config changes)\nrate(haproxy_ic_reconciliation_total[1h]) * 3600\n\n# Ingress growth rate\nderiv(haproxy_ic_resource_count{type=\"ingresses\"}[1d])\n\n# Average reconciliation overhead\navg_over_time(haproxy_ic_reconciliation_duration_seconds_sum[1d]) /\navg_over_time(haproxy_ic_reconciliation_duration_seconds_count[1d])\n</code></pre>"},{"location":"operations/monitoring/#troubleshooting-with-metrics","title":"Troubleshooting with Metrics","text":"<p>High reconciliation error rate: 1. Check <code>haproxy_ic_validation_errors_total</code> - template/config issues 2. Check <code>haproxy_ic_deployment_errors_total</code> - HAProxy connectivity issues 3. Review controller logs for specific error messages</p> <p>Missing metrics: 1. Verify metrics port is enabled (<code>controller.config.controller.metrics_port</code>) 2. Check ServiceMonitor selector matches Prometheus configuration 3. Verify network policies allow scraping</p> <p>Leader election issues: 1. Check if <code>sum(haproxy_ic_leader_election_is_leader) != 1</code> 2. Review <code>rate(haproxy_ic_leader_election_transitions_total[1h])</code> for instability 3. See High Availability Guide for troubleshooting</p>"},{"location":"operations/monitoring/#integration-with-existing-monitoring","title":"Integration with Existing Monitoring","text":""},{"location":"operations/monitoring/#prometheus-operator","title":"Prometheus Operator","text":"<pre><code># values.yaml\nserviceMonitor:\n  enabled: true\n  interval: 30s\n  scrapeTimeout: 10s\n  labels:\n    release: prometheus\n  namespaceSelector:\n    matchNames:\n      - haproxy-template-ic\n</code></pre>"},{"location":"operations/monitoring/#victoria-metrics","title":"Victoria Metrics","text":"<p>Use the same Prometheus scrape configuration - Victoria Metrics is compatible.</p>"},{"location":"operations/monitoring/#datadog","title":"Datadog","text":"<p>Configure Datadog Agent to scrape Prometheus metrics:</p> <pre><code># datadog-agent values\ndatadog:\n  prometheusScrape:\n    enabled: true\n    serviceEndpoints: true\n</code></pre>"},{"location":"operations/monitoring/#see-also","title":"See Also","text":"<ul> <li>Debugging Guide - Runtime introspection and troubleshooting</li> <li>High Availability - Leader election configuration</li> <li>Troubleshooting Guide - General troubleshooting</li> </ul>"},{"location":"operations/performance/","title":"Performance Guide","text":"<p>This guide covers performance tuning and optimization for the HAProxy Template Ingress Controller.</p>"},{"location":"operations/performance/#overview","title":"Overview","text":"<p>Performance optimization involves three areas: - Controller performance - Template rendering, reconciliation cycles - HAProxy performance - Load balancer throughput and latency - Kubernetes integration - Resource watching and event handling</p>"},{"location":"operations/performance/#controller-resource-sizing","title":"Controller Resource Sizing","text":""},{"location":"operations/performance/#recommended-resources","title":"Recommended Resources","text":"Deployment Size CPU Request CPU Limit Memory Request Memory Limit Small (&lt;50 Ingresses) 50m 200m 64Mi 256Mi Medium (50-200 Ingresses) 100m 500m 128Mi 512Mi Large (200+ Ingresses) 200m 1000m 256Mi 1Gi <p>Configure via Helm values:</p> <pre><code># values.yaml\nresources:\n  requests:\n    cpu: 100m\n    memory: 128Mi\n  limits:\n    cpu: 500m\n    memory: 512Mi\n</code></pre>"},{"location":"operations/performance/#memory-considerations","title":"Memory Considerations","text":"<p>Memory usage scales with: - Number of watched resources (Ingresses, Services, Endpoints) - Size of template library - Event buffer size (default 1000 events) - Number of HAProxy pods being managed</p> <p>Monitor memory usage: <pre><code>container_memory_working_set_bytes{container=\"haproxy-template-ic\"}\n</code></pre></p>"},{"location":"operations/performance/#cpu-considerations","title":"CPU Considerations","text":"<p>CPU spikes occur during: - Template rendering (complex templates with many resources) - Initial resource synchronization (startup) - Burst of resource changes (rolling updates)</p> <p>Monitor CPU usage: <pre><code>rate(container_cpu_usage_seconds_total{container=\"haproxy-template-ic\"}[5m])\n</code></pre></p>"},{"location":"operations/performance/#reconciliation-tuning","title":"Reconciliation Tuning","text":""},{"location":"operations/performance/#debounce-interval","title":"Debounce Interval","text":"<p>The controller debounces resource changes to avoid excessive reconciliation:</p> <pre><code># HAProxyTemplateConfig CRD\nspec:\n  controller:\n    reconciliation:\n      debounceInterval: 500ms  # Default\n</code></pre> <p>Tuning guidelines: - Lower (100-300ms): Faster response to changes, higher CPU usage - Default (500ms): Balanced for most workloads - Higher (1-5s): Better for high-churn environments with many changes</p>"},{"location":"operations/performance/#reconciliation-metrics","title":"Reconciliation Metrics","text":"<p>Monitor reconciliation performance:</p> <pre><code># Average reconciliation duration\nrate(haproxy_ic_reconciliation_duration_seconds_sum[5m]) /\nrate(haproxy_ic_reconciliation_duration_seconds_count[5m])\n\n# Reconciliation rate\nrate(haproxy_ic_reconciliation_total[5m])\n\n# P95 reconciliation latency\nhistogram_quantile(0.95, rate(haproxy_ic_reconciliation_duration_seconds_bucket[5m]))\n</code></pre> <p>Target metrics: - Average reconciliation: &lt;500ms - P95 reconciliation: &lt;2s - Error rate: &lt;1%</p>"},{"location":"operations/performance/#template-optimization","title":"Template Optimization","text":""},{"location":"operations/performance/#efficient-template-patterns","title":"Efficient Template Patterns","text":"<p>Use early filtering: <pre><code>{#- GOOD: Filter early, process less data -#}\n{%- set matching_ingresses = resources.ingresses.List() | selectattr(\"spec.ingressClassName\", \"equalto\", \"haproxy\") | list %}\n{%- for ingress in matching_ingresses %}\n  ...\n{%- endfor %}\n\n{#- AVOID: Processing all ingresses then filtering -#}\n{%- for ingress in resources.ingresses.List() %}\n  {%- if ingress.spec.ingressClassName == \"haproxy\" %}\n    ...\n  {%- endif %}\n{%- endfor %}\n</code></pre></p> <p>Use compute_once for expensive operations: <pre><code>{%- set analysis = namespace(sorted_routes=[]) %}\n{%- compute_once analysis %}\n  {#- Expensive computation only runs once per render -#}\n  {%- for route in resources.httproutes.List() %}\n    {%- set _ = analysis.sorted_routes.append(route) %}\n  {%- endfor %}\n{%- endcompute_once %}\n</code></pre></p> <p>Avoid nested loops when possible: <pre><code>{#- AVOID: O(n*m) complexity -#}\n{%- for ingress in ingresses %}\n  {%- for service in services %}\n    {%- if ingress.spec.backend.service.name == service.metadata.name %}\n      ...\n    {%- endif %}\n  {%- endfor %}\n{%- endfor %}\n\n{#- BETTER: Use indexing or filtering -#}\n{%- set service_map = {} %}\n{%- for service in services %}\n  {%- set _ = service_map.update({service.metadata.name: service}) %}\n{%- endfor %}\n{%- for ingress in ingresses %}\n  {%- set service = service_map.get(ingress.spec.backend.service.name) %}\n  ...\n{%- endfor %}\n</code></pre></p>"},{"location":"operations/performance/#template-debugging","title":"Template Debugging","text":"<p>Profile template rendering:</p> <pre><code># Enable template tracing\n./bin/controller validate -f config.yaml --trace\n\n# View trace output\ncat /tmp/template-trace.log\n</code></pre>"},{"location":"operations/performance/#haproxy-optimization","title":"HAProxy Optimization","text":""},{"location":"operations/performance/#configuration-parameters","title":"Configuration Parameters","text":"<p>Key HAProxy parameters for performance:</p> <pre><code>global\n    maxconn {{ controller.config.haproxy.maxconn | default(2000) }}\n    nbthread {{ controller.config.haproxy.nbthread | default(4) }}\n    tune.bufsize {{ controller.config.haproxy.bufsize | default(16384) }}\n    tune.ssl.default-dh-param 2048\n\ndefaults\n    timeout connect 5s\n    timeout client 50s\n    timeout server 50s\n    timeout http-request 10s\n    timeout queue 60s\n</code></pre>"},{"location":"operations/performance/#connection-limits","title":"Connection Limits","text":"<p>Calculate maxconn based on expected load:</p> <pre><code>maxconn = (expected_concurrent_connections * safety_factor) / num_haproxy_pods\n</code></pre> <p>Example: - Expected: 10,000 concurrent connections - Safety factor: 1.5 - HAProxy pods: 3 - maxconn = (10,000 * 1.5) / 3 = 5,000</p>"},{"location":"operations/performance/#thread-configuration","title":"Thread Configuration","text":"<p>Match <code>nbthread</code> to available CPU cores:</p> <pre><code># HAProxy pod resources\nresources:\n  requests:\n    cpu: 2\n  limits:\n    cpu: 4\n\n# HAProxy config\nglobal\n    nbthread 4  # Match CPU limit\n</code></pre>"},{"location":"operations/performance/#buffer-sizing","title":"Buffer Sizing","text":"<p>Increase buffers for large headers or payloads:</p> <pre><code>global\n    tune.bufsize 32768        # 32KB for large headers\n    tune.http.maxhdr 128      # Allow more headers\n</code></pre>"},{"location":"operations/performance/#scaling-strategies","title":"Scaling Strategies","text":""},{"location":"operations/performance/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Scale HAProxy pods for increased traffic:</p> <pre><code>kubectl scale deployment haproxy --replicas=5\n</code></pre> <p>The controller automatically discovers new pods and deploys configuration.</p>"},{"location":"operations/performance/#controller-scaling-ha-mode","title":"Controller Scaling (HA Mode)","text":"<p>For high availability, run multiple controller replicas:</p> <pre><code># values.yaml\nreplicaCount: 3\n\ncontroller:\n  config:\n    controller:\n      leader_election:\n        enabled: true\n</code></pre> <p>Only the leader performs deployments; followers maintain hot-standby status.</p>"},{"location":"operations/performance/#resource-watching-optimization","title":"Resource Watching Optimization","text":"<p>Reduce watched resources to minimize controller load:</p> <pre><code># Only watch specific namespaces\nspec:\n  watchedResources:\n    ingresses:\n      apiVersion: networking.k8s.io/v1\n      resources: ingresses\n      namespaceSelector:\n        matchNames:\n          - production\n          - staging\n\n# Use label selectors\nspec:\n  watchedResources:\n    ingresses:\n      apiVersion: networking.k8s.io/v1\n      resources: ingresses\n      labelSelector:\n        matchLabels:\n          managed-by: haproxy-template-ic\n</code></pre>"},{"location":"operations/performance/#deployment-performance","title":"Deployment Performance","text":""},{"location":"operations/performance/#deployment-latency","title":"Deployment Latency","text":"<p>Monitor deployment time:</p> <pre><code># Average deployment duration\nrate(haproxy_ic_deployment_duration_seconds_sum[5m]) /\nrate(haproxy_ic_deployment_duration_seconds_count[5m])\n\n# P95 deployment latency\nhistogram_quantile(0.95, rate(haproxy_ic_deployment_duration_seconds_bucket[5m]))\n</code></pre> <p>Target metrics: - Average deployment: &lt;1s per HAProxy pod - P95 deployment: &lt;3s</p>"},{"location":"operations/performance/#parallel-deployment","title":"Parallel Deployment","text":"<p>The controller deploys to multiple HAProxy pods in parallel. If deployment is slow:</p> <ol> <li>Check DataPlane API responsiveness</li> <li>Verify network connectivity to HAProxy pods</li> <li>Consider reducing config complexity</li> </ol>"},{"location":"operations/performance/#drift-prevention","title":"Drift Prevention","text":"<p>Configure drift prevention to avoid unnecessary deployments:</p> <pre><code>spec:\n  controller:\n    deployment:\n      driftPreventionInterval: 60s  # Check for drift every 60s\n</code></pre>"},{"location":"operations/performance/#event-processing","title":"Event Processing","text":""},{"location":"operations/performance/#event-buffer-sizing","title":"Event Buffer Sizing","text":"<p>The controller maintains event buffers for debugging:</p> <pre><code>spec:\n  controller:\n    eventBufferSize: 1000  # Default\n</code></pre> <p>Increase for high-throughput environments if you need more event history.</p>"},{"location":"operations/performance/#subscriber-performance","title":"Subscriber Performance","text":"<p>Monitor event subscriber health:</p> <pre><code># Event publishing rate\nrate(haproxy_ic_events_published_total[5m])\n\n# Subscriber count (should be constant)\nhaproxy_ic_event_subscribers\n</code></pre> <p>If subscriber count drops, components may be failing.</p>"},{"location":"operations/performance/#profiling","title":"Profiling","text":""},{"location":"operations/performance/#go-profiling","title":"Go Profiling","text":"<p>Access pprof endpoints for profiling:</p> <pre><code># CPU profile (30 seconds)\ncurl http://localhost:6060/debug/pprof/profile?seconds=30 &gt; cpu.pprof\ngo tool pprof -http=:8080 cpu.pprof\n\n# Memory profile\ncurl http://localhost:6060/debug/pprof/heap &gt; heap.pprof\ngo tool pprof -http=:8080 heap.pprof\n\n# Goroutine dump\ncurl http://localhost:6060/debug/pprof/goroutine?debug=1\n</code></pre>"},{"location":"operations/performance/#common-performance-issues","title":"Common Performance Issues","text":"<p>High memory usage: - Check for memory leaks: growing heap over time - Reduce event buffer size - Limit watched resources</p> <p>High CPU usage: - Profile to find hot spots - Optimize template complexity - Increase debounce interval</p> <p>Slow deployments: - Check DataPlane API health - Verify network latency to HAProxy pods - Consider reducing config size</p>"},{"location":"operations/performance/#performance-checklist","title":"Performance Checklist","text":""},{"location":"operations/performance/#initial-deployment","title":"Initial Deployment","text":"<ul> <li>[ ] Set appropriate resource requests/limits</li> <li>[ ] Configure debounce interval for workload</li> <li>[ ] Set HAProxy maxconn based on expected load</li> <li>[ ] Match nbthread to CPU allocation</li> </ul>"},{"location":"operations/performance/#ongoing-optimization","title":"Ongoing Optimization","text":"<ul> <li>[ ] Monitor reconciliation latency</li> <li>[ ] Monitor deployment latency</li> <li>[ ] Watch for memory growth</li> <li>[ ] Track event subscriber count</li> </ul>"},{"location":"operations/performance/#high-load-environments","title":"High-Load Environments","text":"<ul> <li>[ ] Scale HAProxy pods horizontally</li> <li>[ ] Enable HA mode for controller</li> <li>[ ] Limit watched namespaces</li> <li>[ ] Use label selectors to filter resources</li> <li>[ ] Profile and optimize templates</li> </ul>"},{"location":"operations/performance/#see-also","title":"See Also","text":"<ul> <li>Monitoring Guide - Performance metrics and alerting</li> <li>High Availability - HA deployment patterns</li> <li>Debugging Guide - Performance troubleshooting</li> </ul>"},{"location":"operations/security/","title":"Security Guide","text":"<p>This guide covers security best practices for deploying and operating the HAProxy Template Ingress Controller.</p>"},{"location":"operations/security/#overview","title":"Overview","text":"<p>The controller follows security best practices including: - Principle of least privilege for RBAC - Read-only filesystem for controller containers - Secure credential management via Kubernetes Secrets - Support for TLS throughout the stack</p>"},{"location":"operations/security/#rbac-configuration","title":"RBAC Configuration","text":""},{"location":"operations/security/#controller-service-account","title":"Controller Service Account","text":"<p>The Helm chart creates a service account with minimal required permissions:</p> <pre><code># Automatically created by Helm\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: haproxy-template-ic\n</code></pre>"},{"location":"operations/security/#clusterrole-permissions","title":"ClusterRole Permissions","text":"<p>The controller requires these Kubernetes API permissions:</p> Resource Verbs Purpose pods, namespaces get, list, watch Discover HAProxy pods and namespaces ingresses get, list, watch Watch Ingress resources services, endpoints, endpointslices get, list, watch Discover backend endpoints secrets get, list, watch Load TLS certificates and credentials leases (coordination.k8s.io) get, create, update Leader election for HA deployments haproxytemplateconfigs get, list, watch Watch configuration CRD <p>Customizing RBAC:</p> <pre><code># values.yaml\nrbac:\n  create: true  # Set to false to manage RBAC manually\n</code></pre> <p>If you manage RBAC manually, ensure the service account has access to all resources defined in your <code>watchedResources</code> configuration.</p>"},{"location":"operations/security/#restricting-namespace-access","title":"Restricting Namespace Access","text":"<p>By default, the controller watches resources cluster-wide. To restrict to specific namespaces:</p> <pre><code># HAProxyTemplateConfig CRD\nspec:\n  watchedResources:\n    ingresses:\n      apiVersion: networking.k8s.io/v1\n      resources: ingresses\n      namespaceSelector:\n        matchNames:\n          - production\n          - staging\n</code></pre>"},{"location":"operations/security/#credential-management","title":"Credential Management","text":""},{"location":"operations/security/#dataplane-api-credentials","title":"DataPlane API Credentials","text":"<p>The controller uses credentials to authenticate with the HAProxy DataPlane API:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: haproxy-credentials\ntype: Opaque\nstringData:\n  dataplane_username: admin\n  dataplane_password: &lt;strong-password&gt;\n  # For validation sidecar (optional)\n  validation_username: validator\n  validation_password: &lt;validation-password&gt;\n</code></pre> <p>Best practices: - Use strong, randomly generated passwords - Rotate credentials periodically - Use different credentials for production and validation sidecars - Consider using a secrets manager (Vault, External Secrets Operator)</p>"},{"location":"operations/security/#external-secrets-integration","title":"External Secrets Integration","text":"<p>Integrate with external secrets managers:</p> <pre><code># External Secrets Operator example\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: haproxy-credentials\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: vault-backend\n    kind: ClusterSecretStore\n  target:\n    name: haproxy-credentials\n  data:\n    - secretKey: dataplane_username\n      remoteRef:\n        key: haproxy/dataplane\n        property: username\n    - secretKey: dataplane_password\n      remoteRef:\n        key: haproxy/dataplane\n        property: password\n</code></pre>"},{"location":"operations/security/#debug-endpoint-security","title":"Debug Endpoint Security","text":"<p>The debug endpoints do NOT expose actual credentials:</p> <pre><code># /debug/vars/credentials returns only metadata\ncurl http://localhost:6060/debug/vars/credentials\n</code></pre> <p>Response: <pre><code>{\n  \"version\": \"12345\",\n  \"has_dataplane_creds\": true\n}\n</code></pre></p> <p>Actual passwords are never exposed through debug endpoints.</p>"},{"location":"operations/security/#container-security","title":"Container Security","text":""},{"location":"operations/security/#read-only-filesystem","title":"Read-Only Filesystem","text":"<p>The controller runs with a read-only root filesystem:</p> <pre><code># Enabled by default in Helm chart\nsecurityContext:\n  readOnlyRootFilesystem: true\n</code></pre> <p>Temporary files (for validation) are written to <code>/tmp</code> which is mounted as <code>emptyDir</code>.</p>"},{"location":"operations/security/#security-context","title":"Security Context","text":"<p>Recommended security context:</p> <pre><code># values.yaml\nsecurityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n  runAsGroup: 1000\n  readOnlyRootFilesystem: true\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop:\n      - ALL\n</code></pre>"},{"location":"operations/security/#pod-security-standards","title":"Pod Security Standards","text":"<p>The controller is compatible with Kubernetes Pod Security Standards at the \"restricted\" level:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: haproxy-template-ic\n  labels:\n    pod-security.kubernetes.io/enforce: restricted\n    pod-security.kubernetes.io/audit: restricted\n    pod-security.kubernetes.io/warn: restricted\n</code></pre>"},{"location":"operations/security/#network-policies","title":"Network Policies","text":""},{"location":"operations/security/#restricting-controller-traffic","title":"Restricting Controller Traffic","text":"<p>Limit controller network access:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: haproxy-template-ic\n  namespace: haproxy-template-ic\nspec:\n  podSelector:\n    matchLabels:\n      app.kubernetes.io/name: haproxy-template-ic\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress:\n    # Allow health checks\n    - from: []\n      ports:\n        - port: 8080  # healthz\n        - port: 9090  # metrics\n  egress:\n    # Allow Kubernetes API access\n    - to:\n        - namespaceSelector: {}\n          podSelector:\n            matchLabels:\n              component: kube-apiserver\n      ports:\n        - port: 443\n    # Allow HAProxy DataPlane API access\n    - to:\n        - podSelector:\n            matchLabels:\n              app: haproxy\n      ports:\n        - port: 5555  # DataPlane API\n</code></pre>"},{"location":"operations/security/#restricting-debug-endpoint-access","title":"Restricting Debug Endpoint Access","text":"<p>If you enable the debug port, restrict access:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: haproxy-template-ic-debug\n  namespace: haproxy-template-ic\nspec:\n  podSelector:\n    matchLabels:\n      app.kubernetes.io/name: haproxy-template-ic\n  policyTypes:\n    - Ingress\n  ingress:\n    # Only allow debug access from specific namespace\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              name: monitoring\n      ports:\n        - port: 6060  # debug\n</code></pre>"},{"location":"operations/security/#tls-configuration","title":"TLS Configuration","text":""},{"location":"operations/security/#tls-for-ingress-traffic","title":"TLS for Ingress Traffic","text":"<p>Configure TLS termination through Ingress resources:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-app\nspec:\n  tls:\n    - hosts:\n        - myapp.example.com\n      secretName: myapp-tls\n  rules:\n    - host: myapp.example.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: my-app\n                port:\n                  number: 80\n</code></pre>"},{"location":"operations/security/#tls-certificates-from-secrets","title":"TLS Certificates from Secrets","text":"<p>The controller loads TLS certificates from Kubernetes Secrets:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: myapp-tls\ntype: kubernetes.io/tls\ndata:\n  tls.crt: &lt;base64-encoded-certificate&gt;\n  tls.key: &lt;base64-encoded-private-key&gt;\n</code></pre>"},{"location":"operations/security/#certificate-management-with-cert-manager","title":"Certificate Management with cert-manager","text":"<p>Integrate with cert-manager for automatic certificate management:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: myapp-tls\nspec:\n  secretName: myapp-tls\n  issuerRef:\n    name: letsencrypt-prod\n    kind: ClusterIssuer\n  dnsNames:\n    - myapp.example.com\n</code></pre>"},{"location":"operations/security/#haproxy-dataplane-api-tls","title":"HAProxy DataPlane API TLS","text":"<p>For production deployments, enable TLS for the DataPlane API:</p> <pre><code># HAProxy sidecar configuration\ndataplane:\n  insecure: false  # Require TLS\n  ssl_certificate: /etc/haproxy/ssl/dataplane.crt\n  ssl_key: /etc/haproxy/ssl/dataplane.key\n</code></pre>"},{"location":"operations/security/#secrets-in-templates","title":"Secrets in Templates","text":""},{"location":"operations/security/#secure-handling","title":"Secure Handling","text":"<p>When using secrets in templates, follow these practices:</p> <pre><code>{#- Load secret data - automatically base64 decoded -#}\n{%- for secret in resources.secrets.List() %}\n{%- if secret.metadata.name == \"auth-users\" %}\n  {#- Use secret.data fields - they're decoded automatically -#}\n  userlist authenticated_users\n    user admin password {{ secret.data.password_hash }}\n{%- endif %}\n{%- endfor %}\n</code></pre> <p>Best practices: - Never log secret values - Use password hashes, not plaintext passwords - Limit secret access to specific namespaces - Rotate secrets regularly</p>"},{"location":"operations/security/#password-hash-format","title":"Password Hash Format","text":"<p>For HAProxy authentication, store password hashes (not plaintext):</p> <pre><code># Generate bcrypt hash\nhtpasswd -nbB admin mypassword | cut -d: -f2\n\n# Store in secret (hash only, not username:hash)\nkubectl create secret generic auth-users \\\n  --from-literal=password_hash='$2y$05$...'\n</code></pre>"},{"location":"operations/security/#audit-logging","title":"Audit Logging","text":""},{"location":"operations/security/#controller-logs","title":"Controller Logs","text":"<p>The controller logs security-relevant events:</p> <pre><code># View security-related logs\nkubectl logs -n haproxy-template-ic deployment/haproxy-template-ic | grep -E \"auth|credential|secret\"\n</code></pre>"},{"location":"operations/security/#kubernetes-audit-policy","title":"Kubernetes Audit Policy","text":"<p>Include controller operations in Kubernetes audit policy:</p> <pre><code>apiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n  # Audit secret access\n  - level: Metadata\n    resources:\n      - group: \"\"\n        resources: [\"secrets\"]\n    users: [\"system:serviceaccount:haproxy-template-ic:haproxy-template-ic\"]\n\n  # Audit configuration changes\n  - level: RequestResponse\n    resources:\n      - group: \"haproxy-template-ic.github.io\"\n        resources: [\"haproxytemplateconfigs\"]\n</code></pre>"},{"location":"operations/security/#security-checklist","title":"Security Checklist","text":""},{"location":"operations/security/#deployment-checklist","title":"Deployment Checklist","text":"<ul> <li>[ ] Use strong, unique passwords for DataPlane API credentials</li> <li>[ ] Enable read-only root filesystem</li> <li>[ ] Run as non-root user</li> <li>[ ] Drop all capabilities</li> <li>[ ] Enable network policies to restrict traffic</li> <li>[ ] Use TLS for all external endpoints</li> <li>[ ] Restrict RBAC to required namespaces</li> <li>[ ] Enable Kubernetes audit logging</li> </ul>"},{"location":"operations/security/#operational-checklist","title":"Operational Checklist","text":"<ul> <li>[ ] Rotate credentials periodically</li> <li>[ ] Monitor for unauthorized access attempts</li> <li>[ ] Review RBAC permissions after configuration changes</li> <li>[ ] Keep controller image updated for security patches</li> <li>[ ] Use image scanning in CI/CD pipeline</li> </ul>"},{"location":"operations/security/#production-hardening","title":"Production Hardening","text":"<p>For high-security environments:</p> <pre><code># values.yaml\nsecurityContext:\n  runAsNonRoot: true\n  runAsUser: 65534  # nobody\n  runAsGroup: 65534\n  fsGroup: 65534\n  readOnlyRootFilesystem: true\n  allowPrivilegeEscalation: false\n  seccompProfile:\n    type: RuntimeDefault\n  capabilities:\n    drop:\n      - ALL\n\ncontroller:\n  debugPort: 0  # Disable debug endpoint\n\nrbac:\n  create: true\n</code></pre>"},{"location":"operations/security/#see-also","title":"See Also","text":"<ul> <li>Monitoring Guide - Monitor security-related metrics</li> <li>High Availability - Secure HA deployments</li> <li>Debugging Guide - Secure debugging practices</li> </ul>"}]}